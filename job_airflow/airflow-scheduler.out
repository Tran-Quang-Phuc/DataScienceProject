[[34m2023-12-03T09:16:07.008+0700[0m] {[34mscheduler_job_runner.py:[0m797} INFO[0m - Starting the scheduler[0m
[[34m2023-12-03T09:16:07.009+0700[0m] {[34mscheduler_job_runner.py:[0m804} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-12-03T09:16:07.015+0700[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 66670[0m
[[34m2023-12-03T09:16:07.016+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:16:07.019+0700[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-12-03T09:16:07.038+0700] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-12-03T09:16:24.306+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T02:16:07.288425+00:00, run_after=2023-12-03T02:46:07.288425+00:00[0m
[[34m2023-12-03T09:16:24.369+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:16:24.369+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:16:24.369+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 1/16 running and queued tasks[0m
[[34m2023-12-03T09:16:24.369+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:16:24.371+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task print_folder because previous state change time has not been saved[0m
[[34m2023-12-03T09:16:24.372+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task print_folder because previous state change time has not been saved[0m
[[34m2023-12-03T09:16:24.372+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-03T09:16:24.372+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:24.373+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-03T09:16:24.373+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:24.375+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:25.336+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T01:46:07.288425+00:00/task_id=print_folder permission to 509
[[34m2023-12-03T09:16:25.483+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T01:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:16:25.968+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:26.738+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=manual__2023-12-03T02:16:23.978630+00:00/task_id=print_folder permission to 509
[[34m2023-12-03T09:16:26.881+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:16:23.978630+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:16:27.342+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:16:27.342+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:16:27.349+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:16:26.929556+00:00, run_end_date=2023-12-03 02:16:27.041743+00:00, run_duration=0.112187, state=success, executor_state=success, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:16:24.370408+00:00, queued_by_job_id=1, pid=66861[0m
[[34m2023-12-03T09:16:27.350+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:16:25.531496+00:00, run_end_date=2023-12-03 02:16:25.646874+00:00, run_duration=0.115378, state=success, executor_state=success, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:16:24.370408+00:00, queued_by_job_id=1, pid=66842[0m
[[34m2023-12-03T09:16:27.404+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:16:27.404+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:16:27.404+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 1/16 running and queued tasks[0m
[[34m2023-12-03T09:16:27.404+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:16:27.405+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:16:27.406+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:16:27.406+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T09:16:27.406+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:27.406+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T09:16:27.406+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:27.409+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:28.189+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T01:46:07.288425+00:00/task_id=crawl_data permission to 509
[[34m2023-12-03T09:16:28.327+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T01:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:16:34.026+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:34.812+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=manual__2023-12-03T02:16:23.978630+00:00/task_id=crawl_data permission to 509
[[34m2023-12-03T09:16:34.951+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:16:23.978630+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:16:40.366+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:16:40.366+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:16:40.370+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:16:34.998361+00:00, run_end_date=2023-12-03 02:16:40.016627+00:00, run_duration=5.018266, state=success, executor_state=success, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:16:27.405264+00:00, queued_by_job_id=1, pid=66916[0m
[[34m2023-12-03T09:16:40.371+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:16:28.374315+00:00, run_end_date=2023-12-03 02:16:33.731439+00:00, run_duration=5.357124, state=success, executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:16:27.405264+00:00, queued_by_job_id=1, pid=66864[0m
[[34m2023-12-03T09:16:40.542+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:16:40.542+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:16:40.543+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 1/16 running and queued tasks[0m
[[34m2023-12-03T09:16:40.543+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:16:40.545+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task save_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:16:40.546+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task save_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:16:40.546+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:16:40.547+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:40.547+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:16:40.547+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:40.550+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:41.353+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T01:46:07.288425+00:00/task_id=save_data permission to 509
[[34m2023-12-03T09:16:41.495+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:16:42.231+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:16:43.005+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=manual__2023-12-03T02:16:23.978630+00:00/task_id=save_data permission to 509
[[34m2023-12-03T09:16:43.151+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:16:43.888+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:16:43.888+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:16:43.892+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:16:43.199832+00:00, run_end_date=2023-12-03 02:16:43.612135+00:00, run_duration=0.412303, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:16:40.544254+00:00, queued_by_job_id=1, pid=67037[0m
[[34m2023-12-03T09:16:43.892+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:16:41.542020+00:00, run_end_date=2023-12-03 02:16:41.945041+00:00, run_duration=0.403021, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:16:40.544254+00:00, queued_by_job_id=1, pid=67034[0m
[[34m2023-12-03T09:21:07.207+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:21:42.428+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:21:42.428+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:21:42.428+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:21:42.429+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:21:42.430+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:21:42.432+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:21:43.140+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T09:21:43.279+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:21:43.929+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-03T09:21:43.935+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:21:43.332946+00:00, run_end_date=2023-12-03 02:21:43.664048+00:00, run_duration=0.331102, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:21:42.428779+00:00, queued_by_job_id=1, pid=70121[0m
[[34m2023-12-03T09:21:44.107+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:21:44.107+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:21:44.108+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>[0m
[[34m2023-12-03T09:21:44.109+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:21:44.109+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:21:44.111+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:21:44.844+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T09:21:44.978+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:21:45.658+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-03T09:21:45.663+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:21:45.031170+00:00, run_end_date=2023-12-03 02:21:45.373312+00:00, run_duration=0.342142, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:21:44.108319+00:00, queued_by_job_id=1, pid=70125[0m
[[34m2023-12-03T09:22:32.494+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun my_first_dag @ 2023-12-03 01:46:07.288425+00:00: scheduled__2023-12-03T01:46:07.288425+00:00, state:running, queued_at: 2023-12-03 02:16:24.293048+00:00. externally triggered: False> failed[0m
[[34m2023-12-03T09:22:32.495+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 01:46:07.288425+00:00, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, run_start_date=2023-12-03 02:16:24.323804+00:00, run_end_date=2023-12-03 02:22:32.495673+00:00, run_duration=368.171869, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 01:46:07.288425+00:00, data_interval_end=2023-12-03 02:16:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d[0m
[[34m2023-12-03T09:22:32.498+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T02:16:07.288425+00:00, run_after=2023-12-03T02:46:07.288425+00:00[0m
[[34m2023-12-03T09:22:40.480+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun my_first_dag @ 2023-12-03 02:16:23.978630+00:00: manual__2023-12-03T02:16:23.978630+00:00, state:running, queued_at: 2023-12-03 02:16:24.000999+00:00. externally triggered: True> failed[0m
[[34m2023-12-03T09:22:40.481+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:16:23.978630+00:00, run_id=manual__2023-12-03T02:16:23.978630+00:00, run_start_date=2023-12-03 02:16:24.324068+00:00, run_end_date=2023-12-03 02:22:40.481439+00:00, run_duration=376.157371, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-03 01:46:23.978630+00:00, data_interval_end=2023-12-03 02:16:23.978630+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d[0m
[[34m2023-12-03T09:24:16.458+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>[0m
[[34m2023-12-03T09:24:16.459+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:24:16.459+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>[0m
[[34m2023-12-03T09:24:16.460+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task print_folder because previous state change time has not been saved[0m
[[34m2023-12-03T09:24:16.461+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-03T09:24:16.461+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:24:16.463+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:24:17.340+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=manual__2023-12-03T02:24:15.469689+00:00/task_id=print_folder permission to 509
[[34m2023-12-03T09:24:17.515+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:24:15.469689+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:24:18.003+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:24:18.007+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=manual__2023-12-03T02:24:15.469689+00:00, map_index=-1, run_start_date=2023-12-03 02:24:17.567729+00:00, run_end_date=2023-12-03 02:24:17.692613+00:00, run_duration=0.124884, state=success, executor_state=success, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:24:16.459597+00:00, queued_by_job_id=1, pid=71248[0m
[[34m2023-12-03T09:24:18.054+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>[0m
[[34m2023-12-03T09:24:18.055+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:24:18.055+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>[0m
[[34m2023-12-03T09:24:18.056+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:24:18.057+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T09:24:18.057+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:24:18.059+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:24:18.865+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=manual__2023-12-03T02:24:15.469689+00:00/task_id=crawl_data permission to 509
[[34m2023-12-03T09:24:19.014+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:24:15.469689+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:24:24.397+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:24:24.402+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-03T02:24:15.469689+00:00, map_index=-1, run_start_date=2023-12-03 02:24:19.064935+00:00, run_end_date=2023-12-03 02:24:24.111508+00:00, run_duration=5.046573, state=success, executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:24:18.055735+00:00, queued_by_job_id=1, pid=71251[0m
[[34m2023-12-03T09:24:24.452+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>[0m
[[34m2023-12-03T09:24:24.453+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:24:24.453+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>[0m
[[34m2023-12-03T09:24:24.454+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task save_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:24:24.454+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:24:24.454+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:24:24.457+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:24:25.323+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=manual__2023-12-03T02:24:15.469689+00:00/task_id=save_data permission to 509
[[34m2023-12-03T09:24:25.472+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data manual__2023-12-03T02:24:15.469689+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:24:45.285+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:24:45.288+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=manual__2023-12-03T02:24:15.469689+00:00, map_index=-1, run_start_date=2023-12-03 02:24:25.516739+00:00, run_end_date=2023-12-03 02:24:44.995142+00:00, run_duration=19.478403, state=success, executor_state=success, try_number=1, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:24:24.453686+00:00, queued_by_job_id=1, pid=71302[0m
[[34m2023-12-03T09:24:45.437+0700[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun my_first_dag @ 2023-12-03 02:24:15.469689+00:00: manual__2023-12-03T02:24:15.469689+00:00, state:running, queued_at: 2023-12-03 02:24:15.476258+00:00. externally triggered: True> successful[0m
[[34m2023-12-03T09:24:45.437+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:24:15.469689+00:00, run_id=manual__2023-12-03T02:24:15.469689+00:00, run_start_date=2023-12-03 02:24:16.430799+00:00, run_end_date=2023-12-03 02:24:45.437861+00:00, run_duration=29.007062, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-03 01:54:15.469689+00:00, data_interval_end=2023-12-03 02:24:15.469689+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d[0m
[[34m2023-12-03T09:26:07.244+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:31:07.279+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:36:07.310+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:41:07.352+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:46:07.388+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:46:08.094+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T02:46:07.288425+00:00, run_after=2023-12-03T03:16:07.288425+00:00[0m
[[34m2023-12-03T09:46:08.126+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:46:08.126+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:46:08.126+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:46:08.127+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task print_folder because previous state change time has not been saved[0m
[[34m2023-12-03T09:46:08.127+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-03T09:46:08.127+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:46:08.129+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:46:08.623+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T02:16:07.288425+00:00/task_id=print_folder permission to 509
[[34m2023-12-03T09:46:08.716+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:46:09.071+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:46:09.074+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:46:08.749620+00:00, run_end_date=2023-12-03 02:46:08.825705+00:00, run_duration=0.076085, state=success, executor_state=success, try_number=1, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:46:08.126565+00:00, queued_by_job_id=1, pid=77296[0m
[[34m2023-12-03T09:46:09.111+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:46:09.112+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:46:09.112+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:46:09.112+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:46:09.113+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T09:46:09.113+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:46:09.114+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:46:09.631+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T02:16:07.288425+00:00/task_id=crawl_data permission to 509
[[34m2023-12-03T09:46:09.727+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:46:14.248+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:46:14.251+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:46:09.759615+00:00, run_end_date=2023-12-03 02:46:14.034915+00:00, run_duration=4.2753, state=success, executor_state=success, try_number=1, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:46:09.112438+00:00, queued_by_job_id=1, pid=77299[0m
[[34m2023-12-03T09:46:14.290+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:46:14.290+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:46:14.290+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:46:14.291+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task save_data because previous state change time has not been saved[0m
[[34m2023-12-03T09:46:14.291+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:46:14.291+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:46:14.293+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:46:14.804+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T02:16:07.288425+00:00/task_id=save_data permission to 509
[[34m2023-12-03T09:46:14.892+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:46:22.761+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T09:46:22.764+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:46:14.919826+00:00, run_end_date=2023-12-03 02:46:22.504665+00:00, run_duration=7.584839, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:46:14.290902+00:00, queued_by_job_id=1, pid=77324[0m
[[34m2023-12-03T09:51:07.927+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:51:23.275+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:51:23.275+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:51:23.275+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:51:23.276+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:51:23.276+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:51:23.278+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:51:23.806+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T09:51:23.894+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:51:31.639+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-03T09:51:31.641+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:51:23.920384+00:00, run_end_date=2023-12-03 02:51:31.402771+00:00, run_duration=7.482387, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:51:23.276094+00:00, queued_by_job_id=1, pid=78599[0m
[[34m2023-12-03T09:56:07.968+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T09:56:31.745+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:56:31.745+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T09:56:31.746+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T09:56:31.747+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-03T09:56:31.747+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:56:31.749+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T09:56:32.239+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T09:56:32.327+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T09:56:40.921+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-03T09:56:40.924+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:56:32.355931+00:00, run_end_date=2023-12-03 02:56:40.710592+00:00, run_duration=8.354661, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:56:31.746376+00:00, queued_by_job_id=1, pid=79924[0m
[[34m2023-12-03T09:56:41.064+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun my_first_dag @ 2023-12-03 02:16:07.288425+00:00: scheduled__2023-12-03T02:16:07.288425+00:00, state:running, queued_at: 2023-12-03 02:46:08.092583+00:00. externally triggered: False> failed[0m
[[34m2023-12-03T09:56:41.064+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:16:07.288425+00:00, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, run_start_date=2023-12-03 02:46:08.110583+00:00, run_end_date=2023-12-03 02:56:41.064517+00:00, run_duration=632.953934, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 02:16:07.288425+00:00, data_interval_end=2023-12-03 02:46:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d[0m
[[34m2023-12-03T09:56:41.065+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T02:46:07.288425+00:00, run_after=2023-12-03T03:16:07.288425+00:00[0m
[[34m2023-12-03T10:01:08.005+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:06:08.015+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:11:08.046+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:16:08.144+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T03:16:07.288425+00:00, run_after=2023-12-03T03:46:07.288425+00:00[0m
[[34m2023-12-03T10:16:08.177+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:16:08.178+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:16:08.178+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:16:08.178+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task print_folder because previous state change time has not been saved[0m
[[34m2023-12-03T10:16:08.178+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-03T10:16:08.179+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:16:08.181+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:16:08.665+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T02:46:07.288425+00:00/task_id=print_folder permission to 509
[[34m2023-12-03T10:16:08.755+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:16:09.094+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T10:16:09.096+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:16:08.791679+00:00, run_end_date=2023-12-03 03:16:08.864039+00:00, run_duration=0.07236, state=success, executor_state=success, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 03:16:08.178307+00:00, queued_by_job_id=1, pid=85920[0m
[[34m2023-12-03T10:16:09.107+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:16:09.136+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:16:09.136+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:16:09.137+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:16:09.138+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved[0m
[[34m2023-12-03T10:16:09.138+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T10:16:09.138+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:16:09.140+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:16:09.626+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T02:46:07.288425+00:00/task_id=crawl_data permission to 509
[[34m2023-12-03T10:16:09.716+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:16:10.402+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T10:16:10.405+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:16:09.751958+00:00, run_end_date=2023-12-03 03:16:10.171626+00:00, run_duration=0.419668, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:16:09.137380+00:00, queued_by_job_id=1, pid=85923[0m
[[34m2023-12-03T10:21:09.162+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:21:10.213+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:21:10.213+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:21:10.213+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:21:10.214+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T10:21:10.214+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:21:10.216+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:21:10.702+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T10:21:10.795+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:21:11.486+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-03T10:21:11.489+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:21:10.836444+00:00, run_end_date=2023-12-03 03:21:11.254870+00:00, run_duration=0.418426, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:21:10.213735+00:00, queued_by_job_id=1, pid=87714[0m
[[34m2023-12-03T10:26:09.211+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:26:11.704+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:26:11.705+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:26:11.705+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:26:11.705+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T10:26:11.705+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:26:11.708+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:26:12.210+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T10:26:12.298+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:26:12.985+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-03T10:26:12.987+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:26:12.328500+00:00, run_end_date=2023-12-03 03:26:12.749498+00:00, run_duration=0.420998, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:26:11.705316+00:00, queued_by_job_id=1, pid=89330[0m
[[34m2023-12-03T10:26:13.011+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun my_first_dag @ 2023-12-03 02:46:07.288425+00:00: scheduled__2023-12-03T02:46:07.288425+00:00, state:running, queued_at: 2023-12-03 03:16:08.140437+00:00. externally triggered: False> failed[0m
[[34m2023-12-03T10:26:13.011+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:46:07.288425+00:00, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, run_start_date=2023-12-03 03:16:08.161408+00:00, run_end_date=2023-12-03 03:26:13.011707+00:00, run_duration=604.850299, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 02:46:07.288425+00:00, data_interval_end=2023-12-03 03:16:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d[0m
[[34m2023-12-03T10:26:13.012+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T03:16:07.288425+00:00, run_after=2023-12-03T03:46:07.288425+00:00[0m
[[34m2023-12-03T10:31:09.240+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:36:09.279+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:41:09.335+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:46:08.706+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T03:46:07.288425+00:00, run_after=2023-12-03T04:16:07.288425+00:00[0m
[[34m2023-12-03T10:46:08.737+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:46:08.737+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:46:08.737+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:46:08.738+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task print_folder because previous state change time has not been saved[0m
[[34m2023-12-03T10:46:08.738+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-03T10:46:08.738+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:46:08.740+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:46:09.259+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T03:16:07.288425+00:00/task_id=print_folder permission to 509
[[34m2023-12-03T10:46:09.348+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:46:09.689+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T10:46:09.691+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:46:09.385944+00:00, run_end_date=2023-12-03 03:46:09.462969+00:00, run_duration=0.077025, state=success, executor_state=success, try_number=1, max_tries=2, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 03:46:08.737957+00:00, queued_by_job_id=1, pid=94314[0m
[[34m2023-12-03T10:46:09.702+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:46:09.743+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:46:09.743+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:46:09.743+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:46:09.744+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved[0m
[[34m2023-12-03T10:46:09.744+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T10:46:09.744+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:46:09.746+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:46:10.242+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T03:16:07.288425+00:00/task_id=crawl_data permission to 509
[[34m2023-12-03T10:46:10.332+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:46:11.034+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T10:46:11.037+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:46:10.367271+00:00, run_end_date=2023-12-03 03:46:10.806821+00:00, run_duration=0.43955, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:46:09.743620+00:00, queued_by_job_id=1, pid=94317[0m
[[34m2023-12-03T10:51:09.731+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:51:11.795+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:51:11.795+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:51:11.795+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:51:11.796+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T10:51:11.796+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:51:11.798+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:51:12.299+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T10:51:12.403+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:51:13.103+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-03T10:51:13.106+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:51:12.445954+00:00, run_end_date=2023-12-03 03:51:12.890127+00:00, run_duration=0.444173, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:51:11.795577+00:00, queued_by_job_id=1, pid=95471[0m
[[34m2023-12-03T10:56:09.767+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T10:56:13.279+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:56:13.279+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T10:56:13.279+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T10:56:13.280+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T10:56:13.280+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:56:13.282+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T10:56:13.837+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T10:56:13.978+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T10:56:14.786+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-03T10:56:14.789+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:56:14.026200+00:00, run_end_date=2023-12-03 03:56:14.524569+00:00, run_duration=0.498369, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:56:13.280123+00:00, queued_by_job_id=1, pid=96639[0m
[[34m2023-12-03T10:56:14.814+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun my_first_dag @ 2023-12-03 03:16:07.288425+00:00: scheduled__2023-12-03T03:16:07.288425+00:00, state:running, queued_at: 2023-12-03 03:46:08.705061+00:00. externally triggered: False> failed[0m
[[34m2023-12-03T10:56:14.814+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 03:16:07.288425+00:00, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, run_start_date=2023-12-03 03:46:08.722696+00:00, run_end_date=2023-12-03 03:56:14.814367+00:00, run_duration=606.091671, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 03:16:07.288425+00:00, data_interval_end=2023-12-03 03:46:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d[0m
[[34m2023-12-03T10:56:14.815+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T03:46:07.288425+00:00, run_after=2023-12-03T04:16:07.288425+00:00[0m
[[34m2023-12-03T11:01:09.809+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T11:06:09.835+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T11:11:09.860+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T11:16:08.046+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T04:16:07.288425+00:00, run_after=2023-12-03T04:46:07.288425+00:00[0m
[[34m2023-12-03T11:16:08.063+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:16:08.063+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T11:16:08.063+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:16:08.064+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task print_folder because previous state change time has not been saved[0m
[[34m2023-12-03T11:16:08.064+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-03T11:16:08.064+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:16:08.066+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:16:08.580+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T03:46:07.288425+00:00/task_id=print_folder permission to 509
[[34m2023-12-03T11:16:08.676+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T11:16:09.043+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T11:16:09.045+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:16:08.730026+00:00, run_end_date=2023-12-03 04:16:08.807554+00:00, run_duration=0.077528, state=success, executor_state=success, try_number=1, max_tries=2, job_id=26, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 04:16:08.064230+00:00, queued_by_job_id=1, pid=104028[0m
[[34m2023-12-03T11:16:09.081+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:16:09.081+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T11:16:09.081+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:16:09.082+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved[0m
[[34m2023-12-03T11:16:09.082+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T11:16:09.082+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:16:09.084+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:16:09.591+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=my_first_dag/run_id=scheduled__2023-12-03T03:46:07.288425+00:00/task_id=crawl_data permission to 509
[[34m2023-12-03T11:16:09.680+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T11:16:10.489+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-03T11:16:10.492+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:16:09.721851+00:00, run_end_date=2023-12-03 04:16:10.189493+00:00, run_duration=0.467642, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=27, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 04:16:09.081604+00:00, queued_by_job_id=1, pid=104031[0m
[[34m2023-12-03T11:16:10.503+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T11:21:10.513+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:21:10.513+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T11:21:10.513+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:21:10.514+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T11:21:10.514+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:21:10.516+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:21:11.020+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T11:21:11.113+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T11:21:11.821+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-03T11:21:11.824+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:21:11.149442+00:00, run_end_date=2023-12-03 04:21:11.587635+00:00, run_duration=0.438193, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 04:21:10.513719+00:00, queued_by_job_id=1, pid=106527[0m
[[34m2023-12-03T11:21:11.835+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T11:26:11.746+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:26:11.747+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG my_first_dag has 0/16 running and queued tasks[0m
[[34m2023-12-03T11:26:11.747+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>[0m
[[34m2023-12-03T11:26:11.747+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-03T11:26:11.748+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:26:11.749+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py'][0m
[[34m2023-12-03T11:26:12.304+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py[0m
[[34m2023-12-03T11:26:12.405+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-03T11:26:13.143+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-03T11:26:13.145+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:26:12.442416+00:00, run_end_date=2023-12-03 04:26:12.905070+00:00, run_duration=0.462654, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=29, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 04:26:11.747319+00:00, queued_by_job_id=1, pid=108842[0m
[[34m2023-12-03T11:26:13.159+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T11:26:13.174+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun my_first_dag @ 2023-12-03 03:46:07.288425+00:00: scheduled__2023-12-03T03:46:07.288425+00:00, state:running, queued_at: 2023-12-03 04:16:08.045102+00:00. externally triggered: False> failed[0m
[[34m2023-12-03T11:26:13.174+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 03:46:07.288425+00:00, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, run_start_date=2023-12-03 04:16:08.051610+00:00, run_end_date=2023-12-03 04:26:13.174658+00:00, run_duration=605.123048, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 03:46:07.288425+00:00, data_interval_end=2023-12-03 04:16:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d[0m
[[34m2023-12-03T11:26:13.176+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for my_first_dag to 2023-12-03T04:16:07.288425+00:00, run_after=2023-12-03T04:46:07.288425+00:00[0m
[[34m2023-12-03T11:31:13.183+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-03T11:36:13.220+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
