[[34m2023-12-17T08:36:30.441+0700[0m] {[34mscheduler_job_runner.py:[0m797} INFO[0m - Starting the scheduler[0m
[[34m2023-12-17T08:36:30.442+0700[0m] {[34mscheduler_job_runner.py:[0m804} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-12-17T08:36:30.445+0700[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 324561[0m
[[34m2023-12-17T08:36:30.446+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T08:36:30.448+0700[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-12-17T08:36:30.458+0700] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-12-17T08:40:28.504+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.crawl_careerlink manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:40:28.504+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:40:28.505+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.crawl_careerlink manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:40:28.506+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved[0m
[[34m2023-12-17T08:40:28.506+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2023-12-17T08:40:28.506+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'crawl_careerlink', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:40:28.508+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'crawl_careerlink', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:40:29.054+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/create_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=careerlink_pipeline_1/run_id=manual__2023-12-17T01:40:27.983434+00:00/task_id=crawl_careerlink permission to 509
[[34m2023-12-17T08:40:29.149+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.crawl_careerlink manual__2023-12-17T01:40:27.983434+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:41:23.072+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:41:23.076+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=crawl_careerlink, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:40:29.179082+00:00, run_end_date=2023-12-17 01:41:22.839851+00:00, run_duration=53.660769, state=success, executor_state=success, try_number=1, max_tries=3, job_id=3, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 01:40:28.505436+00:00, queued_by_job_id=2, pid=325934[0m
[[34m2023-12-17T08:41:23.087+0700[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=324561) last sent a heartbeat 54.62 seconds ago! Restarting it[0m
[[34m2023-12-17T08:41:23.092+0700[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 324561. PIDs of all processes in the group: [324561][0m
[[34m2023-12-17T08:41:23.092+0700[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 324561[0m
[[34m2023-12-17T08:41:23.184+0700[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=324561, status='terminated', exitcode=0, started='08:36:30') (324561) terminated with exit code 0[0m
[[34m2023-12-17T08:41:23.188+0700[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 326436[0m
[[34m2023-12-17T08:41:23.191+0700[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-12-17T08:41:23.203+0700] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-12-17T08:41:23.365+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:41:23.366+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:41:23.366+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:41:23.366+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T08:41:23.367+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T08:41:23.367+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:41:23.369+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:41:23.883+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/create_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=careerlink_pipeline_1/run_id=manual__2023-12-17T01:40:27.983434+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T08:41:23.975+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:40:27.983434+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:41:24.309+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:41:24.311+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:41:24.006535+00:00, run_end_date=2023-12-17 01:41:24.074785+00:00, run_duration=0.06825, state=success, executor_state=success, try_number=1, max_tries=3, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:41:23.366488+00:00, queued_by_job_id=2, pid=326454[0m
[[34m2023-12-17T08:41:24.460+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:41:24.460+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:41:24.460+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:41:24.461+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T08:41:24.461+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:41:24.461+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:41:24.463+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:41:24.980+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/create_dag.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=careerlink_pipeline_1/run_id=manual__2023-12-17T01:40:27.983434+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T08:41:25.075+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:41:25.457+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:41:25.460+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:41:25.106653+00:00, run_end_date=2023-12-17 01:41:25.238709+00:00, run_duration=0.132056, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:41:24.461099+00:00, queued_by_job_id=2, pid=326472[0m
[[34m2023-12-17T08:41:30.491+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T08:41:58.543+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for career_link_pipepline_test to 2023-12-17T01:41:54.568691+00:00, run_after=2023-12-17T21:41:54.568691+00:00[0m
[[34m2023-12-17T08:41:58.572+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: career_link_pipepline_test.check_file_exists scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.check_file_exists manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>[0m
[[34m2023-12-17T08:41:58.572+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG career_link_pipepline_test has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:41:58.573+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG career_link_pipepline_test has 1/16 running and queued tasks[0m
[[34m2023-12-17T08:41:58.573+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test.check_file_exists scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.check_file_exists manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>[0m
[[34m2023-12-17T08:41:58.574+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T08:41:58.574+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T08:41:58.574+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:41:58.574+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:41:58.574+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:41:58.574+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:41:58.576+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:41:59.104+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/career_link.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=career_link_pipepline_test/run_id=scheduled__2023-12-16T05:41:54.568691+00:00/task_id=check_file_exists permission to 509
[[34m2023-12-17T08:41:59.208+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: career_link_pipepline_test.check_file_exists scheduled__2023-12-16T05:41:54.568691+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:41:59.543+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:42:00.071+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/career_link.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=career_link_pipepline_test/run_id=manual__2023-12-17T01:41:57.842040+00:00/task_id=check_file_exists permission to 509
[[34m2023-12-17T08:42:00.168+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: career_link_pipepline_test.check_file_exists manual__2023-12-17T01:41:57.842040+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:42:00.508+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:42:00.508+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:42:00.512+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=check_file_exists, run_id=manual__2023-12-17T01:41:57.842040+00:00, map_index=-1, run_start_date=2023-12-17 01:42:00.199716+00:00, run_end_date=2023-12-17 01:42:00.275337+00:00, run_duration=0.075621, state=success, executor_state=success, try_number=1, max_tries=3, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:41:58.573380+00:00, queued_by_job_id=2, pid=326834[0m
[[34m2023-12-17T08:42:00.512+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=check_file_exists, run_id=scheduled__2023-12-16T05:41:54.568691+00:00, map_index=-1, run_start_date=2023-12-17 01:41:59.242025+00:00, run_end_date=2023-12-17 01:41:59.311729+00:00, run_duration=0.069704, state=success, executor_state=success, try_number=1, max_tries=3, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:41:58.573380+00:00, queued_by_job_id=2, pid=326817[0m
[[34m2023-12-17T08:42:00.554+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>[0m
[[34m2023-12-17T08:42:00.554+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG career_link_pipepline_test has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:42:00.555+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG career_link_pipepline_test has 1/16 running and queued tasks[0m
[[34m2023-12-17T08:42:00.555+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>[0m
[[34m2023-12-17T08:42:00.555+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T08:42:00.555+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T08:42:00.556+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-17T08:42:00.556+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:42:00.556+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-17T08:42:00.556+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:42:00.558+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:42:01.059+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/career_link.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=career_link_pipepline_test/run_id=scheduled__2023-12-16T05:41:54.568691+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T08:42:01.171+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: career_link_pipepline_test.clean_data_and_ingest scheduled__2023-12-16T05:41:54.568691+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:42:12.319+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py'][0m
[[34m2023-12-17T08:42:12.845+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/career_link.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=career_link_pipepline_test/run_id=manual__2023-12-17T01:41:57.842040+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T08:42:12.943+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: career_link_pipepline_test.clean_data_and_ingest manual__2023-12-17T01:41:57.842040+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:42:23.990+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:42:23.990+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:42:23.993+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:41:57.842040+00:00, map_index=-1, run_start_date=2023-12-17 01:42:12.968717+00:00, run_end_date=2023-12-17 01:42:23.770176+00:00, run_duration=10.801459, state=success, executor_state=success, try_number=1, max_tries=3, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:42:00.555371+00:00, queued_by_job_id=2, pid=327172[0m
[[34m2023-12-17T08:42:23.993+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T05:41:54.568691+00:00, map_index=-1, run_start_date=2023-12-17 01:42:01.202498+00:00, run_end_date=2023-12-17 01:42:12.104155+00:00, run_duration=10.901657, state=success, executor_state=success, try_number=1, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:42:00.555371+00:00, queued_by_job_id=2, pid=326836[0m
[[34m2023-12-17T08:42:24.149+0700[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun career_link_pipepline_test @ 2023-12-16 05:41:54.568691+00:00: scheduled__2023-12-16T05:41:54.568691+00:00, state:running, queued_at: 2023-12-17 01:41:58.538941+00:00. externally triggered: False> successful[0m
[[34m2023-12-17T08:42:24.150+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=career_link_pipepline_test, execution_date=2023-12-16 05:41:54.568691+00:00, run_id=scheduled__2023-12-16T05:41:54.568691+00:00, run_start_date=2023-12-17 01:41:58.555050+00:00, run_end_date=2023-12-17 01:42:24.150250+00:00, run_duration=25.5952, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 05:41:54.568691+00:00, data_interval_end=2023-12-17 01:41:54.568691+00:00, dag_hash=597b64aa6373d8a9b1b96bd8c81a1a31[0m
[[34m2023-12-17T08:42:24.152+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for career_link_pipepline_test to 2023-12-17T01:41:54.568691+00:00, run_after=2023-12-17T21:41:54.568691+00:00[0m
[[34m2023-12-17T08:42:24.154+0700[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun career_link_pipepline_test @ 2023-12-17 01:41:57.842040+00:00: manual__2023-12-17T01:41:57.842040+00:00, state:running, queued_at: 2023-12-17 01:41:57.849522+00:00. externally triggered: True> successful[0m
[[34m2023-12-17T08:42:24.154+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=career_link_pipepline_test, execution_date=2023-12-17 01:41:57.842040+00:00, run_id=manual__2023-12-17T01:41:57.842040+00:00, run_start_date=2023-12-17 01:41:58.555199+00:00, run_end_date=2023-12-17 01:42:24.154490+00:00, run_duration=25.599291, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 05:41:57.842040+00:00, data_interval_end=2023-12-17 01:41:57.842040+00:00, dag_hash=597b64aa6373d8a9b1b96bd8c81a1a31[0m
[[34m2023-12-17T08:42:25.299+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:42:25.299+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:42:25.299+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:42:25.300+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:42:25.300+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:42:25.302+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:42:25.836+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/create_dag.py[0m
[[34m2023-12-17T08:42:25.929+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:42:26.270+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-17T08:42:26.272+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:42:25.957564+00:00, run_end_date=2023-12-17 01:42:26.073915+00:00, run_duration=0.116351, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:42:25.299963+00:00, queued_by_job_id=2, pid=327523[0m
[[34m2023-12-17T08:43:26.807+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:43:26.807+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:43:26.807+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:43:26.808+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:43:26.809+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:43:26.810+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:43:27.277+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/create_dag.py[0m
[[34m2023-12-17T08:43:27.362+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:43:27.704+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-17T08:43:27.707+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:43:27.383736+00:00, run_end_date=2023-12-17 01:43:27.502164+00:00, run_duration=0.118428, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:43:26.807892+00:00, queued_by_job_id=2, pid=327788[0m
[[34m2023-12-17T08:44:28.406+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:44:28.406+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:44:28.406+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>[0m
[[34m2023-12-17T08:44:28.407+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:44:28.407+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:44:28.409+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py'][0m
[[34m2023-12-17T08:44:28.945+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/create_dag.py[0m
[[34m2023-12-17T08:44:29.016+0700[0m] {[34mcli.py:[0m241} WARNING[0m - Dag 'careerlink_pipeline_1' not found in path /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/create_dag.py; trying path /home/phuc/Practice/DataScience/DSProject/job_airflow/dags[0m
[[34m2023-12-17T08:44:29.017+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags[0m
[[34m2023-12-17T08:44:29.024+0700[0m] {[34mdagbag.py:[0m346} ERROR[0m - Failed to import: /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py", line 65, in <module>
    dag = create_custom_dag(dag_name=f"{source}_pipeline_1", schedule_interval=None, source=source)
  File "/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py", line 58, in create_custom_dag
    t0 >> t1 >> t2 >> t3
NameError: name 't0' is not defined
[[34m2023-12-17T08:44:29.165+0700[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']' returned non-zero exit status 1..[0m
[[34m2023-12-17T08:44:29.166+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=4, map_index=-1)[0m
[[34m2023-12-17T08:44:29.168+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:43:27.383736+00:00, run_end_date=2023-12-17 01:43:27.502164+00:00, run_duration=0.118428, state=queued, executor_state=failed, try_number=4, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:44:28.407107+00:00, queued_by_job_id=2, pid=327788[0m
[[34m2023-12-17T08:44:29.168+0700[0m] {[34mscheduler_job_runner.py:[0m770} ERROR[0m - Executor reports task instance <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2023-12-17T08:44:29.169+0700[0m] {[34mtaskinstance.py:[0m1939} ERROR[0m - Executor reports task instance <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2023-12-17T08:44:29.173+0700[0m] {[34mtaskinstance.py:[0m1400} INFO[0m - Marking task as FAILED. dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, execution_date=20231217T014027, start_date=20231217T014327, end_date=20231217T014429[0m
[[34m2023-12-17T08:44:30.195+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun careerlink_pipeline_1 @ 2023-12-17 01:40:27.983434+00:00: manual__2023-12-17T01:40:27.983434+00:00, state:running, queued_at: 2023-12-17 01:40:28.000845+00:00. externally triggered: True> failed[0m
[[34m2023-12-17T08:44:30.196+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=careerlink_pipeline_1, execution_date=2023-12-17 01:40:27.983434+00:00, run_id=manual__2023-12-17T01:40:27.983434+00:00, run_start_date=2023-12-17 01:40:28.479712+00:00, run_end_date=2023-12-17 01:44:30.196155+00:00, run_duration=241.716443, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:40:27.983434+00:00, data_interval_end=2023-12-17 01:40:27.983434+00:00, dag_hash=cf06b2cb530061a49cad8a491f61caa5[0m
[[34m2023-12-17T08:46:30.517+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T08:46:31.094+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:46:31.094+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:46:31.094+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:46:31.095+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T08:46:31.095+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T08:46:31.095+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:46:31.097+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:46:31.619+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=careerlink_pipeline_1/run_id=manual__2023-12-17T01:46:30.261380+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T08:46:31.720+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:46:30.261380+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:46:32.061+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:46:32.063+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:46:31.750943+00:00, run_end_date=2023-12-17 01:46:31.825422+00:00, run_duration=0.074479, state=success, executor_state=success, try_number=1, max_tries=3, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:46:31.094705+00:00, queued_by_job_id=2, pid=328775[0m
[[34m2023-12-17T08:46:32.101+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:46:32.101+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:46:32.101+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:46:32.102+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T08:46:32.102+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:46:32.102+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:46:32.104+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:46:32.625+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=careerlink_pipeline_1/run_id=manual__2023-12-17T01:46:30.261380+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T08:46:32.718+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:46:33.091+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:46:33.093+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:46:32.748939+00:00, run_end_date=2023-12-17 01:46:32.883962+00:00, run_duration=0.135023, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:46:32.101991+00:00, queued_by_job_id=2, pid=328777[0m
[[34m2023-12-17T08:47:33.683+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:47:33.683+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:47:33.683+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:47:33.684+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:47:33.684+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:47:33.686+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:47:34.181+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T08:47:34.275+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:47:34.630+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-17T08:47:34.632+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:47:34.303214+00:00, run_end_date=2023-12-17 01:47:34.423180+00:00, run_duration=0.119966, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:47:33.684052+00:00, queued_by_job_id=2, pid=329401[0m
[[34m2023-12-17T08:48:35.276+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:48:35.276+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG careerlink_pipeline_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:48:35.276+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>[0m
[[34m2023-12-17T08:48:35.277+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:48:35.277+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:48:35.279+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T08:48:35.798+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T08:48:35.892+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:48:36.264+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-17T08:48:36.267+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:48:35.921740+00:00, run_end_date=2023-12-17 01:48:36.045393+00:00, run_duration=0.123653, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:48:35.276764+00:00, queued_by_job_id=2, pid=329989[0m
[[34m2023-12-17T08:49:13.608+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun careerlink_pipeline_1 @ 2023-12-17 01:46:30.261380+00:00: manual__2023-12-17T01:46:30.261380+00:00, state:running, queued_at: 2023-12-17 01:46:30.267393+00:00. externally triggered: True> failed[0m
[[34m2023-12-17T08:49:13.608+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=careerlink_pipeline_1, execution_date=2023-12-17 01:46:30.261380+00:00, run_id=manual__2023-12-17T01:46:30.261380+00:00, run_start_date=2023-12-17 01:46:31.075056+00:00, run_end_date=2023-12-17 01:49:13.608489+00:00, run_duration=162.533433, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:46:30.261380+00:00, data_interval_end=2023-12-17 01:46:30.261380+00:00, dag_hash=d2ad1b986cbd90150590f88c9edf1d36[0m
[[34m2023-12-17T08:51:30.564+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T08:52:52.018+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:52:52.019+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:52:52.019+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:52:52.019+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T08:52:52.020+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T08:52:52.020+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:52:52.022+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:52:52.539+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=sjvlksdnv/run_id=manual__2023-12-17T01:52:50.516305+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T08:52:52.632+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:52:50.516305+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:52:52.978+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:52:52.980+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:52:52.662184+00:00, run_end_date=2023-12-17 01:52:52.730208+00:00, run_duration=0.068024, state=success, executor_state=success, try_number=1, max_tries=3, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:52:52.019375+00:00, queued_by_job_id=2, pid=331454[0m
[[34m2023-12-17T08:52:53.014+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:52:53.014+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:52:53.014+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:52:53.015+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T08:52:53.015+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:52:53.015+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:52:53.017+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:52:53.530+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=sjvlksdnv/run_id=manual__2023-12-17T01:52:50.516305+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T08:52:53.627+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:52:54.009+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:52:54.012+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:52:53.657195+00:00, run_end_date=2023-12-17 01:52:53.789899+00:00, run_duration=0.132704, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:52:53.014786+00:00, queued_by_job_id=2, pid=331472[0m
[[34m2023-12-17T08:53:54.711+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:53:54.711+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:53:54.711+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:53:54.712+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:53:54.712+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:53:54.714+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:53:55.212+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
[[34m2023-12-17T08:53:55.313+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:53:55.709+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-17T08:53:55.712+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:53:55.344680+00:00, run_end_date=2023-12-17 01:53:55.482778+00:00, run_duration=0.138098, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:53:54.711730+00:00, queued_by_job_id=2, pid=332087[0m
[[34m2023-12-17T08:54:55.526+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:54:55.527+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:54:55.527+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>[0m
[[34m2023-12-17T08:54:55.528+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:54:55.528+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:54:55.529+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:54:56.086+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
[[34m2023-12-17T08:54:56.171+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:54:56.513+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-17T08:54:56.516+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:54:56.192643+00:00, run_end_date=2023-12-17 01:54:56.312515+00:00, run_duration=0.119872, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:54:55.527493+00:00, queued_by_job_id=2, pid=332673[0m
[[34m2023-12-17T08:55:13.529+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for sjvlksdnv to 2023-12-17T01:55:12.291602+00:00, run_after=2023-12-17T21:55:12.291602+00:00[0m
[[34m2023-12-17T08:55:13.564+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.check_daily_file_exists scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>[0m
[[34m2023-12-17T08:55:13.564+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:55:13.565+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.check_daily_file_exists scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>[0m
[[34m2023-12-17T08:55:13.565+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T08:55:13.566+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T08:55:13.566+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:55:13.567+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:55:14.103+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=sjvlksdnv/run_id=scheduled__2023-12-16T05:55:12.291602+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T08:55:14.200+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.check_daily_file_exists scheduled__2023-12-16T05:55:12.291602+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:55:14.507+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:55:14.510+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=check_daily_file_exists, run_id=scheduled__2023-12-16T05:55:12.291602+00:00, map_index=-1, run_start_date=2023-12-17 01:55:14.233290+00:00, run_end_date=2023-12-17 01:55:14.297791+00:00, run_duration=0.064501, state=success, executor_state=success, try_number=1, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:55:13.565278+00:00, queued_by_job_id=2, pid=332847[0m
[[34m2023-12-17T08:55:14.549+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>[0m
[[34m2023-12-17T08:55:14.549+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:55:14.549+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>[0m
[[34m2023-12-17T08:55:14.550+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T08:55:14.550+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:55:14.550+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:55:14.552+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:55:15.062+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=sjvlksdnv/run_id=scheduled__2023-12-16T05:55:12.291602+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T08:55:15.149+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.clean_data_and_ingest scheduled__2023-12-16T05:55:12.291602+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:55:15.478+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:55:15.480+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T05:55:12.291602+00:00, map_index=-1, run_start_date=2023-12-17 01:55:15.171734+00:00, run_end_date=2023-12-17 01:55:15.298811+00:00, run_duration=0.127077, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:55:14.549782+00:00, queued_by_job_id=2, pid=332864[0m
[2023-12-17T08:55:27.358+0700] {manager.py:543} INFO - DAG careerlink_pipeline_1 is missing and will be deactivated.
[2023-12-17T08:55:27.360+0700] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2023-12-17T08:55:27.368+0700] {manager.py:557} INFO - Deleted DAG careerlink_pipeline_1 in serialized_dag table
[[34m2023-12-17T08:56:06.121+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun sjvlksdnv @ 2023-12-17 01:52:50.516305+00:00: manual__2023-12-17T01:52:50.516305+00:00, state:running, queued_at: 2023-12-17 01:52:50.524300+00:00. externally triggered: True> failed[0m
[[34m2023-12-17T08:56:06.122+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=sjvlksdnv, execution_date=2023-12-17 01:52:50.516305+00:00, run_id=manual__2023-12-17T01:52:50.516305+00:00, run_start_date=2023-12-17 01:52:51.999844+00:00, run_end_date=2023-12-17 01:56:06.122202+00:00, run_duration=194.122358, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:52:50.516305+00:00, data_interval_end=2023-12-17 01:52:50.516305+00:00, dag_hash=0a633e910157b1637b704117f925587c[0m
[[34m2023-12-17T08:56:13.089+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun sjvlksdnv @ 2023-12-16 05:55:12.291602+00:00: scheduled__2023-12-16T05:55:12.291602+00:00, state:running, queued_at: 2023-12-17 01:55:13.524145+00:00. externally triggered: False> failed[0m
[[34m2023-12-17T08:56:13.089+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=sjvlksdnv, execution_date=2023-12-16 05:55:12.291602+00:00, run_id=scheduled__2023-12-16T05:55:12.291602+00:00, run_start_date=2023-12-17 01:55:13.545479+00:00, run_end_date=2023-12-17 01:56:13.089515+00:00, run_duration=59.544036, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 05:55:12.291602+00:00, data_interval_end=2023-12-17 01:55:12.291602+00:00, dag_hash=0a633e910157b1637b704117f925587c[0m
[[34m2023-12-17T08:56:13.091+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for sjvlksdnv to 2023-12-17T01:55:12.291602+00:00, run_after=2023-12-17T21:55:12.291602+00:00[0m
[[34m2023-12-17T08:56:30.606+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T08:57:13.636+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:57:13.636+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:57:13.637+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:57:13.637+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T08:57:13.637+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T08:57:13.638+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:57:13.639+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:57:14.159+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=sjvlksdnv/run_id=manual__2023-12-17T01:57:12.724097+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T08:57:14.252+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:57:12.724097+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:57:14.589+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:57:14.592+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:57:14.282016+00:00, run_end_date=2023-12-17 01:57:14.353320+00:00, run_duration=0.071304, state=success, executor_state=success, try_number=1, max_tries=3, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:57:13.637262+00:00, queued_by_job_id=2, pid=333893[0m
[[34m2023-12-17T08:57:14.631+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:57:14.631+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:57:14.631+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:57:14.632+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T08:57:14.632+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:57:14.632+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:57:14.634+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:57:15.143+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=sjvlksdnv/run_id=manual__2023-12-17T01:57:12.724097+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T08:57:15.238+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:57:15.671+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:57:15.674+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:57:15.270450+00:00, run_end_date=2023-12-17 01:57:15.410911+00:00, run_duration=0.140461, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:57:14.631995+00:00, queued_by_job_id=2, pid=333895[0m
[2023-12-17T08:57:27.609+0700] {manager.py:543} INFO - DAG tét is missing and will be deactivated.
[2023-12-17T08:57:27.611+0700] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2023-12-17T08:57:27.613+0700] {manager.py:557} INFO - Deleted DAG tét in serialized_dag table
[[34m2023-12-17T08:59:05.875+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:59:05.875+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:59:05.875+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:59:05.876+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T08:59:05.876+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:59:05.878+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:59:06.399+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
[[34m2023-12-17T08:59:06.496+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:59:19.560+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-17T08:59:19.562+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:59:06.528542+00:00, run_end_date=2023-12-17 01:59:19.369853+00:00, run_duration=12.841311, state=success, executor_state=success, try_number=2, max_tries=4, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:59:05.875860+00:00, queued_by_job_id=2, pid=334976[0m
[[34m2023-12-17T08:59:19.610+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:59:19.610+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T08:59:19.610+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T08:59:19.611+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upsert_to_warehouse because previous state change time has not been saved[0m
[[34m2023-12-17T08:59:19.611+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-17T08:59:19.611+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:59:19.613+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T08:59:20.108+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=sjvlksdnv/run_id=manual__2023-12-17T01:57:12.724097+00:00/task_id=upsert_to_warehouse permission to 509
[[34m2023-12-17T08:59:20.194+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T08:59:27.202+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T08:59:27.205+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:59:20.216010+00:00, run_end_date=2023-12-17 01:59:26.992309+00:00, run_duration=6.776299, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:59:19.610880+00:00, queued_by_job_id=2, pid=335388[0m
[[34m2023-12-17T09:01:30.639+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:02:27.315+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T09:02:27.315+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:02:27.315+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T09:02:27.316+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-17T09:02:27.316+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T09:02:27.318+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T09:02:27.780+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
[[34m2023-12-17T09:02:27.864+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:02:34.845+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-17T09:02:34.847+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 02:02:27.885892+00:00, run_end_date=2023-12-17 02:02:34.644731+00:00, run_duration=6.758839, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 02:02:27.316120+00:00, queued_by_job_id=2, pid=336959[0m
[[34m2023-12-17T09:05:34.962+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T09:05:34.962+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:05:34.962+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T09:05:34.963+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=3, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-17T09:05:34.963+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T09:05:34.968+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T09:05:35.481+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
[[34m2023-12-17T09:05:35.576+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:05:43.312+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-17T09:05:43.314+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 02:05:35.604030+00:00, run_end_date=2023-12-17 02:05:43.137001+00:00, run_duration=7.532971, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 02:05:34.963174+00:00, queued_by_job_id=2, pid=338922[0m
[[34m2023-12-17T09:06:30.677+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:08:43.427+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T09:08:43.427+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG sjvlksdnv has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:08:43.427+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>[0m
[[34m2023-12-17T09:08:43.429+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=4, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-17T09:08:43.429+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T09:08:43.431+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py'][0m
[[34m2023-12-17T09:08:43.905+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/sjdkj.py[0m
[[34m2023-12-17T09:08:43.989+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:08:51.156+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=4, map_index=-1)[0m
[[34m2023-12-17T09:08:51.159+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 02:08:44.011019+00:00, run_end_date=2023-12-17 02:08:50.965787+00:00, run_duration=6.954768, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 02:08:43.428257+00:00, queued_by_job_id=2, pid=340976[0m
[[34m2023-12-17T09:08:51.327+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun sjvlksdnv @ 2023-12-17 01:57:12.724097+00:00: manual__2023-12-17T01:57:12.724097+00:00, state:running, queued_at: 2023-12-17 01:57:12.729023+00:00. externally triggered: True> failed[0m
[[34m2023-12-17T09:08:51.328+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=sjvlksdnv, execution_date=2023-12-17 01:57:12.724097+00:00, run_id=manual__2023-12-17T01:57:12.724097+00:00, run_start_date=2023-12-17 01:57:13.616197+00:00, run_end_date=2023-12-17 02:08:51.328069+00:00, run_duration=697.711872, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 05:57:12.724097+00:00, data_interval_end=2023-12-17 01:57:12.724097+00:00, dag_hash=0a633e910157b1637b704117f925587c[0m
[[34m2023-12-17T09:11:30.706+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:16:30.749+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:21:30.787+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:26:30.831+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:31:30.842+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:33:37.379+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for test_1 to 2023-12-17T02:33:21.397961+00:00, run_after=2023-12-17T22:33:21.397961+00:00[0m
[[34m2023-12-17T09:33:37.410+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_1.crawl_careerlink scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:33:37.410+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:33:37.410+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 1/16 running and queued tasks[0m
[[34m2023-12-17T09:33:37.410+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.crawl_careerlink scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:33:37.411+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved[0m
[[34m2023-12-17T09:33:37.411+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved[0m
[[34m2023-12-17T09:33:37.411+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2023-12-17T09:33:37.411+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:33:37.411+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2023-12-17T09:33:37.411+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:33:37.413+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:33:37.939+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=scheduled__2023-12-16T06:33:21.397961+00:00/task_id=crawl_careerlink permission to 509
[[34m2023-12-17T09:33:38.036+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.crawl_careerlink scheduled__2023-12-16T06:33:21.397961+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:34:20.950+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:34:21.455+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=manual__2023-12-17T02:33:36.360825+00:00/task_id=crawl_careerlink permission to 509
[[34m2023-12-17T09:34:21.547+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.crawl_careerlink manual__2023-12-17T02:33:36.360825+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:35:08.519+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T09:35:08.519+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T09:35:08.522+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=crawl_careerlink, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:34:21.581415+00:00, run_end_date=2023-12-17 02:35:08.300836+00:00, run_duration=46.719421, state=success, executor_state=success, try_number=1, max_tries=3, job_id=30, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 02:33:37.410829+00:00, queued_by_job_id=2, pid=348372[0m
[[34m2023-12-17T09:35:08.522+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=crawl_careerlink, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:33:38.071056+00:00, run_end_date=2023-12-17 02:34:20.719136+00:00, run_duration=42.64808, state=success, executor_state=success, try_number=1, max_tries=3, job_id=29, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 02:33:37.410829+00:00, queued_by_job_id=2, pid=347948[0m
[[34m2023-12-17T09:35:08.533+0700[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=326436) last sent a heartbeat 91.17 seconds ago! Restarting it[0m
[[34m2023-12-17T09:35:08.538+0700[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 326436. PIDs of all processes in the group: [326436][0m
[[34m2023-12-17T09:35:08.538+0700[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 326436[0m
[[34m2023-12-17T09:35:08.630+0700[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=326436, status='terminated', exitcode=0, started='08:41:22') (326436) terminated with exit code 0[0m
[[34m2023-12-17T09:35:08.634+0700[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 348798[0m
[[34m2023-12-17T09:35:08.638+0700[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-12-17T09:35:08.652+0700] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-12-17T09:35:08.801+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_1.check_daily_file_exists scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:35:08.802+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:35:08.802+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 1/16 running and queued tasks[0m
[[34m2023-12-17T09:35:08.802+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.check_daily_file_exists scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:35:08.803+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T09:35:08.803+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T09:35:08.803+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T09:35:08.803+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:08.804+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T09:35:08.804+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:08.806+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:09.314+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=scheduled__2023-12-16T06:33:21.397961+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T09:35:09.406+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.check_daily_file_exists scheduled__2023-12-16T06:33:21.397961+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:35:09.750+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:10.272+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=manual__2023-12-17T02:33:36.360825+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T09:35:10.363+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T02:33:36.360825+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:35:10.716+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T09:35:10.716+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T09:35:10.719+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:35:10.397455+00:00, run_end_date=2023-12-17 02:35:10.467227+00:00, run_duration=0.069772, state=success, executor_state=success, try_number=1, max_tries=3, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 02:35:08.802673+00:00, queued_by_job_id=2, pid=348849[0m
[[34m2023-12-17T09:35:10.719+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=check_daily_file_exists, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:35:09.440855+00:00, run_end_date=2023-12-17 02:35:09.510975+00:00, run_duration=0.07012, state=success, executor_state=success, try_number=1, max_tries=3, job_id=31, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 02:35:08.802673+00:00, queued_by_job_id=2, pid=348816[0m
[[34m2023-12-17T09:35:10.869+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:35:10.869+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:35:10.869+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 1/16 running and queued tasks[0m
[[34m2023-12-17T09:35:10.869+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:35:10.870+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T09:35:10.870+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T09:35:10.870+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:35:10.870+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:10.871+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:35:10.871+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:10.872+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:11.380+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=scheduled__2023-12-16T06:33:21.397961+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T09:35:11.472+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:35:20.576+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:35:21.093+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=manual__2023-12-17T02:33:36.360825+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T09:35:21.187+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:35:32.416+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T09:35:32.417+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T09:35:32.419+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:35:21.220964+00:00, run_end_date=2023-12-17 02:35:32.150118+00:00, run_duration=10.929154, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:35:10.870086+00:00, queued_by_job_id=2, pid=349152[0m
[[34m2023-12-17T09:35:32.420+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:35:11.506750+00:00, run_end_date=2023-12-17 02:35:20.325226+00:00, run_duration=8.818476, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=33, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:35:10.870086+00:00, queued_by_job_id=2, pid=348852[0m
[[34m2023-12-17T09:36:30.890+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:38:20.554+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>[0m
[[34m2023-12-17T09:38:20.554+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:38:20.554+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>[0m
[[34m2023-12-17T09:38:20.555+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:38:20.555+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:38:20.557+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:38:21.032+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T09:38:21.126+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:38:30.018+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-17T09:38:30.021+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:38:21.159421+00:00, run_end_date=2023-12-17 02:38:29.746575+00:00, run_duration=8.587154, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=35, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:38:20.555192+00:00, queued_by_job_id=2, pid=351096[0m
[[34m2023-12-17T09:38:32.969+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:38:32.969+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:38:32.970+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:38:32.970+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:38:32.971+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:38:32.972+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:38:33.565+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T09:38:33.682+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:38:42.419+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=2, map_index=-1)[0m
[[34m2023-12-17T09:38:42.422+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:38:33.719466+00:00, run_end_date=2023-12-17 02:38:42.193953+00:00, run_duration=8.474487, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=36, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:38:32.970298+00:00, queued_by_job_id=2, pid=351410[0m
[[34m2023-12-17T09:41:29.947+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>[0m
[[34m2023-12-17T09:41:29.947+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:41:29.947+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>[0m
[[34m2023-12-17T09:41:29.948+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:41:29.948+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:41:29.950+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:41:30.433+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T09:41:30.520+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:41:34.669+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-17T09:41:34.671+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:41:30.547356+00:00, run_end_date=2023-12-17 02:41:34.492000+00:00, run_duration=3.944644, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=37, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:41:29.947895+00:00, queued_by_job_id=2, pid=353584[0m
[[34m2023-12-17T09:41:34.691+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:41:42.868+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:41:42.868+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:41:42.868+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:41:42.869+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:41:42.869+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:41:42.871+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:41:43.348+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T09:41:43.437+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:41:47.594+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=3, map_index=-1)[0m
[[34m2023-12-17T09:41:47.597+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:41:43.465366+00:00, run_end_date=2023-12-17 02:41:47.394243+00:00, run_duration=3.928877, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=38, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:41:42.868830+00:00, queued_by_job_id=2, pid=353938[0m
[[34m2023-12-17T09:44:35.009+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>[0m
[[34m2023-12-17T09:44:35.010+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:44:35.010+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>[0m
[[34m2023-12-17T09:44:35.010+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:44:35.011+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:44:35.012+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:44:35.480+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T09:44:35.566+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:44:39.963+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=4, map_index=-1)[0m
[[34m2023-12-17T09:44:39.966+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:44:35.594061+00:00, run_end_date=2023-12-17 02:44:39.751597+00:00, run_duration=4.157536, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=39, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:44:35.010445+00:00, queued_by_job_id=2, pid=355485[0m
[[34m2023-12-17T09:44:39.999+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun test_1 @ 2023-12-16 06:33:21.397961+00:00: scheduled__2023-12-16T06:33:21.397961+00:00, state:running, queued_at: 2023-12-17 02:33:37.374817+00:00. externally triggered: False> failed[0m
[[34m2023-12-17T09:44:40.000+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=test_1, execution_date=2023-12-16 06:33:21.397961+00:00, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, run_start_date=2023-12-17 02:33:37.392843+00:00, run_end_date=2023-12-17 02:44:40.000050+00:00, run_duration=662.607207, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 06:33:21.397961+00:00, data_interval_end=2023-12-17 02:33:21.397961+00:00, dag_hash=6cbc0fa2fc22135dbb147534e184480a[0m
[[34m2023-12-17T09:44:40.002+0700[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for test_1 to 2023-12-17T02:33:21.397961+00:00, run_after=2023-12-17T22:33:21.397961+00:00[0m
[[34m2023-12-17T09:44:48.141+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:44:48.141+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T09:44:48.141+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>[0m
[[34m2023-12-17T09:44:48.142+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T09:44:48.142+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:44:48.145+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T09:44:48.619+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
[[34m2023-12-17T09:44:48.709+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T09:44:52.758+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=4, map_index=-1)[0m
[[34m2023-12-17T09:44:52.761+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:44:48.734557+00:00, run_end_date=2023-12-17 02:44:52.539894+00:00, run_duration=3.805337, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=40, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:44:48.142040+00:00, queued_by_job_id=2, pid=355702[0m
[[34m2023-12-17T09:44:52.901+0700[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun test_1 @ 2023-12-17 02:33:36.360825+00:00: manual__2023-12-17T02:33:36.360825+00:00, state:running, queued_at: 2023-12-17 02:33:36.367976+00:00. externally triggered: True> failed[0m
[[34m2023-12-17T09:44:52.901+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=test_1, execution_date=2023-12-17 02:33:36.360825+00:00, run_id=manual__2023-12-17T02:33:36.360825+00:00, run_start_date=2023-12-17 02:33:37.392986+00:00, run_end_date=2023-12-17 02:44:52.901467+00:00, run_duration=675.508481, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 06:33:36.360825+00:00, data_interval_end=2023-12-17 02:33:36.360825+00:00, dag_hash=6cbc0fa2fc22135dbb147534e184480a[0m
[[34m2023-12-17T09:46:34.718+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:51:34.744+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T09:56:34.785+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T10:01:14.077+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:01:14.077+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T10:01:14.077+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:01:14.078+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved[0m
[[34m2023-12-17T10:01:14.078+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2023-12-17T10:01:14.078+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:01:14.080+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:01:14.625+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=manual__2023-12-17T03:01:13.587963+00:00/task_id=crawl_careerlink permission to 509
[[34m2023-12-17T10:01:14.724+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.crawl_careerlink manual__2023-12-17T03:01:13.587963+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T10:01:49.677+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T10:01:49.680+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=crawl_careerlink, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:01:14.759131+00:00, run_end_date=2023-12-17 03:01:49.430547+00:00, run_duration=34.671416, state=success, executor_state=success, try_number=1, max_tries=3, job_id=41, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 03:01:14.077834+00:00, queued_by_job_id=2, pid=361088[0m
[[34m2023-12-17T10:01:49.707+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T10:01:49.710+0700[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2023-12-17T10:01:49.839+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:01:49.839+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T10:01:49.840+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:01:49.840+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved[0m
[[34m2023-12-17T10:01:49.841+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-12-17T10:01:49.841+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:01:49.843+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:01:50.353+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=manual__2023-12-17T03:01:13.587963+00:00/task_id=check_daily_file_exists permission to 509
[[34m2023-12-17T10:01:50.444+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T03:01:13.587963+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T10:01:50.795+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T10:01:50.798+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:01:50.478232+00:00, run_end_date=2023-12-17 03:01:50.554505+00:00, run_duration=0.076273, state=success, executor_state=success, try_number=1, max_tries=3, job_id=42, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 03:01:49.840348+00:00, queued_by_job_id=2, pid=361445[0m
[[34m2023-12-17T10:01:50.945+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:01:50.945+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T10:01:50.945+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:01:50.946+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved[0m
[[34m2023-12-17T10:01:50.946+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-12-17T10:01:50.946+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:01:50.948+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:01:51.455+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=manual__2023-12-17T03:01:13.587963+00:00/task_id=clean_data_and_ingest permission to 509
[[34m2023-12-17T10:01:51.552+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T03:01:13.587963+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T10:02:02.399+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T10:02:02.402+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:01:51.586343+00:00, run_end_date=2023-12-17 03:02:02.185495+00:00, run_duration=10.599152, state=success, executor_state=success, try_number=1, max_tries=3, job_id=43, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 03:01:50.945823+00:00, queued_by_job_id=2, pid=361463[0m
[[34m2023-12-17T10:02:02.549+0700[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_1.upsert_to_warehouse manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:02:02.549+0700[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG test_1 has 0/16 running and queued tasks[0m
[[34m2023-12-17T10:02:02.549+0700[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_1.upsert_to_warehouse manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>[0m
[[34m2023-12-17T10:02:02.550+0700[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upsert_to_warehouse because previous state change time has not been saved[0m
[[34m2023-12-17T10:02:02.550+0700[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='test_1', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-12-17T10:02:02.550+0700[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'upsert_to_warehouse', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:02:02.552+0700[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'upsert_to_warehouse', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py'][0m
[[34m2023-12-17T10:02:03.075+0700[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/phuc/Practice/DataScience/DSProject/job_airflow/dags/pipeline_dags.py[0m
Changing /home/phuc/Practice/DataScience/DSProject/job_airflow/logs/dag_id=test_1/run_id=manual__2023-12-17T03:01:13.587963+00:00/task_id=upsert_to_warehouse permission to 509
[[34m2023-12-17T10:02:03.167+0700[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: test_1.upsert_to_warehouse manual__2023-12-17T03:01:13.587963+00:00 [queued]> on host phuc-ASUS[0m
[[34m2023-12-17T10:02:15.037+0700[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)[0m
[[34m2023-12-17T10:02:15.039+0700[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=test_1, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:02:03.201423+00:00, run_end_date=2023-12-17 03:02:14.812392+00:00, run_duration=11.610969, state=success, executor_state=success, try_number=1, max_tries=3, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 03:02:02.549828+00:00, queued_by_job_id=2, pid=361820[0m
[[34m2023-12-17T10:02:15.071+0700[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun test_1 @ 2023-12-17 03:01:13.587963+00:00: manual__2023-12-17T03:01:13.587963+00:00, state:running, queued_at: 2023-12-17 03:01:13.594697+00:00. externally triggered: True> successful[0m
[[34m2023-12-17T10:02:15.071+0700[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=test_1, execution_date=2023-12-17 03:01:13.587963+00:00, run_id=manual__2023-12-17T03:01:13.587963+00:00, run_start_date=2023-12-17 03:01:14.060180+00:00, run_end_date=2023-12-17 03:02:15.071894+00:00, run_duration=61.011714, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 07:01:13.587963+00:00, data_interval_end=2023-12-17 03:01:13.587963+00:00, dag_hash=6cbc0fa2fc22135dbb147534e184480a[0m
[[34m2023-12-17T10:06:49.733+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T10:11:49.757+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T10:16:49.788+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T10:21:49.815+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-12-17T10:26:49.847+0700[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
