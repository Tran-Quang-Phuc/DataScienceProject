2023-12-02 13:18:10,500 INFO - Starting the scheduler
2023-12-02 13:18:10,500 INFO - Processing each file at most -1 times
2023-12-02 13:18:10,503 INFO - Launched DagFileProcessorManager with pid: 80807
2023-12-02 13:18:10,504 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:18:10,506 INFO - Configured default timezone Timezone('UTC')
2023-12-02 13:23:10,767 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:26:44,967 ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: job.id

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 844, in _execute
    self._run_scheduler_loop()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 985, in _run_scheduler_loop
    perform_heartbeat(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 350, in perform_heartbeat
    job.heartbeat(heartbeat_callback=heartbeat_callback, session=session)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 211, in heartbeat
    session.commit()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: job.id
[SQL: INSERT INTO job (id, dag_id, state, job_type, start_date, end_date, latest_heartbeat, executor_class, hostname, unixname) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ((1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:18:10.476553', None, '2023-12-02 06:26:39.723838', None, 'phuc-ASUS', 'phuc'), (1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:18:10.476553', None, '2023-12-02 06:26:39.723838', None, 'phuc-ASUS', 'phuc'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2023-12-02 13:26:45,980 INFO - Sending Signals.SIGTERM to group 80807. PIDs of all processes in the group: [80807]
2023-12-02 13:26:45,980 INFO - Sending the signal Signals.SIGTERM to group 80807
2023-12-02 13:26:46,113 INFO - Process psutil.Process(pid=80807, status='terminated', exitcode=0, started='13:18:10') (80807) terminated with exit code 0
2023-12-02 13:26:46,114 INFO - Exited execute loop
2023-12-02 13:26:46,123 ERROR - Exception when running scheduler job
Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: job.id

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 47, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 289, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 318, in execute_job
    ret = execute_callable()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 844, in _execute
    self._run_scheduler_loop()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 985, in _run_scheduler_loop
    perform_heartbeat(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 350, in perform_heartbeat
    job.heartbeat(heartbeat_callback=heartbeat_callback, session=session)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 211, in heartbeat
    session.commit()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: job.id
[SQL: INSERT INTO job (id, dag_id, state, job_type, start_date, end_date, latest_heartbeat, executor_class, hostname, unixname) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ((1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:18:10.476553', None, '2023-12-02 06:26:39.723838', None, 'phuc-ASUS', 'phuc'), (1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:18:10.476553', None, '2023-12-02 06:26:39.723838', None, 'phuc-ASUS', 'phuc'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2023-12-02 13:28:16,301 INFO - Starting the scheduler
2023-12-02 13:28:16,302 INFO - Processing each file at most -1 times
2023-12-02 13:28:16,306 INFO - Launched DagFileProcessorManager with pid: 84381
2023-12-02 13:28:16,307 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:28:16,310 INFO - Configured default timezone Timezone('UTC')
2023-12-02 13:33:16,464 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:38:16,711 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:38:33,909 INFO - Setting next_dagrun for my_first_dag to 2023-12-02T06:38:16.584995+00:00, run_after=2023-12-02T07:08:16.584995+00:00
2023-12-02 13:38:33,960 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.say_my_name scheduled__2023-12-02T06:08:16.584995+00:00 [scheduled]>
2023-12-02 13:38:33,960 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 13:38:33,960 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.say_my_name scheduled__2023-12-02T06:08:16.584995+00:00 [scheduled]>
2023-12-02 13:38:33,962 WARNING - cannot record scheduled_duration for task say_my_name because previous state change time has not been saved
2023-12-02 13:38:33,963 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='scheduled__2023-12-02T06:08:16.584995+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 13:38:33,963 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'scheduled__2023-12-02T06:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:33,966 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'scheduled__2023-12-02T06:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:35,397 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='scheduled__2023-12-02T06:08:16.584995+00:00', try_number=1, map_index=-1)
2023-12-02 13:38:35,404 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=say_my_name, run_id=scheduled__2023-12-02T06:08:16.584995+00:00, map_index=-1, run_start_date=2023-12-02 06:38:34.979512+00:00, run_end_date=2023-12-02 06:38:35.096693+00:00, run_duration=0.117181, state=success, executor_state=success, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-02 06:38:33.961313+00:00, queued_by_job_id=2, pid=86739
2023-12-02 13:38:35,454 INFO - 2 tasks up for execution:
	<TaskInstance: my_first_dag.say_my_name manual__2023-12-02T06:38:33.111543+00:00 [scheduled]>
	<TaskInstance: my_first_dag.multiply_my_number_by_23 scheduled__2023-12-02T06:08:16.584995+00:00 [scheduled]>
2023-12-02 13:38:35,454 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 13:38:35,454 INFO - DAG my_first_dag has 1/16 running and queued tasks
2023-12-02 13:38:35,454 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.say_my_name manual__2023-12-02T06:38:33.111543+00:00 [scheduled]>
	<TaskInstance: my_first_dag.multiply_my_number_by_23 scheduled__2023-12-02T06:08:16.584995+00:00 [scheduled]>
2023-12-02 13:38:35,457 WARNING - cannot record scheduled_duration for task say_my_name because previous state change time has not been saved
2023-12-02 13:38:35,457 WARNING - cannot record scheduled_duration for task multiply_my_number_by_23 because previous state change time has not been saved
2023-12-02 13:38:35,457 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='manual__2023-12-02T06:38:33.111543+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 13:38:35,457 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'manual__2023-12-02T06:38:33.111543+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:35,458 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='scheduled__2023-12-02T06:08:16.584995+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 13:38:35,458 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'scheduled__2023-12-02T06:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:35,460 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'manual__2023-12-02T06:38:33.111543+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:36,825 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'scheduled__2023-12-02T06:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:38,135 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='manual__2023-12-02T06:38:33.111543+00:00', try_number=1, map_index=-1)
2023-12-02 13:38:38,136 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='scheduled__2023-12-02T06:08:16.584995+00:00', try_number=1, map_index=-1)
2023-12-02 13:38:38,143 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=multiply_my_number_by_23, run_id=scheduled__2023-12-02T06:08:16.584995+00:00, map_index=-1, run_start_date=2023-12-02 06:38:37.769418+00:00, run_end_date=2023-12-02 06:38:37.869319+00:00, run_duration=0.099901, state=success, executor_state=success, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 06:38:35.455573+00:00, queued_by_job_id=2, pid=86745
2023-12-02 13:38:38,143 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=say_my_name, run_id=manual__2023-12-02T06:38:33.111543+00:00, map_index=-1, run_start_date=2023-12-02 06:38:36.412285+00:00, run_end_date=2023-12-02 06:38:36.520439+00:00, run_duration=0.108154, state=success, executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-02 06:38:35.455573+00:00, queued_by_job_id=2, pid=86742
2023-12-02 13:38:38,184 INFO - Marking run <DagRun my_first_dag @ 2023-12-02 06:08:16.584995+00:00: scheduled__2023-12-02T06:08:16.584995+00:00, state:running, queued_at: 2023-12-02 06:38:33.901129+00:00. externally triggered: False> successful
2023-12-02 13:38:38,185 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 06:08:16.584995+00:00, run_id=scheduled__2023-12-02T06:08:16.584995+00:00, run_start_date=2023-12-02 06:38:33.919364+00:00, run_end_date=2023-12-02 06:38:38.185454+00:00, run_duration=4.26609, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-02 06:08:16.584995+00:00, data_interval_end=2023-12-02 06:38:16.584995+00:00, dag_hash=035ca8d2f297029d450f40652ef8ce8e
2023-12-02 13:38:38,187 INFO - Setting next_dagrun for my_first_dag to 2023-12-02T06:38:16.584995+00:00, run_after=2023-12-02T07:08:16.584995+00:00
2023-12-02 13:38:38,195 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.multiply_my_number_by_23 manual__2023-12-02T06:38:33.111543+00:00 [scheduled]>
2023-12-02 13:38:38,196 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 13:38:38,196 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.multiply_my_number_by_23 manual__2023-12-02T06:38:33.111543+00:00 [scheduled]>
2023-12-02 13:38:38,197 WARNING - cannot record scheduled_duration for task multiply_my_number_by_23 because previous state change time has not been saved
2023-12-02 13:38:38,198 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='manual__2023-12-02T06:38:33.111543+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 13:38:38,198 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'manual__2023-12-02T06:38:33.111543+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:38,200 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'manual__2023-12-02T06:38:33.111543+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:39,521 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='manual__2023-12-02T06:38:33.111543+00:00', try_number=1, map_index=-1)
2023-12-02 13:38:39,525 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=multiply_my_number_by_23, run_id=manual__2023-12-02T06:38:33.111543+00:00, map_index=-1, run_start_date=2023-12-02 06:38:39.181278+00:00, run_end_date=2023-12-02 06:38:39.278074+00:00, run_duration=0.096796, state=success, executor_state=success, try_number=1, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 06:38:38.196543+00:00, queued_by_job_id=2, pid=86764
2023-12-02 13:38:39,552 INFO - Marking run <DagRun my_first_dag @ 2023-12-02 06:38:33.111543+00:00: manual__2023-12-02T06:38:33.111543+00:00, state:running, queued_at: 2023-12-02 06:38:33.137459+00:00. externally triggered: True> successful
2023-12-02 13:38:39,553 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 06:38:33.111543+00:00, run_id=manual__2023-12-02T06:38:33.111543+00:00, run_start_date=2023-12-02 06:38:35.426370+00:00, run_end_date=2023-12-02 06:38:39.553211+00:00, run_duration=4.126841, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 06:08:33.111543+00:00, data_interval_end=2023-12-02 06:38:33.111543+00:00, dag_hash=035ca8d2f297029d450f40652ef8ce8e
2023-12-02 13:38:43,196 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.say_my_name manual__2023-12-02T06:38:42.985937+00:00 [scheduled]>
2023-12-02 13:38:43,196 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 13:38:43,196 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.say_my_name manual__2023-12-02T06:38:42.985937+00:00 [scheduled]>
2023-12-02 13:38:43,197 WARNING - cannot record scheduled_duration for task say_my_name because previous state change time has not been saved
2023-12-02 13:38:43,198 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='manual__2023-12-02T06:38:42.985937+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 13:38:43,198 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'manual__2023-12-02T06:38:42.985937+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:43,200 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'manual__2023-12-02T06:38:42.985937+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:44,778 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='manual__2023-12-02T06:38:42.985937+00:00', try_number=1, map_index=-1)
2023-12-02 13:38:44,782 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=say_my_name, run_id=manual__2023-12-02T06:38:42.985937+00:00, map_index=-1, run_start_date=2023-12-02 06:38:44.356466+00:00, run_end_date=2023-12-02 06:38:44.476617+00:00, run_duration=0.120151, state=success, executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-02 06:38:43.196894+00:00, queued_by_job_id=2, pid=86782
2023-12-02 13:38:44,834 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.multiply_my_number_by_23 manual__2023-12-02T06:38:42.985937+00:00 [scheduled]>
2023-12-02 13:38:44,834 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 13:38:44,834 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.multiply_my_number_by_23 manual__2023-12-02T06:38:42.985937+00:00 [scheduled]>
2023-12-02 13:38:44,836 WARNING - cannot record scheduled_duration for task multiply_my_number_by_23 because previous state change time has not been saved
2023-12-02 13:38:44,836 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='manual__2023-12-02T06:38:42.985937+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 13:38:44,836 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'manual__2023-12-02T06:38:42.985937+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:44,839 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'manual__2023-12-02T06:38:42.985937+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 13:38:46,127 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='manual__2023-12-02T06:38:42.985937+00:00', try_number=1, map_index=-1)
2023-12-02 13:38:46,131 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=multiply_my_number_by_23, run_id=manual__2023-12-02T06:38:42.985937+00:00, map_index=-1, run_start_date=2023-12-02 06:38:45.781057+00:00, run_end_date=2023-12-02 06:38:45.872543+00:00, run_duration=0.091486, state=success, executor_state=success, try_number=1, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 06:38:44.835268+00:00, queued_by_job_id=2, pid=86785
2023-12-02 13:38:46,159 INFO - Marking run <DagRun my_first_dag @ 2023-12-02 06:38:42.985937+00:00: manual__2023-12-02T06:38:42.985937+00:00, state:running, queued_at: 2023-12-02 06:38:42.997425+00:00. externally triggered: True> successful
2023-12-02 13:38:46,160 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 06:38:42.985937+00:00, run_id=manual__2023-12-02T06:38:42.985937+00:00, run_start_date=2023-12-02 06:38:43.179419+00:00, run_end_date=2023-12-02 06:38:46.160124+00:00, run_duration=2.980705, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 06:08:42.985937+00:00, data_interval_end=2023-12-02 06:38:42.985937+00:00, dag_hash=035ca8d2f297029d450f40652ef8ce8e
2023-12-02 13:43:16,739 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:48:16,779 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:53:16,817 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 13:58:16,859 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:00:14,915 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.say_my_name manual__2023-12-02T07:00:13.362115+00:00 [scheduled]>
2023-12-02 14:00:14,915 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:00:14,915 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.say_my_name manual__2023-12-02T07:00:13.362115+00:00 [scheduled]>
2023-12-02 14:00:14,916 WARNING - cannot record scheduled_duration for task say_my_name because previous state change time has not been saved
2023-12-02 14:00:14,916 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='manual__2023-12-02T07:00:13.362115+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-02 14:00:14,916 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'manual__2023-12-02T07:00:13.362115+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:00:14,918 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'say_my_name', 'manual__2023-12-02T07:00:13.362115+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:00:15,961 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='say_my_name', run_id='manual__2023-12-02T07:00:13.362115+00:00', try_number=1, map_index=-1)
2023-12-02 14:00:15,964 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=say_my_name, run_id=manual__2023-12-02T07:00:13.362115+00:00, map_index=-1, run_start_date=2023-12-02 07:00:15.642565+00:00, run_end_date=2023-12-02 07:00:15.720123+00:00, run_duration=0.077558, state=success, executor_state=success, try_number=1, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-02 07:00:14.915996+00:00, queued_by_job_id=2, pid=92465
2023-12-02 14:00:15,997 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.multiply_my_number_by_23 manual__2023-12-02T07:00:13.362115+00:00 [scheduled]>
2023-12-02 14:00:15,998 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:00:15,998 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.multiply_my_number_by_23 manual__2023-12-02T07:00:13.362115+00:00 [scheduled]>
2023-12-02 14:00:15,998 WARNING - cannot record scheduled_duration for task multiply_my_number_by_23 because previous state change time has not been saved
2023-12-02 14:00:15,999 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='manual__2023-12-02T07:00:13.362115+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:00:15,999 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'manual__2023-12-02T07:00:13.362115+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:00:16,004 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'multiply_my_number_by_23', 'manual__2023-12-02T07:00:13.362115+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:00:17,051 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='multiply_my_number_by_23', run_id='manual__2023-12-02T07:00:13.362115+00:00', try_number=1, map_index=-1)
2023-12-02 14:00:17,054 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=multiply_my_number_by_23, run_id=manual__2023-12-02T07:00:13.362115+00:00, map_index=-1, run_start_date=2023-12-02 07:00:16.732682+00:00, run_end_date=2023-12-02 07:00:16.801583+00:00, run_duration=0.068901, state=success, executor_state=success, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-02 07:00:15.998396+00:00, queued_by_job_id=2, pid=92469
2023-12-02 14:00:17,092 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.write_into_deltalake manual__2023-12-02T07:00:13.362115+00:00 [scheduled]>
2023-12-02 14:00:17,092 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:00:17,092 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.write_into_deltalake manual__2023-12-02T07:00:13.362115+00:00 [scheduled]>
2023-12-02 14:00:17,093 WARNING - cannot record scheduled_duration for task write_into_deltalake because previous state change time has not been saved
2023-12-02 14:00:17,093 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='write_into_deltalake', run_id='manual__2023-12-02T07:00:13.362115+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 14:00:17,093 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'write_into_deltalake', 'manual__2023-12-02T07:00:13.362115+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:00:17,095 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'write_into_deltalake', 'manual__2023-12-02T07:00:13.362115+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:00:28,656 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='write_into_deltalake', run_id='manual__2023-12-02T07:00:13.362115+00:00', try_number=1, map_index=-1)
2023-12-02 14:00:28,659 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=write_into_deltalake, run_id=manual__2023-12-02T07:00:13.362115+00:00, map_index=-1, run_start_date=2023-12-02 07:00:17.801985+00:00, run_end_date=2023-12-02 07:00:28.403966+00:00, run_duration=10.601981, state=success, executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 07:00:17.092996+00:00, queued_by_job_id=2, pid=92471
2023-12-02 14:00:28,700 INFO - Marking run <DagRun my_first_dag @ 2023-12-02 07:00:13.362115+00:00: manual__2023-12-02T07:00:13.362115+00:00, state:running, queued_at: 2023-12-02 07:00:13.367842+00:00. externally triggered: True> successful
2023-12-02 14:00:28,700 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 07:00:13.362115+00:00, run_id=manual__2023-12-02T07:00:13.362115+00:00, run_start_date=2023-12-02 07:00:14.896623+00:00, run_end_date=2023-12-02 07:00:28.700316+00:00, run_duration=13.803693, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 06:30:13.362115+00:00, data_interval_end=2023-12-02 07:00:13.362115+00:00, dag_hash=f88629856af590f3fb5cb0a47879a5bc
2023-12-02 14:03:16,906 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:06:04,513 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:06:03.176940+00:00 [scheduled]>
2023-12-02 14:06:04,513 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:06:04,513 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:06:03.176940+00:00 [scheduled]>
2023-12-02 14:06:04,514 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 14:06:04,514 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:06:03.176940+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-02 14:06:04,514 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:06:03.176940+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:06:04,516 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:06:03.176940+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:06:05,924 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:06:03.176940+00:00', try_number=1, map_index=-1)
2023-12-02 14:06:05,927 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-02T07:06:03.176940+00:00, map_index=-1, run_start_date=2023-12-02 07:06:05.264918+00:00, run_end_date=2023-12-02 07:06:05.652815+00:00, run_duration=0.387897, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-02 07:06:04.514014+00:00, queued_by_job_id=2, pid=94063
2023-12-02 14:06:14,819 ERROR - Failed to get task for ti <TaskInstance: my_first_dag.multiply_my_number_by_23 manual__2023-12-02T07:06:03.176940+00:00 [None]>. Marking it as removed.
2023-12-02 14:08:16,957 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:08:17,512 INFO - Setting next_dagrun for my_first_dag to 2023-12-02T07:08:16.584995+00:00, run_after=2023-12-02T07:38:16.584995+00:00
2023-12-02 14:08:17,537 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T06:38:16.584995+00:00 [scheduled]>
2023-12-02 14:08:17,537 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:08:17,537 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T06:38:16.584995+00:00 [scheduled]>
2023-12-02 14:08:17,538 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 14:08:17,538 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T06:38:16.584995+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:08:17,538 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T06:38:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:08:17,540 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T06:38:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:08:18,909 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T06:38:16.584995+00:00', try_number=1, map_index=-1)
2023-12-02 14:08:18,912 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-02T06:38:16.584995+00:00, map_index=-1, run_start_date=2023-12-02 07:08:18.255529+00:00, run_end_date=2023-12-02 07:08:18.670946+00:00, run_duration=0.415417, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-02 07:08:17.537831+00:00, queued_by_job_id=2, pid=94638
2023-12-02 14:09:00,383 ERROR - Marking run <DagRun my_first_dag @ 2023-12-02 07:06:03.176940+00:00: manual__2023-12-02T07:06:03.176940+00:00, state:running, queued_at: 2023-12-02 07:06:03.181524+00:00. externally triggered: True> failed
2023-12-02 14:09:00,384 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 07:06:03.176940+00:00, run_id=manual__2023-12-02T07:06:03.176940+00:00, run_start_date=2023-12-02 07:06:04.489413+00:00, run_end_date=2023-12-02 07:09:00.384168+00:00, run_duration=175.894755, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 06:36:03.176940+00:00, data_interval_end=2023-12-02 07:06:03.176940+00:00, dag_hash=ee40491f8e269b4dfefd61c8e6ebebf2
2023-12-02 14:09:08,438 ERROR - Marking run <DagRun my_first_dag @ 2023-12-02 06:38:16.584995+00:00: scheduled__2023-12-02T06:38:16.584995+00:00, state:running, queued_at: 2023-12-02 07:08:17.511196+00:00. externally triggered: False> failed
2023-12-02 14:09:08,438 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 06:38:16.584995+00:00, run_id=scheduled__2023-12-02T06:38:16.584995+00:00, run_start_date=2023-12-02 07:08:17.523429+00:00, run_end_date=2023-12-02 07:09:08.438289+00:00, run_duration=50.91486, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-02 06:38:16.584995+00:00, data_interval_end=2023-12-02 07:08:16.584995+00:00, dag_hash=ee40491f8e269b4dfefd61c8e6ebebf2
2023-12-02 14:09:08,439 INFO - Setting next_dagrun for my_first_dag to 2023-12-02T07:08:16.584995+00:00, run_after=2023-12-02T07:38:16.584995+00:00
2023-12-02 14:13:16,994 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:14:28,868 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:14:27.277652+00:00 [scheduled]>
2023-12-02 14:14:28,868 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:14:28,868 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:14:27.277652+00:00 [scheduled]>
2023-12-02 14:14:28,869 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 14:14:28,869 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:14:27.277652+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:14:28,869 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:14:27.277652+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:14:28,871 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:14:27.277652+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:14:30,456 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:14:27.277652+00:00', try_number=1, map_index=-1)
2023-12-02 14:14:30,459 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-02T07:14:27.277652+00:00, map_index=-1, run_start_date=2023-12-02 07:14:29.752341+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-02 07:14:28.868827+00:00, queued_by_job_id=2, pid=96174
2023-12-02 14:14:32,313 WARNING - Failing (1) jobs without heartbeat after 2023-12-02 07:09:32.309378+00:00
2023-12-02 14:14:32,313 ERROR - Detected zombie job: {'full_filepath': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py', 'processor_subdir': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags', 'msg': "{'DAG Id': 'my_first_dag', 'Task Id': 'crawl_data', 'Run Id': 'manual__2023-12-02T07:14:27.277652+00:00', 'Hostname': 'phuc-ASUS'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7f32918cc520>, 'is_failure_callback': True}
2023-12-02 14:15:27,926 ERROR - Marking run <DagRun my_first_dag @ 2023-12-02 07:14:27.277652+00:00: manual__2023-12-02T07:14:27.277652+00:00, state:running, queued_at: 2023-12-02 07:14:27.284250+00:00. externally triggered: True> failed
2023-12-02 14:15:27,926 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 07:14:27.277652+00:00, run_id=manual__2023-12-02T07:14:27.277652+00:00, run_start_date=2023-12-02 07:14:28.848322+00:00, run_end_date=2023-12-02 07:15:27.926876+00:00, run_duration=59.078554, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 06:44:27.277652+00:00, data_interval_end=2023-12-02 07:14:27.277652+00:00, dag_hash=39b75f4c34ff79ef57c3f54ce150b281
2023-12-02 14:18:17,043 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:23:17,051 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:23:19,639 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:23:18.658016+00:00 [scheduled]>
2023-12-02 14:23:19,640 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:23:19,640 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:23:18.658016+00:00 [scheduled]>
2023-12-02 14:23:19,640 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 14:23:19,641 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:23:18.658016+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:23:19,641 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:23:18.658016+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:23:19,642 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:23:18.658016+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:23:21,274 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:23:18.658016+00:00', try_number=1, map_index=-1)
2023-12-02 14:23:21,277 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-02T07:23:18.658016+00:00, map_index=-1, run_start_date=2023-12-02 07:23:20.571676+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-02 07:23:19.640344+00:00, queued_by_job_id=2, pid=98425
2023-12-02 14:23:23,995 WARNING - Failing (1) jobs without heartbeat after 2023-12-02 07:18:23.992575+00:00
2023-12-02 14:23:23,995 ERROR - Detected zombie job: {'full_filepath': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py', 'processor_subdir': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags', 'msg': "{'DAG Id': 'my_first_dag', 'Task Id': 'crawl_data', 'Run Id': 'manual__2023-12-02T07:23:18.658016+00:00', 'Hostname': 'phuc-ASUS'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7f328ef5c730>, 'is_failure_callback': True}
2023-12-02 14:28:17,106 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:28:25,075 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:23:18.658016+00:00 [scheduled]>
2023-12-02 14:28:25,076 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:28:25,076 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:23:18.658016+00:00 [scheduled]>
2023-12-02 14:28:25,077 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:23:18.658016+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:28:25,077 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:23:18.658016+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:28:25,079 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:23:18.658016+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:28:26,746 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:23:18.658016+00:00', try_number=2, map_index=-1)
2023-12-02 14:28:26,749 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-02T07:23:18.658016+00:00, map_index=-1, run_start_date=2023-12-02 07:28:25.990485+00:00, run_end_date=None, run_duration=3.780907, state=running, executor_state=success, try_number=2, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-02 07:28:25.076422+00:00, queued_by_job_id=2, pid=99556
2023-12-02 14:28:26,762 WARNING - Failing (1) jobs without heartbeat after 2023-12-02 07:23:26.760446+00:00
2023-12-02 14:28:26,763 ERROR - Detected zombie job: {'full_filepath': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py', 'processor_subdir': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags', 'msg': "{'DAG Id': 'my_first_dag', 'Task Id': 'crawl_data', 'Run Id': 'manual__2023-12-02T07:23:18.658016+00:00', 'Hostname': 'phuc-ASUS'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7f328ee5beb0>, 'is_failure_callback': True}
2023-12-02 14:33:17,148 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:33:27,295 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:23:18.658016+00:00 [scheduled]>
2023-12-02 14:33:27,295 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:33:27,295 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T07:23:18.658016+00:00 [scheduled]>
2023-12-02 14:33:27,296 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:23:18.658016+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:33:27,297 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:23:18.658016+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:33:27,299 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T07:23:18.658016+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:33:28,737 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T07:23:18.658016+00:00', try_number=3, map_index=-1)
2023-12-02 14:33:28,739 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-02T07:23:18.658016+00:00, map_index=-1, run_start_date=2023-12-02 07:33:28.116989+00:00, run_end_date=None, run_duration=1.173765, state=running, executor_state=success, try_number=3, max_tries=2, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-02 07:33:27.296261+00:00, queued_by_job_id=2, pid=100632
2023-12-02 14:33:28,752 WARNING - Failing (1) jobs without heartbeat after 2023-12-02 07:28:28.750189+00:00
2023-12-02 14:33:28,752 ERROR - Detected zombie job: {'full_filepath': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py', 'processor_subdir': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags', 'msg': "{'DAG Id': 'my_first_dag', 'Task Id': 'crawl_data', 'Run Id': 'manual__2023-12-02T07:23:18.658016+00:00', 'Hostname': 'phuc-ASUS'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7f328ef8ca90>, 'is_failure_callback': True}
2023-12-02 14:33:30,203 ERROR - Marking run <DagRun my_first_dag @ 2023-12-02 07:23:18.658016+00:00: manual__2023-12-02T07:23:18.658016+00:00, state:running, queued_at: 2023-12-02 07:23:18.663911+00:00. externally triggered: True> failed
2023-12-02 14:33:30,203 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 07:23:18.658016+00:00, run_id=manual__2023-12-02T07:23:18.658016+00:00, run_start_date=2023-12-02 07:23:19.620252+00:00, run_end_date=2023-12-02 07:33:30.203715+00:00, run_duration=610.583463, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 06:53:18.658016+00:00, data_interval_end=2023-12-02 07:23:18.658016+00:00, dag_hash=39b75f4c34ff79ef57c3f54ce150b281
2023-12-02 14:38:17,177 INFO - Setting next_dagrun for my_first_dag to 2023-12-02T07:38:16.584995+00:00, run_after=2023-12-02T08:08:16.584995+00:00
2023-12-02 14:38:17,210 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T07:08:16.584995+00:00 [scheduled]>
2023-12-02 14:38:17,211 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:38:17,211 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T07:08:16.584995+00:00 [scheduled]>
2023-12-02 14:38:17,212 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 14:38:17,212 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T07:08:16.584995+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:38:17,212 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T07:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:38:17,214 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T07:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:38:18,866 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T07:08:16.584995+00:00', try_number=1, map_index=-1)
2023-12-02 14:38:18,869 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-02T07:08:16.584995+00:00, map_index=-1, run_start_date=2023-12-02 07:38:18.189185+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-02 07:38:17.211574+00:00, queued_by_job_id=2, pid=101628
2023-12-02 14:38:18,880 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:38:19,782 WARNING - Failing (1) jobs without heartbeat after 2023-12-02 07:33:19.778026+00:00
2023-12-02 14:38:19,782 ERROR - Detected zombie job: {'full_filepath': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py', 'processor_subdir': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags', 'msg': "{'DAG Id': 'my_first_dag', 'Task Id': 'crawl_data', 'Run Id': 'scheduled__2023-12-02T07:08:16.584995+00:00', 'Hostname': 'phuc-ASUS'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7f328ee4faf0>, 'is_failure_callback': True}
2023-12-02 14:43:18,917 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:43:20,995 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T07:08:16.584995+00:00 [scheduled]>
2023-12-02 14:43:20,995 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 14:43:20,996 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T07:08:16.584995+00:00 [scheduled]>
2023-12-02 14:43:20,997 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T07:08:16.584995+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-02 14:43:20,997 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T07:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:43:20,999 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T07:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 14:43:22,316 ERROR - Failed to execute task Command '['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T07:08:16.584995+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']' returned non-zero exit status 1..
2023-12-02 14:43:22,317 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T07:08:16.584995+00:00', try_number=2, map_index=-1)
2023-12-02 14:43:22,320 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-02T07:08:16.584995+00:00, map_index=-1, run_start_date=2023-12-02 07:38:18.189185+00:00, run_end_date=2023-12-02 07:38:20.171923+00:00, run_duration=1.982738, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-02 07:43:20.996418+00:00, queued_by_job_id=2, pid=101628
2023-12-02 14:43:22,320 ERROR - Executor reports task instance <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T07:08:16.584995+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
2023-12-02 14:43:22,320 ERROR - Executor reports task instance <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T07:08:16.584995+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
2023-12-02 14:43:22,328 INFO - Marking task as UP_FOR_RETRY. dag_id=my_first_dag, task_id=crawl_data, execution_date=20231202T070816, start_date=20231202T073818, end_date=20231202T074322
2023-12-02 14:48:18,957 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:53:18,973 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 14:58:19,006 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 15:03:19,043 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 15:08:19,079 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 15:13:19,125 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 15:18:19,168 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 15:23:19,219 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 16:17:03,715 ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: job.id

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 844, in _execute
    self._run_scheduler_loop()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 985, in _run_scheduler_loop
    perform_heartbeat(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 350, in perform_heartbeat
    job.heartbeat(heartbeat_callback=heartbeat_callback, session=session)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 211, in heartbeat
    session.commit()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: job.id
[SQL: INSERT INTO job (id, dag_id, state, job_type, start_date, end_date, latest_heartbeat, executor_class, hostname, unixname) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ((2, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:28:16.264908', None, '2023-12-02 09:16:58.659362', None, 'phuc-ASUS', 'phuc'), (2, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:28:16.264908', None, '2023-12-02 09:16:58.659362', None, 'phuc-ASUS', 'phuc'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2023-12-02 16:17:04,738 INFO - Sending Signals.SIGTERM to group 84381. PIDs of all processes in the group: [84381]
2023-12-02 16:17:04,738 INFO - Sending the signal Signals.SIGTERM to group 84381
2023-12-02 16:17:04,871 INFO - Process psutil.Process(pid=84381, status='terminated', exitcode=0, started='13:28:16') (84381) terminated with exit code 0
2023-12-02 16:17:04,872 INFO - Exited execute loop
2023-12-02 16:17:04,881 ERROR - Exception when running scheduler job
Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: job.id

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 47, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 289, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 318, in execute_job
    ret = execute_callable()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 844, in _execute
    self._run_scheduler_loop()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 985, in _run_scheduler_loop
    perform_heartbeat(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 350, in perform_heartbeat
    job.heartbeat(heartbeat_callback=heartbeat_callback, session=session)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 211, in heartbeat
    session.commit()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: job.id
[SQL: INSERT INTO job (id, dag_id, state, job_type, start_date, end_date, latest_heartbeat, executor_class, hostname, unixname) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ((2, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:28:16.264908', None, '2023-12-02 09:16:58.659362', None, 'phuc-ASUS', 'phuc'), (2, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-02 06:28:16.264908', None, '2023-12-02 09:16:58.659362', None, 'phuc-ASUS', 'phuc'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2023-12-02 16:20:32,719 INFO - Starting the scheduler
2023-12-02 16:20:32,720 INFO - Processing each file at most -1 times
2023-12-02 16:20:32,724 INFO - Launched DagFileProcessorManager with pid: 113471
2023-12-02 16:20:32,726 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 16:20:32,728 INFO - Configured default timezone Timezone('UTC')
2023-12-02 16:21:31,395 INFO - Setting next_dagrun for my_first_dag to 2023-12-02T09:21:07.091166+00:00, run_after=2023-12-02T09:51:07.091166+00:00
2023-12-02 16:21:31,445 INFO - 6 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.write_into_deltalake scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_folder manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
	<TaskInstance: my_first_dag.write_into_deltalake manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
2023-12-02 16:21:31,446 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 16:21:31,446 INFO - DAG my_first_dag has 1/16 running and queued tasks
2023-12-02 16:21:31,446 INFO - DAG my_first_dag has 2/16 running and queued tasks
2023-12-02 16:21:31,446 INFO - DAG my_first_dag has 3/16 running and queued tasks
2023-12-02 16:21:31,446 INFO - DAG my_first_dag has 4/16 running and queued tasks
2023-12-02 16:21:31,446 INFO - DAG my_first_dag has 5/16 running and queued tasks
2023-12-02 16:21:31,446 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.write_into_deltalake scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_folder manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
	<TaskInstance: my_first_dag.write_into_deltalake manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
2023-12-02 16:21:31,449 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-02 16:21:31,449 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 16:21:31,449 WARNING - cannot record scheduled_duration for task write_into_deltalake because previous state change time has not been saved
2023-12-02 16:21:31,449 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-02 16:21:31,449 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 16:21:31,449 WARNING - cannot record scheduled_duration for task write_into_deltalake because previous state change time has not been saved
2023-12-02 16:21:31,450 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:21:31,450 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:31,450 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:21:31,451 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:31,451 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='write_into_deltalake', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:21:31,451 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'write_into_deltalake', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:31,451 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:21:31,451 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:31,451 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:21:31,451 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:31,451 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='write_into_deltalake', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:21:31,451 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'write_into_deltalake', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:31,454 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:33,331 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:35,615 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'write_into_deltalake', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:43,356 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:45,147 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:47,435 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'write_into_deltalake', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:21:55,165 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1)
2023-12-02 16:21:55,166 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1)
2023-12-02 16:21:55,166 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='write_into_deltalake', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1)
2023-12-02 16:21:55,166 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1)
2023-12-02 16:21:55,166 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1)
2023-12-02 16:21:55,166 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='write_into_deltalake', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1)
2023-12-02 16:21:55,173 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-02T08:51:07.091166+00:00, map_index=-1, run_start_date=2023-12-02 09:21:34.682661+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 09:21:31.447351+00:00, queued_by_job_id=1, pid=113873
2023-12-02 16:21:55,173 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-02T08:51:07.091166+00:00, map_index=-1, run_start_date=2023-12-02 09:21:32.834618+00:00, run_end_date=2023-12-02 09:21:32.952337+00:00, run_duration=0.117719, state=success, executor_state=success, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-02 09:21:31.447351+00:00, queued_by_job_id=1, pid=113854
2023-12-02 16:21:55,173 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=write_into_deltalake, run_id=scheduled__2023-12-02T08:51:07.091166+00:00, map_index=-1, run_start_date=2023-12-02 09:21:36.908187+00:00, run_end_date=2023-12-02 09:21:42.955730+00:00, run_duration=6.047543, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 09:21:31.447351+00:00, queued_by_job_id=1, pid=113905
2023-12-02 16:21:55,174 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-02T09:21:30.376105+00:00, map_index=-1, run_start_date=2023-12-02 09:21:46.499919+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 09:21:31.447351+00:00, queued_by_job_id=1, pid=114183
2023-12-02 16:21:55,174 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=manual__2023-12-02T09:21:30.376105+00:00, map_index=-1, run_start_date=2023-12-02 09:21:44.669181+00:00, run_end_date=2023-12-02 09:21:44.781452+00:00, run_duration=0.112271, state=success, executor_state=success, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-02 09:21:31.447351+00:00, queued_by_job_id=1, pid=114165
2023-12-02 16:21:55,174 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=write_into_deltalake, run_id=manual__2023-12-02T09:21:30.376105+00:00, map_index=-1, run_start_date=2023-12-02 09:21:48.784120+00:00, run_end_date=2023-12-02 09:21:54.754813+00:00, run_duration=5.970693, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-12-02 09:21:31.447351+00:00, queued_by_job_id=1, pid=114218
2023-12-02 16:21:55,209 WARNING - Failing (2) jobs without heartbeat after 2023-12-02 09:16:55.206296+00:00
2023-12-02 16:21:55,209 ERROR - Detected zombie job: {'full_filepath': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py', 'processor_subdir': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags', 'msg': "{'DAG Id': 'my_first_dag', 'Task Id': 'crawl_data', 'Run Id': 'manual__2023-12-02T09:21:30.376105+00:00', 'Hostname': 'phuc-ASUS'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff7befe1420>, 'is_failure_callback': True}
2023-12-02 16:21:55,210 ERROR - Detected zombie job: {'full_filepath': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags/test_dag.py', 'processor_subdir': '/home/phuc/Practice/DataScience/DSProject/job_airflow/dags', 'msg': "{'DAG Id': 'my_first_dag', 'Task Id': 'crawl_data', 'Run Id': 'scheduled__2023-12-02T08:51:07.091166+00:00', 'Hostname': 'phuc-ASUS'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff7bac46350>, 'is_failure_callback': True}
2023-12-02 16:25:20,994 ERROR - Failed to get task for ti <TaskInstance: my_first_dag.crawl_data scheduled__2023-12-02T08:51:07.091166+00:00 [failed]>. Marking it as removed.
2023-12-02 16:25:21,004 ERROR - Failed to get task for ti <TaskInstance: my_first_dag.crawl_data manual__2023-12-02T09:21:30.376105+00:00 [failed]>. Marking it as removed.
2023-12-02 16:25:32,888 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 16:26:23,102 ERROR - Failed to get task for ti <TaskInstance: my_first_dag.write_into_deltalake scheduled__2023-12-02T08:51:07.091166+00:00 [up_for_retry]>. Marking it as removed.
2023-12-02 16:26:23,109 ERROR - Failed to get task for ti <TaskInstance: my_first_dag.write_into_deltalake manual__2023-12-02T09:21:30.376105+00:00 [up_for_retry]>. Marking it as removed.
2023-12-02 16:26:23,122 INFO - 2 tasks up for execution:
	<TaskInstance: my_first_dag.print_name scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_name manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
2023-12-02 16:26:23,122 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 16:26:23,123 INFO - DAG my_first_dag has 1/16 running and queued tasks
2023-12-02 16:26:23,123 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_name scheduled__2023-12-02T08:51:07.091166+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_name manual__2023-12-02T09:21:30.376105+00:00 [scheduled]>
2023-12-02 16:26:23,124 WARNING - cannot record scheduled_duration for task print_name because previous state change time has not been saved
2023-12-02 16:26:23,124 WARNING - cannot record scheduled_duration for task print_name because previous state change time has not been saved
2023-12-02 16:26:23,125 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_name', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:26:23,125 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_name', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:26:23,125 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_name', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:26:23,125 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_name', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:26:23,127 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_name', 'scheduled__2023-12-02T08:51:07.091166+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:26:25,069 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_name', 'manual__2023-12-02T09:21:30.376105+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:26:26,759 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_name', run_id='scheduled__2023-12-02T08:51:07.091166+00:00', try_number=1, map_index=-1)
2023-12-02 16:26:26,759 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_name', run_id='manual__2023-12-02T09:21:30.376105+00:00', try_number=1, map_index=-1)
2023-12-02 16:26:26,769 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_name, run_id=manual__2023-12-02T09:21:30.376105+00:00, map_index=-1, run_start_date=2023-12-02 09:26:26.158346+00:00, run_end_date=2023-12-02 09:26:26.283320+00:00, run_duration=0.124974, state=success, executor_state=success, try_number=1, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-02 09:26:23.123569+00:00, queued_by_job_id=1, pid=116439
2023-12-02 16:26:26,769 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_name, run_id=scheduled__2023-12-02T08:51:07.091166+00:00, map_index=-1, run_start_date=2023-12-02 09:26:24.549605+00:00, run_end_date=2023-12-02 09:26:24.673193+00:00, run_duration=0.123588, state=success, executor_state=success, try_number=1, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-02 09:26:23.123569+00:00, queued_by_job_id=1, pid=116436
2023-12-02 16:26:26,830 INFO - Marking run <DagRun my_first_dag @ 2023-12-02 08:51:07.091166+00:00: scheduled__2023-12-02T08:51:07.091166+00:00, state:running, queued_at: 2023-12-02 09:21:31.389557+00:00. externally triggered: False> successful
2023-12-02 16:26:26,830 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 08:51:07.091166+00:00, run_id=scheduled__2023-12-02T08:51:07.091166+00:00, run_start_date=2023-12-02 09:21:31.409962+00:00, run_end_date=2023-12-02 09:26:26.830837+00:00, run_duration=295.420875, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-02 08:51:07.091166+00:00, data_interval_end=2023-12-02 09:21:07.091166+00:00, dag_hash=6bdebd7d0e02c4fc15c2e96933dde778
2023-12-02 16:26:26,834 INFO - Setting next_dagrun for my_first_dag to 2023-12-02T09:21:07.091166+00:00, run_after=2023-12-02T09:51:07.091166+00:00
2023-12-02 16:26:26,837 INFO - Marking run <DagRun my_first_dag @ 2023-12-02 09:21:30.376105+00:00: manual__2023-12-02T09:21:30.376105+00:00, state:running, queued_at: 2023-12-02 09:21:30.403091+00:00. externally triggered: True> successful
2023-12-02 16:26:26,837 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 09:21:30.376105+00:00, run_id=manual__2023-12-02T09:21:30.376105+00:00, run_start_date=2023-12-02 09:21:31.410118+00:00, run_end_date=2023-12-02 09:26:26.837436+00:00, run_duration=295.427318, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 08:51:30.376105+00:00, data_interval_end=2023-12-02 09:21:30.376105+00:00, dag_hash=6bdebd7d0e02c4fc15c2e96933dde778
2023-12-02 16:30:32,954 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 16:31:08,593 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder manual__2023-12-02T09:31:07.358440+00:00 [scheduled]>
2023-12-02 16:31:08,594 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 16:31:08,594 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder manual__2023-12-02T09:31:07.358440+00:00 [scheduled]>
2023-12-02 16:31:08,595 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-02 16:31:08,595 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-02T09:31:07.358440+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-02 16:31:08,596 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-02T09:31:07.358440+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:31:08,598 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-02T09:31:07.358440+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:31:10,430 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-02T09:31:07.358440+00:00', try_number=1, map_index=-1)
2023-12-02 16:31:10,436 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=manual__2023-12-02T09:31:07.358440+00:00, map_index=-1, run_start_date=2023-12-02 09:31:09.931738+00:00, run_end_date=2023-12-02 09:31:10.046895+00:00, run_duration=0.115157, state=success, executor_state=success, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-02 09:31:08.594627+00:00, queued_by_job_id=1, pid=117812
2023-12-02 16:31:10,486 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T09:31:07.358440+00:00 [scheduled]>
2023-12-02 16:31:10,487 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-02 16:31:10,487 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-02T09:31:07.358440+00:00 [scheduled]>
2023-12-02 16:31:10,488 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-02 16:31:10,488 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T09:31:07.358440+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-02 16:31:10,489 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T09:31:07.358440+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:31:10,491 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-02T09:31:07.358440+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-02 16:31:15,608 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-02T09:31:07.358440+00:00', try_number=1, map_index=-1)
2023-12-02 16:31:15,612 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-02T09:31:07.358440+00:00, map_index=-1, run_start_date=2023-12-02 09:31:11.831138+00:00, run_end_date=2023-12-02 09:31:15.234964+00:00, run_duration=3.403826, state=success, executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-02 09:31:10.487678+00:00, queued_by_job_id=1, pid=117847
2023-12-02 16:31:16,372 INFO - Marking run <DagRun my_first_dag @ 2023-12-02 09:31:07.358440+00:00: manual__2023-12-02T09:31:07.358440+00:00, state:running, queued_at: 2023-12-02 09:31:07.369833+00:00. externally triggered: True> successful
2023-12-02 16:31:16,372 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-02 09:31:07.358440+00:00, run_id=manual__2023-12-02T09:31:07.358440+00:00, run_start_date=2023-12-02 09:31:08.569892+00:00, run_end_date=2023-12-02 09:31:16.372820+00:00, run_duration=7.802928, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-02 09:01:07.358440+00:00, data_interval_end=2023-12-02 09:31:07.358440+00:00, dag_hash=6d9f5c8138ef933274fdd77525498679
2023-12-02 16:35:32,997 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 16:40:33,048 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 16:45:33,104 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-02 16:50:33,150 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:16:07,008 INFO - Starting the scheduler
2023-12-03 09:16:07,009 INFO - Processing each file at most -1 times
2023-12-03 09:16:07,015 INFO - Launched DagFileProcessorManager with pid: 66670
2023-12-03 09:16:07,016 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:16:07,019 INFO - Configured default timezone Timezone('UTC')
2023-12-03 09:16:24,306 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T02:16:07.288425+00:00, run_after=2023-12-03T02:46:07.288425+00:00
2023-12-03 09:16:24,369 INFO - 2 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:16:24,369 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:16:24,369 INFO - DAG my_first_dag has 1/16 running and queued tasks
2023-12-03 09:16:24,369 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:16:24,371 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-03 09:16:24,372 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-03 09:16:24,372 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-03 09:16:24,372 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:24,373 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-03 09:16:24,373 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:24,375 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:25,968 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:27,342 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 09:16:27,342 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1)
2023-12-03 09:16:27,349 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:16:26.929556+00:00, run_end_date=2023-12-03 02:16:27.041743+00:00, run_duration=0.112187, state=success, executor_state=success, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:16:24.370408+00:00, queued_by_job_id=1, pid=66861
2023-12-03 09:16:27,350 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:16:25.531496+00:00, run_end_date=2023-12-03 02:16:25.646874+00:00, run_duration=0.115378, state=success, executor_state=success, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:16:24.370408+00:00, queued_by_job_id=1, pid=66842
2023-12-03 09:16:27,404 INFO - 2 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:16:27,404 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:16:27,404 INFO - DAG my_first_dag has 1/16 running and queued tasks
2023-12-03 09:16:27,404 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:16:27,405 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-03 09:16:27,406 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-03 09:16:27,406 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-03 09:16:27,406 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:27,406 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-03 09:16:27,406 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:27,409 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:34,026 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:40,366 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 09:16:40,366 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1)
2023-12-03 09:16:40,370 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:16:34.998361+00:00, run_end_date=2023-12-03 02:16:40.016627+00:00, run_duration=5.018266, state=success, executor_state=success, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:16:27.405264+00:00, queued_by_job_id=1, pid=66916
2023-12-03 09:16:40,371 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:16:28.374315+00:00, run_end_date=2023-12-03 02:16:33.731439+00:00, run_duration=5.357124, state=success, executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:16:27.405264+00:00, queued_by_job_id=1, pid=66864
2023-12-03 09:16:40,542 INFO - 2 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:16:40,542 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:16:40,543 INFO - DAG my_first_dag has 1/16 running and queued tasks
2023-12-03 09:16:40,543 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:16:40,545 WARNING - cannot record scheduled_duration for task save_data because previous state change time has not been saved
2023-12-03 09:16:40,546 WARNING - cannot record scheduled_duration for task save_data because previous state change time has not been saved
2023-12-03 09:16:40,546 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:16:40,547 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:40,547 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:16:40,547 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:40,550 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:42,231 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:16:43,888 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 09:16:43,888 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=1, map_index=-1)
2023-12-03 09:16:43,892 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:16:43.199832+00:00, run_end_date=2023-12-03 02:16:43.612135+00:00, run_duration=0.412303, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:16:40.544254+00:00, queued_by_job_id=1, pid=67037
2023-12-03 09:16:43,892 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:16:41.542020+00:00, run_end_date=2023-12-03 02:16:41.945041+00:00, run_duration=0.403021, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:16:40.544254+00:00, queued_by_job_id=1, pid=67034
2023-12-03 09:21:07,207 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:21:42,428 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
2023-12-03 09:21:42,428 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:21:42,428 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T01:46:07.288425+00:00 [scheduled]>
2023-12-03 09:21:42,429 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:21:42,430 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:21:42,432 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T01:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:21:43,929 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T01:46:07.288425+00:00', try_number=2, map_index=-1)
2023-12-03 09:21:43,935 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:21:43.332946+00:00, run_end_date=2023-12-03 02:21:43.664048+00:00, run_duration=0.331102, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:21:42.428779+00:00, queued_by_job_id=1, pid=70121
2023-12-03 09:21:44,107 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:21:44,107 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:21:44,108 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:16:23.978630+00:00 [scheduled]>
2023-12-03 09:21:44,109 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:21:44,109 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:21:44,111 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:16:23.978630+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:21:45,658 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:16:23.978630+00:00', try_number=2, map_index=-1)
2023-12-03 09:21:45,663 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=manual__2023-12-03T02:16:23.978630+00:00, map_index=-1, run_start_date=2023-12-03 02:21:45.031170+00:00, run_end_date=2023-12-03 02:21:45.373312+00:00, run_duration=0.342142, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:21:44.108319+00:00, queued_by_job_id=1, pid=70125
2023-12-03 09:22:32,494 ERROR - Marking run <DagRun my_first_dag @ 2023-12-03 01:46:07.288425+00:00: scheduled__2023-12-03T01:46:07.288425+00:00, state:running, queued_at: 2023-12-03 02:16:24.293048+00:00. externally triggered: False> failed
2023-12-03 09:22:32,495 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 01:46:07.288425+00:00, run_id=scheduled__2023-12-03T01:46:07.288425+00:00, run_start_date=2023-12-03 02:16:24.323804+00:00, run_end_date=2023-12-03 02:22:32.495673+00:00, run_duration=368.171869, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 01:46:07.288425+00:00, data_interval_end=2023-12-03 02:16:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d
2023-12-03 09:22:32,498 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T02:16:07.288425+00:00, run_after=2023-12-03T02:46:07.288425+00:00
2023-12-03 09:22:40,480 ERROR - Marking run <DagRun my_first_dag @ 2023-12-03 02:16:23.978630+00:00: manual__2023-12-03T02:16:23.978630+00:00, state:running, queued_at: 2023-12-03 02:16:24.000999+00:00. externally triggered: True> failed
2023-12-03 09:22:40,481 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:16:23.978630+00:00, run_id=manual__2023-12-03T02:16:23.978630+00:00, run_start_date=2023-12-03 02:16:24.324068+00:00, run_end_date=2023-12-03 02:22:40.481439+00:00, run_duration=376.157371, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-03 01:46:23.978630+00:00, data_interval_end=2023-12-03 02:16:23.978630+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d
2023-12-03 09:24:16,458 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>
2023-12-03 09:24:16,459 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:24:16,459 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>
2023-12-03 09:24:16,460 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-03 09:24:16,461 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-03 09:24:16,461 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:24:16,463 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:24:18,003 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1)
2023-12-03 09:24:18,007 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=manual__2023-12-03T02:24:15.469689+00:00, map_index=-1, run_start_date=2023-12-03 02:24:17.567729+00:00, run_end_date=2023-12-03 02:24:17.692613+00:00, run_duration=0.124884, state=success, executor_state=success, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:24:16.459597+00:00, queued_by_job_id=1, pid=71248
2023-12-03 09:24:18,054 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>
2023-12-03 09:24:18,055 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:24:18,055 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>
2023-12-03 09:24:18,056 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-03 09:24:18,057 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-03 09:24:18,057 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:24:18,059 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:24:24,397 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1)
2023-12-03 09:24:24,402 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=manual__2023-12-03T02:24:15.469689+00:00, map_index=-1, run_start_date=2023-12-03 02:24:19.064935+00:00, run_end_date=2023-12-03 02:24:24.111508+00:00, run_duration=5.046573, state=success, executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:24:18.055735+00:00, queued_by_job_id=1, pid=71251
2023-12-03 09:24:24,452 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>
2023-12-03 09:24:24,453 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:24:24,453 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data manual__2023-12-03T02:24:15.469689+00:00 [scheduled]>
2023-12-03 09:24:24,454 WARNING - cannot record scheduled_duration for task save_data because previous state change time has not been saved
2023-12-03 09:24:24,454 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:24:24,454 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:24:24,457 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'manual__2023-12-03T02:24:15.469689+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:24:45,285 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='manual__2023-12-03T02:24:15.469689+00:00', try_number=1, map_index=-1)
2023-12-03 09:24:45,288 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=manual__2023-12-03T02:24:15.469689+00:00, map_index=-1, run_start_date=2023-12-03 02:24:25.516739+00:00, run_end_date=2023-12-03 02:24:44.995142+00:00, run_duration=19.478403, state=success, executor_state=success, try_number=1, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:24:24.453686+00:00, queued_by_job_id=1, pid=71302
2023-12-03 09:24:45,437 INFO - Marking run <DagRun my_first_dag @ 2023-12-03 02:24:15.469689+00:00: manual__2023-12-03T02:24:15.469689+00:00, state:running, queued_at: 2023-12-03 02:24:15.476258+00:00. externally triggered: True> successful
2023-12-03 09:24:45,437 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:24:15.469689+00:00, run_id=manual__2023-12-03T02:24:15.469689+00:00, run_start_date=2023-12-03 02:24:16.430799+00:00, run_end_date=2023-12-03 02:24:45.437861+00:00, run_duration=29.007062, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-03 01:54:15.469689+00:00, data_interval_end=2023-12-03 02:24:15.469689+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d
2023-12-03 09:26:07,244 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:31:07,279 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:36:07,310 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:41:07,352 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:46:07,388 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:46:08,094 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T02:46:07.288425+00:00, run_after=2023-12-03T03:16:07.288425+00:00
2023-12-03 09:46:08,126 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:46:08,126 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:46:08,126 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:46:08,127 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-03 09:46:08,127 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-03 09:46:08,127 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:46:08,129 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:46:09,071 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 09:46:09,074 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:46:08.749620+00:00, run_end_date=2023-12-03 02:46:08.825705+00:00, run_duration=0.076085, state=success, executor_state=success, try_number=1, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 02:46:08.126565+00:00, queued_by_job_id=1, pid=77296
2023-12-03 09:46:09,111 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:46:09,112 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:46:09,112 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:46:09,112 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-03 09:46:09,113 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-03 09:46:09,113 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:46:09,114 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:46:14,248 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 09:46:14,251 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:46:09.759615+00:00, run_end_date=2023-12-03 02:46:14.034915+00:00, run_duration=4.2753, state=success, executor_state=success, try_number=1, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 02:46:09.112438+00:00, queued_by_job_id=1, pid=77299
2023-12-03 09:46:14,290 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:46:14,290 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:46:14,290 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:46:14,291 WARNING - cannot record scheduled_duration for task save_data because previous state change time has not been saved
2023-12-03 09:46:14,291 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:46:14,291 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:46:14,293 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:46:22,761 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 09:46:22,764 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:46:14.919826+00:00, run_end_date=2023-12-03 02:46:22.504665+00:00, run_duration=7.584839, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:46:14.290902+00:00, queued_by_job_id=1, pid=77324
2023-12-03 09:51:07,927 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:51:23,275 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:51:23,275 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:51:23,275 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:51:23,276 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:51:23,276 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:51:23,278 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:51:31,639 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=2, map_index=-1)
2023-12-03 09:51:31,641 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:51:23.920384+00:00, run_end_date=2023-12-03 02:51:31.402771+00:00, run_duration=7.482387, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:51:23.276094+00:00, queued_by_job_id=1, pid=78599
2023-12-03 09:56:07,968 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 09:56:31,745 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:56:31,745 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 09:56:31,746 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.save_data scheduled__2023-12-03T02:16:07.288425+00:00 [scheduled]>
2023-12-03 09:56:31,747 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 1 and queue default
2023-12-03 09:56:31,747 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:56:31,749 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'save_data', 'scheduled__2023-12-03T02:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 09:56:40,921 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='save_data', run_id='scheduled__2023-12-03T02:16:07.288425+00:00', try_number=3, map_index=-1)
2023-12-03 09:56:40,924 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=save_data, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 02:56:32.355931+00:00, run_end_date=2023-12-03 02:56:40.710592+00:00, run_duration=8.354661, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-03 02:56:31.746376+00:00, queued_by_job_id=1, pid=79924
2023-12-03 09:56:41,064 ERROR - Marking run <DagRun my_first_dag @ 2023-12-03 02:16:07.288425+00:00: scheduled__2023-12-03T02:16:07.288425+00:00, state:running, queued_at: 2023-12-03 02:46:08.092583+00:00. externally triggered: False> failed
2023-12-03 09:56:41,064 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:16:07.288425+00:00, run_id=scheduled__2023-12-03T02:16:07.288425+00:00, run_start_date=2023-12-03 02:46:08.110583+00:00, run_end_date=2023-12-03 02:56:41.064517+00:00, run_duration=632.953934, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 02:16:07.288425+00:00, data_interval_end=2023-12-03 02:46:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d
2023-12-03 09:56:41,065 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T02:46:07.288425+00:00, run_after=2023-12-03T03:16:07.288425+00:00
2023-12-03 10:01:08,005 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:06:08,015 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:11:08,046 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:16:08,144 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T03:16:07.288425+00:00, run_after=2023-12-03T03:46:07.288425+00:00
2023-12-03 10:16:08,177 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:16:08,178 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:16:08,178 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:16:08,178 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-03 10:16:08,178 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-03 10:16:08,179 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:16:08,181 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:16:09,094 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 10:16:09,096 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:16:08.791679+00:00, run_end_date=2023-12-03 03:16:08.864039+00:00, run_duration=0.07236, state=success, executor_state=success, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 03:16:08.178307+00:00, queued_by_job_id=1, pid=85920
2023-12-03 10:16:09,107 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:16:09,136 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:16:09,136 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:16:09,137 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:16:09,138 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-03 10:16:09,138 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-03 10:16:09,138 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:16:09,140 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:16:10,402 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 10:16:10,405 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:16:09.751958+00:00, run_end_date=2023-12-03 03:16:10.171626+00:00, run_duration=0.419668, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:16:09.137380+00:00, queued_by_job_id=1, pid=85923
2023-12-03 10:21:09,162 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:21:10,213 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:21:10,213 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:21:10,213 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:21:10,214 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-03 10:21:10,214 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:21:10,216 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:21:11,486 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=2, map_index=-1)
2023-12-03 10:21:11,489 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:21:10.836444+00:00, run_end_date=2023-12-03 03:21:11.254870+00:00, run_duration=0.418426, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:21:10.213735+00:00, queued_by_job_id=1, pid=87714
2023-12-03 10:26:09,211 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:26:11,704 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:26:11,705 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:26:11,705 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T02:46:07.288425+00:00 [scheduled]>
2023-12-03 10:26:11,705 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-03 10:26:11,705 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:26:11,708 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T02:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:26:12,985 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T02:46:07.288425+00:00', try_number=3, map_index=-1)
2023-12-03 10:26:12,987 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:26:12.328500+00:00, run_end_date=2023-12-03 03:26:12.749498+00:00, run_duration=0.420998, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:26:11.705316+00:00, queued_by_job_id=1, pid=89330
2023-12-03 10:26:13,011 ERROR - Marking run <DagRun my_first_dag @ 2023-12-03 02:46:07.288425+00:00: scheduled__2023-12-03T02:46:07.288425+00:00, state:running, queued_at: 2023-12-03 03:16:08.140437+00:00. externally triggered: False> failed
2023-12-03 10:26:13,011 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 02:46:07.288425+00:00, run_id=scheduled__2023-12-03T02:46:07.288425+00:00, run_start_date=2023-12-03 03:16:08.161408+00:00, run_end_date=2023-12-03 03:26:13.011707+00:00, run_duration=604.850299, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 02:46:07.288425+00:00, data_interval_end=2023-12-03 03:16:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d
2023-12-03 10:26:13,012 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T03:16:07.288425+00:00, run_after=2023-12-03T03:46:07.288425+00:00
2023-12-03 10:31:09,240 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:36:09,279 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:41:09,335 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:46:08,706 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T03:46:07.288425+00:00, run_after=2023-12-03T04:16:07.288425+00:00
2023-12-03 10:46:08,737 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:46:08,737 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:46:08,737 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:46:08,738 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-03 10:46:08,738 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-03 10:46:08,738 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:46:08,740 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:46:09,689 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 10:46:09,691 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:46:09.385944+00:00, run_end_date=2023-12-03 03:46:09.462969+00:00, run_duration=0.077025, state=success, executor_state=success, try_number=1, max_tries=2, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 03:46:08.737957+00:00, queued_by_job_id=1, pid=94314
2023-12-03 10:46:09,702 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:46:09,743 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:46:09,743 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:46:09,743 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:46:09,744 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-03 10:46:09,744 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-03 10:46:09,744 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:46:09,746 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:46:11,034 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 10:46:11,037 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:46:10.367271+00:00, run_end_date=2023-12-03 03:46:10.806821+00:00, run_duration=0.43955, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:46:09.743620+00:00, queued_by_job_id=1, pid=94317
2023-12-03 10:51:09,731 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:51:11,795 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:51:11,795 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:51:11,795 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:51:11,796 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-03 10:51:11,796 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:51:11,798 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:51:13,103 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=2, map_index=-1)
2023-12-03 10:51:13,106 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:51:12.445954+00:00, run_end_date=2023-12-03 03:51:12.890127+00:00, run_duration=0.444173, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:51:11.795577+00:00, queued_by_job_id=1, pid=95471
2023-12-03 10:56:09,767 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 10:56:13,279 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:56:13,279 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 10:56:13,279 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:16:07.288425+00:00 [scheduled]>
2023-12-03 10:56:13,280 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-03 10:56:13,280 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:56:13,282 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:16:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 10:56:14,786 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:16:07.288425+00:00', try_number=3, map_index=-1)
2023-12-03 10:56:14,789 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 03:56:14.026200+00:00, run_end_date=2023-12-03 03:56:14.524569+00:00, run_duration=0.498369, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 03:56:13.280123+00:00, queued_by_job_id=1, pid=96639
2023-12-03 10:56:14,814 ERROR - Marking run <DagRun my_first_dag @ 2023-12-03 03:16:07.288425+00:00: scheduled__2023-12-03T03:16:07.288425+00:00, state:running, queued_at: 2023-12-03 03:46:08.705061+00:00. externally triggered: False> failed
2023-12-03 10:56:14,814 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 03:16:07.288425+00:00, run_id=scheduled__2023-12-03T03:16:07.288425+00:00, run_start_date=2023-12-03 03:46:08.722696+00:00, run_end_date=2023-12-03 03:56:14.814367+00:00, run_duration=606.091671, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 03:16:07.288425+00:00, data_interval_end=2023-12-03 03:46:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d
2023-12-03 10:56:14,815 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T03:46:07.288425+00:00, run_after=2023-12-03T04:16:07.288425+00:00
2023-12-03 11:01:09,809 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 11:06:09,835 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 11:11:09,860 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 11:16:08,046 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T04:16:07.288425+00:00, run_after=2023-12-03T04:46:07.288425+00:00
2023-12-03 11:16:08,063 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:16:08,063 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 11:16:08,063 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.print_folder scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:16:08,064 WARNING - cannot record scheduled_duration for task print_folder because previous state change time has not been saved
2023-12-03 11:16:08,064 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-03 11:16:08,064 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:16:08,066 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'print_folder', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:16:09,043 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='print_folder', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 11:16:09,045 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=print_folder, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:16:08.730026+00:00, run_end_date=2023-12-03 04:16:08.807554+00:00, run_duration=0.077528, state=success, executor_state=success, try_number=1, max_tries=2, job_id=26, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-12-03 04:16:08.064230+00:00, queued_by_job_id=1, pid=104028
2023-12-03 11:16:09,081 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:16:09,081 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 11:16:09,081 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:16:09,082 WARNING - cannot record scheduled_duration for task crawl_data because previous state change time has not been saved
2023-12-03 11:16:09,082 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-03 11:16:09,082 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:16:09,084 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:16:10,489 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=1, map_index=-1)
2023-12-03 11:16:10,492 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:16:09.721851+00:00, run_end_date=2023-12-03 04:16:10.189493+00:00, run_duration=0.467642, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=27, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 04:16:09.081604+00:00, queued_by_job_id=1, pid=104031
2023-12-03 11:16:10,503 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 11:21:10,513 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:21:10,513 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 11:21:10,513 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:21:10,514 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-03 11:21:10,514 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:21:10,516 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:21:11,821 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=2, map_index=-1)
2023-12-03 11:21:11,824 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:21:11.149442+00:00, run_end_date=2023-12-03 04:21:11.587635+00:00, run_duration=0.438193, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 04:21:10.513719+00:00, queued_by_job_id=1, pid=106527
2023-12-03 11:21:11,835 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 11:26:11,746 INFO - 1 tasks up for execution:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:26:11,747 INFO - DAG my_first_dag has 0/16 running and queued tasks
2023-12-03 11:26:11,747 INFO - Setting the following tasks to queued state:
	<TaskInstance: my_first_dag.crawl_data scheduled__2023-12-03T03:46:07.288425+00:00 [scheduled]>
2023-12-03 11:26:11,747 INFO - Sending TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-03 11:26:11,748 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:26:11,749 INFO - Executing command: ['airflow', 'tasks', 'run', 'my_first_dag', 'crawl_data', 'scheduled__2023-12-03T03:46:07.288425+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
2023-12-03 11:26:13,143 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='my_first_dag', task_id='crawl_data', run_id='scheduled__2023-12-03T03:46:07.288425+00:00', try_number=3, map_index=-1)
2023-12-03 11:26:13,145 INFO - TaskInstance Finished: dag_id=my_first_dag, task_id=crawl_data, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, map_index=-1, run_start_date=2023-12-03 04:26:12.442416+00:00, run_end_date=2023-12-03 04:26:12.905070+00:00, run_duration=0.462654, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=29, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-03 04:26:11.747319+00:00, queued_by_job_id=1, pid=108842
2023-12-03 11:26:13,159 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 11:26:13,174 ERROR - Marking run <DagRun my_first_dag @ 2023-12-03 03:46:07.288425+00:00: scheduled__2023-12-03T03:46:07.288425+00:00, state:running, queued_at: 2023-12-03 04:16:08.045102+00:00. externally triggered: False> failed
2023-12-03 11:26:13,174 INFO - DagRun Finished: dag_id=my_first_dag, execution_date=2023-12-03 03:46:07.288425+00:00, run_id=scheduled__2023-12-03T03:46:07.288425+00:00, run_start_date=2023-12-03 04:16:08.051610+00:00, run_end_date=2023-12-03 04:26:13.174658+00:00, run_duration=605.123048, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-03 03:46:07.288425+00:00, data_interval_end=2023-12-03 04:16:07.288425+00:00, dag_hash=fae2e6e757a01881ccd1da260aef0a7d
2023-12-03 11:26:13,176 INFO - Setting next_dagrun for my_first_dag to 2023-12-03T04:16:07.288425+00:00, run_after=2023-12-03T04:46:07.288425+00:00
2023-12-03 11:31:13,183 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-03 11:36:13,220 INFO - Adopting or resetting orphaned tasks for active dag runs
