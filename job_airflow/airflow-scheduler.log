2023-12-17 08:03:00,856 INFO - Starting the scheduler
2023-12-17 08:03:00,856 INFO - Processing each file at most -1 times
2023-12-17 08:03:00,861 INFO - Launched DagFileProcessorManager with pid: 309552
2023-12-17 08:03:00,863 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:03:00,865 INFO - Configured default timezone Timezone('UTC')
2023-12-17 08:03:53,146 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:03:53,147 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:03:53,147 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:03:53,149 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 08:03:53,150 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 08:03:53,150 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:03:53,153 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:05:16,173 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=1, map_index=-1)
2023-12-17 08:05:16,184 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=crawl_careerlink, run_id=manual__2023-12-17T01:03:52.502924+00:00, map_index=-1, run_start_date=2023-12-17 01:03:54.530631+00:00, run_end_date=2023-12-17 01:05:15.865391+00:00, run_duration=81.33476, state=success, executor_state=success, try_number=1, max_tries=3, job_id=2, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 01:03:53.148113+00:00, queued_by_job_id=1, pid=310020
2023-12-17 08:05:16,195 ERROR - DagFileProcessorManager (PID=309552) last sent a heartbeat 83.10 seconds ago! Restarting it
2023-12-17 08:05:16,205 INFO - Sending Signals.SIGTERM to group 309552. PIDs of all processes in the group: [309552]
2023-12-17 08:05:16,205 INFO - Sending the signal Signals.SIGTERM to group 309552
2023-12-17 08:05:16,337 INFO - Process psutil.Process(pid=309552, status='terminated', exitcode=0, started='08:03:00') (309552) terminated with exit code 0
2023-12-17 08:05:16,341 INFO - Launched DagFileProcessorManager with pid: 310770
2023-12-17 08:05:16,346 INFO - Configured default timezone Timezone('UTC')
2023-12-17 08:05:16,555 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.check_daily_file_exists manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:05:16,555 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:05:16,556 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.check_daily_file_exists manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:05:16,557 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:05:16,557 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:05:16,557 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'check_daily_file_exists', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:05:16,559 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'check_daily_file_exists', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:05:18,167 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=1, map_index=-1)
2023-12-17 08:05:18,171 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:03:52.502924+00:00, map_index=-1, run_start_date=2023-12-17 01:05:17.766656+00:00, run_end_date=2023-12-17 01:05:17.865202+00:00, run_duration=0.098546, state=success, executor_state=success, try_number=1, max_tries=3, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:05:16.556328+00:00, queued_by_job_id=1, pid=310773
2023-12-17 08:05:18,325 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:05:18,326 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:05:18,326 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:05:18,327 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:05:18,327 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:05:18,328 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:05:18,330 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:05:20,113 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=1, map_index=-1)
2023-12-17 08:05:20,117 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:03:52.502924+00:00, map_index=-1, run_start_date=2023-12-17 01:05:19.600339+00:00, run_end_date=2023-12-17 01:05:19.794742+00:00, run_duration=0.194403, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:05:18.326903+00:00, queued_by_job_id=1, pid=310806
2023-12-17 08:06:20,173 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:06:20,173 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:06:20,173 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:06:20,174 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:06:20,175 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:06:20,177 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:06:21,922 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=2, map_index=-1)
2023-12-17 08:06:21,925 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:03:52.502924+00:00, map_index=-1, run_start_date=2023-12-17 01:06:21.412622+00:00, run_end_date=2023-12-17 01:06:21.611833+00:00, run_duration=0.199211, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:06:20.173977+00:00, queued_by_job_id=1, pid=311433
2023-12-17 08:07:22,036 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:07:22,037 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:07:22,037 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:07:22,038 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:07:22,038 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:07:22,040 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:07:23,807 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=3, map_index=-1)
2023-12-17 08:07:23,812 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:03:52.502924+00:00, map_index=-1, run_start_date=2023-12-17 01:07:23.143225+00:00, run_end_date=2023-12-17 01:07:23.412909+00:00, run_duration=0.269684, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:07:22.037747+00:00, queued_by_job_id=1, pid=312097
2023-12-17 08:07:27,182 INFO - Setting next_dagrun for career_link_pipepline_test to 2023-12-17T01:07:19.667400+00:00, run_after=2023-12-17T21:07:19.667400+00:00
2023-12-17 08:07:27,217 INFO - 2 tasks up for execution:
	<TaskInstance: career_link_pipepline_test.check_file_exists scheduled__2023-12-16T05:07:19.667400+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.check_file_exists manual__2023-12-17T01:07:26.063296+00:00 [scheduled]>
2023-12-17 08:07:27,218 INFO - DAG career_link_pipepline_test has 0/16 running and queued tasks
2023-12-17 08:07:27,218 INFO - DAG career_link_pipepline_test has 1/16 running and queued tasks
2023-12-17 08:07:27,218 INFO - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test.check_file_exists scheduled__2023-12-16T05:07:19.667400+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.check_file_exists manual__2023-12-17T01:07:26.063296+00:00 [scheduled]>
2023-12-17 08:07:27,219 WARNING - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved
2023-12-17 08:07:27,220 WARNING - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved
2023-12-17 08:07:27,220 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:07:19.667400+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:07:27,220 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'scheduled__2023-12-16T05:07:19.667400+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:07:27,221 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='manual__2023-12-17T01:07:26.063296+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:07:27,221 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'manual__2023-12-17T01:07:26.063296+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:07:27,223 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'scheduled__2023-12-16T05:07:19.667400+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:07:28,877 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'manual__2023-12-17T01:07:26.063296+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:07:30,494 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:07:19.667400+00:00', try_number=1, map_index=-1)
2023-12-17 08:07:30,495 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='manual__2023-12-17T01:07:26.063296+00:00', try_number=1, map_index=-1)
2023-12-17 08:07:30,500 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=check_file_exists, run_id=manual__2023-12-17T01:07:26.063296+00:00, map_index=-1, run_start_date=2023-12-17 01:07:30.112448+00:00, run_end_date=2023-12-17 01:07:30.208315+00:00, run_duration=0.095867, state=success, executor_state=success, try_number=1, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:07:27.218679+00:00, queued_by_job_id=1, pid=312183
2023-12-17 08:07:30,500 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=check_file_exists, run_id=scheduled__2023-12-16T05:07:19.667400+00:00, map_index=-1, run_start_date=2023-12-17 01:07:28.483787+00:00, run_end_date=2023-12-17 01:07:28.579725+00:00, run_duration=0.095938, state=success, executor_state=success, try_number=1, max_tries=3, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:07:27.218679+00:00, queued_by_job_id=1, pid=312150
2023-12-17 08:07:30,682 INFO - 2 tasks up for execution:
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest scheduled__2023-12-16T05:07:19.667400+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest manual__2023-12-17T01:07:26.063296+00:00 [scheduled]>
2023-12-17 08:07:30,682 INFO - DAG career_link_pipepline_test has 0/16 running and queued tasks
2023-12-17 08:07:30,682 INFO - DAG career_link_pipepline_test has 1/16 running and queued tasks
2023-12-17 08:07:30,682 INFO - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest scheduled__2023-12-16T05:07:19.667400+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest manual__2023-12-17T01:07:26.063296+00:00 [scheduled]>
2023-12-17 08:07:30,683 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:07:30,683 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:07:30,684 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:07:19.667400+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 08:07:30,684 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:07:19.667400+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:07:30,684 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:07:26.063296+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 08:07:30,684 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'manual__2023-12-17T01:07:26.063296+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:07:30,686 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:07:19.667400+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:07:48,808 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'manual__2023-12-17T01:07:26.063296+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:08:08,218 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:07:19.667400+00:00', try_number=1, map_index=-1)
2023-12-17 08:08:08,219 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:07:26.063296+00:00', try_number=1, map_index=-1)
2023-12-17 08:08:08,222 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:07:26.063296+00:00, map_index=-1, run_start_date=2023-12-17 01:07:50.241775+00:00, run_end_date=2023-12-17 01:08:07.925102+00:00, run_duration=17.683327, state=success, executor_state=success, try_number=1, max_tries=3, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:07:30.682970+00:00, queued_by_job_id=1, pid=312600
2023-12-17 08:08:08,222 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T05:07:19.667400+00:00, map_index=-1, run_start_date=2023-12-17 01:07:31.878268+00:00, run_end_date=2023-12-17 01:07:48.485917+00:00, run_duration=16.607649, state=success, executor_state=success, try_number=1, max_tries=3, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:07:30.682970+00:00, queued_by_job_id=1, pid=312187
2023-12-17 08:08:08,246 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:08:08,247 INFO - Marked 1 SchedulerJob instances as failed
2023-12-17 08:08:08,391 INFO - Marking run <DagRun career_link_pipepline_test @ 2023-12-16 05:07:19.667400+00:00: scheduled__2023-12-16T05:07:19.667400+00:00, state:running, queued_at: 2023-12-17 01:07:27.176218+00:00. externally triggered: False> successful
2023-12-17 08:08:08,391 INFO - DagRun Finished: dag_id=career_link_pipepline_test, execution_date=2023-12-16 05:07:19.667400+00:00, run_id=scheduled__2023-12-16T05:07:19.667400+00:00, run_start_date=2023-12-17 01:07:27.190299+00:00, run_end_date=2023-12-17 01:08:08.391624+00:00, run_duration=41.201325, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 05:07:19.667400+00:00, data_interval_end=2023-12-17 01:07:19.667400+00:00, dag_hash=597b64aa6373d8a9b1b96bd8c81a1a31
2023-12-17 08:08:08,393 INFO - Setting next_dagrun for career_link_pipepline_test to 2023-12-17T01:07:19.667400+00:00, run_after=2023-12-17T21:07:19.667400+00:00
2023-12-17 08:08:08,395 INFO - Marking run <DagRun career_link_pipepline_test @ 2023-12-17 01:07:26.063296+00:00: manual__2023-12-17T01:07:26.063296+00:00, state:running, queued_at: 2023-12-17 01:07:26.078475+00:00. externally triggered: True> successful
2023-12-17 08:08:08,395 INFO - DagRun Finished: dag_id=career_link_pipepline_test, execution_date=2023-12-17 01:07:26.063296+00:00, run_id=manual__2023-12-17T01:07:26.063296+00:00, run_start_date=2023-12-17 01:07:27.190483+00:00, run_end_date=2023-12-17 01:08:08.395866+00:00, run_duration=41.205383, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 05:07:26.063296+00:00, data_interval_end=2023-12-17 01:07:26.063296+00:00, dag_hash=597b64aa6373d8a9b1b96bd8c81a1a31
2023-12-17 08:08:23,715 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:08:23,715 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:08:23,715 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:03:52.502924+00:00 [scheduled]>
2023-12-17 08:08:23,716 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:08:23,716 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:08:23,718 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:03:52.502924+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:08:25,040 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:03:52.502924+00:00', try_number=4, map_index=-1)
2023-12-17 08:08:25,045 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:03:52.502924+00:00, map_index=-1, run_start_date=2023-12-17 01:08:24.611409+00:00, run_end_date=2023-12-17 01:08:24.757877+00:00, run_duration=0.146468, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:08:23.715802+00:00, queued_by_job_id=1, pid=313065
2023-12-17 08:08:25,199 ERROR - Marking run <DagRun careerlink_pipeline @ 2023-12-17 01:03:52.502924+00:00: manual__2023-12-17T01:03:52.502924+00:00, state:running, queued_at: 2023-12-17 01:03:52.523267+00:00. externally triggered: True> failed
2023-12-17 08:08:25,199 INFO - DagRun Finished: dag_id=careerlink_pipeline, execution_date=2023-12-17 01:03:52.502924+00:00, run_id=manual__2023-12-17T01:03:52.502924+00:00, run_start_date=2023-12-17 01:03:53.107868+00:00, run_end_date=2023-12-17 01:08:25.199474+00:00, run_duration=272.091606, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:03:52.502924+00:00, data_interval_end=2023-12-17 01:03:52.502924+00:00, dag_hash=1c712fcaa2271bebc85a0b31afa9037c
2023-12-17 08:13:08,380 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:16:18,370 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:16:18,370 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:16:18,370 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:16:18,372 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 08:16:18,372 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 08:16:18,372 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:16:18,375 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:17:34,866 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=1, map_index=-1)
2023-12-17 08:17:34,869 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=crawl_careerlink, run_id=manual__2023-12-17T01:16:17.870974+00:00, map_index=-1, run_start_date=2023-12-17 01:16:19.712880+00:00, run_end_date=2023-12-17 01:17:34.647647+00:00, run_duration=74.934767, state=success, executor_state=success, try_number=1, max_tries=3, job_id=12, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 01:16:18.371272+00:00, queued_by_job_id=1, pid=315641
2023-12-17 08:17:34,880 ERROR - DagFileProcessorManager (PID=310770) last sent a heartbeat 76.54 seconds ago! Restarting it
2023-12-17 08:17:34,885 INFO - Sending Signals.SIGTERM to group 310770. PIDs of all processes in the group: [310770]
2023-12-17 08:17:34,885 INFO - Sending the signal Signals.SIGTERM to group 310770
2023-12-17 08:17:34,978 INFO - Process psutil.Process(pid=310770, status='terminated', exitcode=0, started='08:05:16') (310770) terminated with exit code 0
2023-12-17 08:17:34,981 INFO - Launched DagFileProcessorManager with pid: 316323
2023-12-17 08:17:34,985 INFO - Configured default timezone Timezone('UTC')
2023-12-17 08:17:35,168 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.check_daily_file_exists manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:17:35,168 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:17:35,168 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.check_daily_file_exists manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:17:35,169 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:17:35,169 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:17:35,170 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'check_daily_file_exists', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:17:35,172 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'check_daily_file_exists', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:17:36,415 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=1, map_index=-1)
2023-12-17 08:17:36,418 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:16:17.870974+00:00, map_index=-1, run_start_date=2023-12-17 01:17:36.095441+00:00, run_end_date=2023-12-17 01:17:36.172126+00:00, run_duration=0.076685, state=success, executor_state=success, try_number=1, max_tries=3, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:17:35.168900+00:00, queued_by_job_id=1, pid=316343
2023-12-17 08:17:36,558 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:17:36,558 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:17:36,559 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:17:36,559 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:17:36,560 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:17:36,560 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:17:36,562 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:17:37,858 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=1, map_index=-1)
2023-12-17 08:17:37,861 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:16:17.870974+00:00, map_index=-1, run_start_date=2023-12-17 01:17:37.463719+00:00, run_end_date=2023-12-17 01:17:37.617842+00:00, run_duration=0.154123, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:17:36.559317+00:00, queued_by_job_id=1, pid=316346
2023-12-17 08:18:08,543 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:18:37,707 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:18:37,707 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:18:37,707 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:18:37,709 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:18:37,709 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:18:37,712 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:18:39,596 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=2, map_index=-1)
2023-12-17 08:18:39,599 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:16:17.870974+00:00, map_index=-1, run_start_date=2023-12-17 01:18:39.157610+00:00, run_end_date=2023-12-17 01:18:39.328996+00:00, run_duration=0.171386, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:18:37.708172+00:00, queued_by_job_id=1, pid=316711
2023-12-17 08:19:39,372 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:19:39,373 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:19:39,373 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:19:39,374 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:19:39,374 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:19:39,376 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:19:41,199 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=3, map_index=-1)
2023-12-17 08:19:41,202 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:16:17.870974+00:00, map_index=-1, run_start_date=2023-12-17 01:19:40.703640+00:00, run_end_date=2023-12-17 01:19:40.898126+00:00, run_duration=0.194486, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:19:39.373677+00:00, queued_by_job_id=1, pid=317337
2023-12-17 08:20:41,065 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:20:41,066 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:20:41,066 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:16:17.870974+00:00 [scheduled]>
2023-12-17 08:20:41,067 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:20:41,067 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:20:41,070 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:16:17.870974+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:20:42,794 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:16:17.870974+00:00', try_number=4, map_index=-1)
2023-12-17 08:20:42,797 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:16:17.870974+00:00, map_index=-1, run_start_date=2023-12-17 01:20:42.280295+00:00, run_end_date=2023-12-17 01:20:42.480011+00:00, run_duration=0.199716, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:20:41.066572+00:00, queued_by_job_id=1, pid=317971
2023-12-17 08:20:42,931 ERROR - Marking run <DagRun careerlink_pipeline @ 2023-12-17 01:16:17.870974+00:00: manual__2023-12-17T01:16:17.870974+00:00, state:running, queued_at: 2023-12-17 01:16:17.880939+00:00. externally triggered: True> failed
2023-12-17 08:20:42,931 INFO - DagRun Finished: dag_id=careerlink_pipeline, execution_date=2023-12-17 01:16:17.870974+00:00, run_id=manual__2023-12-17T01:16:17.870974+00:00, run_start_date=2023-12-17 01:16:18.347169+00:00, run_end_date=2023-12-17 01:20:42.931381+00:00, run_duration=264.584212, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:16:17.870974+00:00, data_interval_end=2023-12-17 01:16:17.870974+00:00, dag_hash=3ed92bf97425a1f1271e66654d6cba72
2023-12-17 08:20:49,190 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:20:48.150376+00:00 [scheduled]>
2023-12-17 08:20:49,190 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:20:49,190 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:20:48.150376+00:00 [scheduled]>
2023-12-17 08:20:49,191 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 08:20:49,192 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:20:48.150376+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 08:20:49,192 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:20:48.150376+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:20:49,194 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:20:48.150376+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:21:46,384 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:20:48.150376+00:00', try_number=1, map_index=-1)
2023-12-17 08:21:46,387 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=crawl_careerlink, run_id=manual__2023-12-17T01:20:48.150376+00:00, map_index=-1, run_start_date=2023-12-17 01:20:50.531005+00:00, run_end_date=2023-12-17 01:21:35.935362+00:00, run_duration=45.404357, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=18, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 01:20:49.191116+00:00, queued_by_job_id=1, pid=318076
2023-12-17 08:21:46,398 ERROR - DagFileProcessorManager (PID=316323) last sent a heartbeat 57.33 seconds ago! Restarting it
2023-12-17 08:21:46,405 INFO - Sending Signals.SIGTERM to group 316323. PIDs of all processes in the group: [316323]
2023-12-17 08:21:46,406 INFO - Sending the signal Signals.SIGTERM to group 316323
2023-12-17 08:21:46,538 INFO - Process psutil.Process(pid=316323, status='terminated', exitcode=0, started='08:17:34') (316323) terminated with exit code 0
2023-12-17 08:21:46,544 INFO - Launched DagFileProcessorManager with pid: 318630
2023-12-17 08:21:46,552 INFO - Configured default timezone Timezone('UTC')
2023-12-17 08:22:06,637 ERROR - Marking run <DagRun careerlink_pipeline @ 2023-12-17 01:20:48.150376+00:00: manual__2023-12-17T01:20:48.150376+00:00, state:running, queued_at: 2023-12-17 01:20:48.157671+00:00. externally triggered: True> failed
2023-12-17 08:22:06,638 INFO - DagRun Finished: dag_id=careerlink_pipeline, execution_date=2023-12-17 01:20:48.150376+00:00, run_id=manual__2023-12-17T01:20:48.150376+00:00, run_start_date=2023-12-17 01:20:49.078313+00:00, run_end_date=2023-12-17 01:22:06.638213+00:00, run_duration=77.5599, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:20:48.150376+00:00, data_interval_end=2023-12-17 01:20:48.150376+00:00, dag_hash=3ed92bf97425a1f1271e66654d6cba72
2023-12-17 08:22:06,646 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:22:05.469774+00:00 [scheduled]>
2023-12-17 08:22:06,646 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:22:06,646 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.crawl_careerlink manual__2023-12-17T01:22:05.469774+00:00 [scheduled]>
2023-12-17 08:22:06,648 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 08:22:06,648 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:22:05.469774+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 08:22:06,648 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:22:05.469774+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:22:06,650 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'crawl_careerlink', 'manual__2023-12-17T01:22:05.469774+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:22:53,876 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:22:05.469774+00:00', try_number=1, map_index=-1)
2023-12-17 08:22:53,880 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=crawl_careerlink, run_id=manual__2023-12-17T01:22:05.469774+00:00, map_index=-1, run_start_date=2023-12-17 01:22:07.928287+00:00, run_end_date=2023-12-17 01:22:53.576670+00:00, run_duration=45.648383, state=success, executor_state=success, try_number=1, max_tries=3, job_id=19, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 01:22:06.647390+00:00, queued_by_job_id=1, pid=318865
2023-12-17 08:22:54,065 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.check_daily_file_exists manual__2023-12-17T01:22:05.469774+00:00 [scheduled]>
2023-12-17 08:22:54,066 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:22:54,066 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.check_daily_file_exists manual__2023-12-17T01:22:05.469774+00:00 [scheduled]>
2023-12-17 08:22:54,067 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:22:54,067 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:22:05.469774+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:22:54,067 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'check_daily_file_exists', 'manual__2023-12-17T01:22:05.469774+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:22:54,070 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'check_daily_file_exists', 'manual__2023-12-17T01:22:05.469774+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:22:55,730 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:22:05.469774+00:00', try_number=1, map_index=-1)
2023-12-17 08:22:55,733 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:22:05.469774+00:00, map_index=-1, run_start_date=2023-12-17 01:22:55.331775+00:00, run_end_date=2023-12-17 01:22:55.431396+00:00, run_duration=0.099621, state=success, executor_state=success, try_number=1, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:22:54.066591+00:00, queued_by_job_id=1, pid=319172
2023-12-17 08:22:55,878 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:22:05.469774+00:00 [scheduled]>
2023-12-17 08:22:55,879 INFO - DAG careerlink_pipeline has 0/16 running and queued tasks
2023-12-17 08:22:55,879 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline.clean_data_and_ingest manual__2023-12-17T01:22:05.469774+00:00 [scheduled]>
2023-12-17 08:22:55,880 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:22:55,880 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:22:05.469774+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:22:55,880 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:22:05.469774+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:22:55,882 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline', 'clean_data_and_ingest', 'manual__2023-12-17T01:22:05.469774+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:22:57,652 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:22:05.469774+00:00', try_number=1, map_index=-1)
2023-12-17 08:22:57,657 INFO - TaskInstance Finished: dag_id=careerlink_pipeline, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:22:05.469774+00:00, map_index=-1, run_start_date=2023-12-17 01:22:57.124170+00:00, run_end_date=2023-12-17 01:22:57.333138+00:00, run_duration=0.208968, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:22:55.879666+00:00, queued_by_job_id=1, pid=319190
2023-12-17 08:23:08,656 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:23:18,799 ERROR - Marking run <DagRun careerlink_pipeline @ 2023-12-17 01:22:05.469774+00:00: manual__2023-12-17T01:22:05.469774+00:00, state:running, queued_at: 2023-12-17 01:22:05.481282+00:00. externally triggered: True> failed
2023-12-17 08:23:18,800 INFO - DagRun Finished: dag_id=careerlink_pipeline, execution_date=2023-12-17 01:22:05.469774+00:00, run_id=manual__2023-12-17T01:22:05.469774+00:00, run_start_date=2023-12-17 01:22:06.624930+00:00, run_end_date=2023-12-17 01:23:18.800285+00:00, run_duration=72.175355, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:22:05.469774+00:00, data_interval_end=2023-12-17 01:22:05.469774+00:00, dag_hash=3ed92bf97425a1f1271e66654d6cba72
2023-12-17 08:28:08,803 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:31:58,566 INFO - Setting next_dagrun for career_link_pipepline_test_1 to 2023-12-17T01:31:48.259143+00:00, run_after=2023-12-17T21:31:48.259143+00:00
2023-12-17 08:31:58,589 INFO - 1 tasks up for execution:
	<TaskInstance: career_link_pipepline_test_1.check_file_exists scheduled__2023-12-16T05:31:48.259143+00:00 [scheduled]>
2023-12-17 08:31:58,589 INFO - DAG career_link_pipepline_test_1 has 0/16 running and queued tasks
2023-12-17 08:31:58,590 INFO - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test_1.check_file_exists scheduled__2023-12-16T05:31:48.259143+00:00 [scheduled]>
2023-12-17 08:31:58,591 WARNING - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved
2023-12-17 08:31:58,591 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:31:48.259143+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:31:58,591 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'check_file_exists', 'scheduled__2023-12-16T05:31:48.259143+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:31:58,593 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'check_file_exists', 'scheduled__2023-12-16T05:31:48.259143+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:31:59,837 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:31:48.259143+00:00', try_number=1, map_index=-1)
2023-12-17 08:31:59,840 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test_1, task_id=check_file_exists, run_id=scheduled__2023-12-16T05:31:48.259143+00:00, map_index=-1, run_start_date=2023-12-17 01:31:59.489308+00:00, run_end_date=2023-12-17 01:31:59.562845+00:00, run_duration=0.073537, state=success, executor_state=success, try_number=1, max_tries=3, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:31:58.590541+00:00, queued_by_job_id=1, pid=322147
2023-12-17 08:31:59,990 INFO - 2 tasks up for execution:
	<TaskInstance: career_link_pipepline_test_1.check_file_exists manual__2023-12-17T01:31:58.305488+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test_1.clean_data_and_ingest scheduled__2023-12-16T05:31:48.259143+00:00 [scheduled]>
2023-12-17 08:31:59,991 INFO - DAG career_link_pipepline_test_1 has 0/16 running and queued tasks
2023-12-17 08:31:59,991 INFO - DAG career_link_pipepline_test_1 has 1/16 running and queued tasks
2023-12-17 08:31:59,991 INFO - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test_1.check_file_exists manual__2023-12-17T01:31:58.305488+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test_1.clean_data_and_ingest scheduled__2023-12-16T05:31:48.259143+00:00 [scheduled]>
2023-12-17 08:31:59,992 WARNING - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved
2023-12-17 08:31:59,992 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:31:59,993 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='check_file_exists', run_id='manual__2023-12-17T01:31:58.305488+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:31:59,993 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'check_file_exists', 'manual__2023-12-17T01:31:58.305488+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:31:59,993 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:31:48.259143+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 08:31:59,993 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:31:48.259143+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:31:59,995 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'check_file_exists', 'manual__2023-12-17T01:31:58.305488+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:32:01,183 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:31:48.259143+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:32:12,934 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='check_file_exists', run_id='manual__2023-12-17T01:31:58.305488+00:00', try_number=1, map_index=-1)
2023-12-17 08:32:12,934 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:31:48.259143+00:00', try_number=1, map_index=-1)
2023-12-17 08:32:12,939 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T05:31:48.259143+00:00, map_index=-1, run_start_date=2023-12-17 01:32:02.052065+00:00, run_end_date=2023-12-17 01:32:12.675760+00:00, run_duration=10.623695, state=success, executor_state=success, try_number=1, max_tries=3, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:31:59.991731+00:00, queued_by_job_id=1, pid=322183
2023-12-17 08:32:12,939 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test_1, task_id=check_file_exists, run_id=manual__2023-12-17T01:31:58.305488+00:00, map_index=-1, run_start_date=2023-12-17 01:32:00.848296+00:00, run_end_date=2023-12-17 01:32:00.919311+00:00, run_duration=0.071015, state=success, executor_state=success, try_number=1, max_tries=3, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:31:59.991731+00:00, queued_by_job_id=1, pid=322180
2023-12-17 08:32:13,087 INFO - Marking run <DagRun career_link_pipepline_test_1 @ 2023-12-16 05:31:48.259143+00:00: scheduled__2023-12-16T05:31:48.259143+00:00, state:running, queued_at: 2023-12-17 01:31:58.563666+00:00. externally triggered: False> successful
2023-12-17 08:32:13,087 INFO - DagRun Finished: dag_id=career_link_pipepline_test_1, execution_date=2023-12-16 05:31:48.259143+00:00, run_id=scheduled__2023-12-16T05:31:48.259143+00:00, run_start_date=2023-12-17 01:31:58.571448+00:00, run_end_date=2023-12-17 01:32:13.087799+00:00, run_duration=14.516351, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 05:31:48.259143+00:00, data_interval_end=2023-12-17 01:31:48.259143+00:00, dag_hash=fb77d72e76ec43f8e27609a366c66027
2023-12-17 08:32:13,089 INFO - Setting next_dagrun for career_link_pipepline_test_1 to 2023-12-17T01:31:48.259143+00:00, run_after=2023-12-17T21:31:48.259143+00:00
2023-12-17 08:32:13,096 INFO - 1 tasks up for execution:
	<TaskInstance: career_link_pipepline_test_1.clean_data_and_ingest manual__2023-12-17T01:31:58.305488+00:00 [scheduled]>
2023-12-17 08:32:13,096 INFO - DAG career_link_pipepline_test_1 has 0/16 running and queued tasks
2023-12-17 08:32:13,096 INFO - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test_1.clean_data_and_ingest manual__2023-12-17T01:31:58.305488+00:00 [scheduled]>
2023-12-17 08:32:13,097 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:32:13,098 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:31:58.305488+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 08:32:13,098 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:31:58.305488+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:32:13,099 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:31:58.305488+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:32:24,297 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:31:58.305488+00:00', try_number=1, map_index=-1)
2023-12-17 08:32:24,299 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:31:58.305488+00:00, map_index=-1, run_start_date=2023-12-17 01:32:13.937822+00:00, run_end_date=2023-12-17 01:32:24.104676+00:00, run_duration=10.166854, state=success, executor_state=success, try_number=1, max_tries=3, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:32:13.097269+00:00, queued_by_job_id=1, pid=322515
2023-12-17 08:32:24,458 INFO - Marking run <DagRun career_link_pipepline_test_1 @ 2023-12-17 01:31:58.305488+00:00: manual__2023-12-17T01:31:58.305488+00:00, state:running, queued_at: 2023-12-17 01:31:58.312587+00:00. externally triggered: True> successful
2023-12-17 08:32:24,459 INFO - DagRun Finished: dag_id=career_link_pipepline_test_1, execution_date=2023-12-17 01:31:58.305488+00:00, run_id=manual__2023-12-17T01:31:58.305488+00:00, run_start_date=2023-12-17 01:31:59.976081+00:00, run_end_date=2023-12-17 01:32:24.458966+00:00, run_duration=24.482885, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 05:31:58.305488+00:00, data_interval_end=2023-12-17 01:31:58.305488+00:00, dag_hash=fb77d72e76ec43f8e27609a366c66027
2023-12-17 08:33:08,948 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:36:00,349 ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: job.id

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 844, in _execute
    self._run_scheduler_loop()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 985, in _run_scheduler_loop
    perform_heartbeat(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 350, in perform_heartbeat
    job.heartbeat(heartbeat_callback=heartbeat_callback, session=session)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 211, in heartbeat
    session.commit()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: job.id
[SQL: INSERT INTO job (id, dag_id, state, job_type, start_date, end_date, latest_heartbeat, executor_class, hostname, unixname) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ((1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-17 01:03:00.820558', None, '2023-12-17 01:35:55.098051', None, 'phuc-ASUS', 'phuc'), (1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-17 01:03:00.820558', None, '2023-12-17 01:35:55.098051', None, 'phuc-ASUS', 'phuc'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2023-12-17 08:36:01,368 INFO - Sending Signals.SIGTERM to group 318630. PIDs of all processes in the group: [318630]
2023-12-17 08:36:01,368 INFO - Sending the signal Signals.SIGTERM to group 318630
2023-12-17 08:36:01,461 INFO - Process psutil.Process(pid=318630, status='terminated', exitcode=0, started='08:21:46') (318630) terminated with exit code 0
2023-12-17 08:36:01,461 INFO - Exited execute loop
2023-12-17 08:36:01,472 ERROR - Exception when running scheduler job
Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: job.id

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 47, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 289, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 318, in execute_job
    ret = execute_callable()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 844, in _execute
    self._run_scheduler_loop()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 985, in _run_scheduler_loop
    perform_heartbeat(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 350, in perform_heartbeat
    job.heartbeat(heartbeat_callback=heartbeat_callback, session=session)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 211, in heartbeat
    session.commit()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1890, in _execute_context
    self.dialect.do_executemany(
  File "/home/phuc/Practice/DataScience/DSProject/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 733, in do_executemany
    cursor.executemany(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: job.id
[SQL: INSERT INTO job (id, dag_id, state, job_type, start_date, end_date, latest_heartbeat, executor_class, hostname, unixname) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ((1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-17 01:03:00.820558', None, '2023-12-17 01:35:55.098051', None, 'phuc-ASUS', 'phuc'), (1, None, <JobState.RUNNING: 'running'>, 'SchedulerJob', '2023-12-17 01:03:00.820558', None, '2023-12-17 01:35:55.098051', None, 'phuc-ASUS', 'phuc'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
2023-12-17 08:36:30,441 INFO - Starting the scheduler
2023-12-17 08:36:30,442 INFO - Processing each file at most -1 times
2023-12-17 08:36:30,445 INFO - Launched DagFileProcessorManager with pid: 324561
2023-12-17 08:36:30,446 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:36:30,448 INFO - Configured default timezone Timezone('UTC')
2023-12-17 08:40:28,504 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.crawl_careerlink manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:40:28,504 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:40:28,505 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.crawl_careerlink manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:40:28,506 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 08:40:28,506 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 08:40:28,506 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'crawl_careerlink', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:40:28,508 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'crawl_careerlink', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:41:23,072 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1)
2023-12-17 08:41:23,076 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=crawl_careerlink, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:40:29.179082+00:00, run_end_date=2023-12-17 01:41:22.839851+00:00, run_duration=53.660769, state=success, executor_state=success, try_number=1, max_tries=3, job_id=3, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 01:40:28.505436+00:00, queued_by_job_id=2, pid=325934
2023-12-17 08:41:23,087 ERROR - DagFileProcessorManager (PID=324561) last sent a heartbeat 54.62 seconds ago! Restarting it
2023-12-17 08:41:23,092 INFO - Sending Signals.SIGTERM to group 324561. PIDs of all processes in the group: [324561]
2023-12-17 08:41:23,092 INFO - Sending the signal Signals.SIGTERM to group 324561
2023-12-17 08:41:23,184 INFO - Process psutil.Process(pid=324561, status='terminated', exitcode=0, started='08:36:30') (324561) terminated with exit code 0
2023-12-17 08:41:23,188 INFO - Launched DagFileProcessorManager with pid: 326436
2023-12-17 08:41:23,191 INFO - Configured default timezone Timezone('UTC')
2023-12-17 08:41:23,365 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:41:23,366 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:41:23,366 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:41:23,366 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:41:23,367 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:41:23,367 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:41:23,369 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:41:24,309 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1)
2023-12-17 08:41:24,311 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:41:24.006535+00:00, run_end_date=2023-12-17 01:41:24.074785+00:00, run_duration=0.06825, state=success, executor_state=success, try_number=1, max_tries=3, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:41:23.366488+00:00, queued_by_job_id=2, pid=326454
2023-12-17 08:41:24,460 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:41:24,460 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:41:24,460 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:41:24,461 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:41:24,461 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:41:24,461 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:41:24,463 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:41:25,457 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=1, map_index=-1)
2023-12-17 08:41:25,460 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:41:25.106653+00:00, run_end_date=2023-12-17 01:41:25.238709+00:00, run_duration=0.132056, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:41:24.461099+00:00, queued_by_job_id=2, pid=326472
2023-12-17 08:41:30,491 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:41:58,543 INFO - Setting next_dagrun for career_link_pipepline_test to 2023-12-17T01:41:54.568691+00:00, run_after=2023-12-17T21:41:54.568691+00:00
2023-12-17 08:41:58,572 INFO - 2 tasks up for execution:
	<TaskInstance: career_link_pipepline_test.check_file_exists scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.check_file_exists manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>
2023-12-17 08:41:58,572 INFO - DAG career_link_pipepline_test has 0/16 running and queued tasks
2023-12-17 08:41:58,573 INFO - DAG career_link_pipepline_test has 1/16 running and queued tasks
2023-12-17 08:41:58,573 INFO - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test.check_file_exists scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.check_file_exists manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>
2023-12-17 08:41:58,574 WARNING - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved
2023-12-17 08:41:58,574 WARNING - cannot record scheduled_duration for task check_file_exists because previous state change time has not been saved
2023-12-17 08:41:58,574 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:41:58,574 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:41:58,574 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:41:58,574 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:41:58,576 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:41:59,543 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'check_file_exists', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:42:00,508 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1)
2023-12-17 08:42:00,508 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='check_file_exists', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1)
2023-12-17 08:42:00,512 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=check_file_exists, run_id=manual__2023-12-17T01:41:57.842040+00:00, map_index=-1, run_start_date=2023-12-17 01:42:00.199716+00:00, run_end_date=2023-12-17 01:42:00.275337+00:00, run_duration=0.075621, state=success, executor_state=success, try_number=1, max_tries=3, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:41:58.573380+00:00, queued_by_job_id=2, pid=326834
2023-12-17 08:42:00,512 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=check_file_exists, run_id=scheduled__2023-12-16T05:41:54.568691+00:00, map_index=-1, run_start_date=2023-12-17 01:41:59.242025+00:00, run_end_date=2023-12-17 01:41:59.311729+00:00, run_duration=0.069704, state=success, executor_state=success, try_number=1, max_tries=3, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-12-17 01:41:58.573380+00:00, queued_by_job_id=2, pid=326817
2023-12-17 08:42:00,554 INFO - 2 tasks up for execution:
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>
2023-12-17 08:42:00,554 INFO - DAG career_link_pipepline_test has 0/16 running and queued tasks
2023-12-17 08:42:00,555 INFO - DAG career_link_pipepline_test has 1/16 running and queued tasks
2023-12-17 08:42:00,555 INFO - Setting the following tasks to queued state:
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest scheduled__2023-12-16T05:41:54.568691+00:00 [scheduled]>
	<TaskInstance: career_link_pipepline_test.clean_data_and_ingest manual__2023-12-17T01:41:57.842040+00:00 [scheduled]>
2023-12-17 08:42:00,555 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:42:00,555 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:42:00,556 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 08:42:00,556 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:42:00,556 INFO - Sending TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 08:42:00,556 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:42:00,558 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:41:54.568691+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:42:12,319 INFO - Executing command: ['airflow', 'tasks', 'run', 'career_link_pipepline_test', 'clean_data_and_ingest', 'manual__2023-12-17T01:41:57.842040+00:00', '--local', '--subdir', 'DAGS_FOLDER/career_link.py']
2023-12-17 08:42:23,990 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:41:54.568691+00:00', try_number=1, map_index=-1)
2023-12-17 08:42:23,990 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='career_link_pipepline_test', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:41:57.842040+00:00', try_number=1, map_index=-1)
2023-12-17 08:42:23,993 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:41:57.842040+00:00, map_index=-1, run_start_date=2023-12-17 01:42:12.968717+00:00, run_end_date=2023-12-17 01:42:23.770176+00:00, run_duration=10.801459, state=success, executor_state=success, try_number=1, max_tries=3, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:42:00.555371+00:00, queued_by_job_id=2, pid=327172
2023-12-17 08:42:23,993 INFO - TaskInstance Finished: dag_id=career_link_pipepline_test, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T05:41:54.568691+00:00, map_index=-1, run_start_date=2023-12-17 01:42:01.202498+00:00, run_end_date=2023-12-17 01:42:12.104155+00:00, run_duration=10.901657, state=success, executor_state=success, try_number=1, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:42:00.555371+00:00, queued_by_job_id=2, pid=326836
2023-12-17 08:42:24,149 INFO - Marking run <DagRun career_link_pipepline_test @ 2023-12-16 05:41:54.568691+00:00: scheduled__2023-12-16T05:41:54.568691+00:00, state:running, queued_at: 2023-12-17 01:41:58.538941+00:00. externally triggered: False> successful
2023-12-17 08:42:24,150 INFO - DagRun Finished: dag_id=career_link_pipepline_test, execution_date=2023-12-16 05:41:54.568691+00:00, run_id=scheduled__2023-12-16T05:41:54.568691+00:00, run_start_date=2023-12-17 01:41:58.555050+00:00, run_end_date=2023-12-17 01:42:24.150250+00:00, run_duration=25.5952, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 05:41:54.568691+00:00, data_interval_end=2023-12-17 01:41:54.568691+00:00, dag_hash=597b64aa6373d8a9b1b96bd8c81a1a31
2023-12-17 08:42:24,152 INFO - Setting next_dagrun for career_link_pipepline_test to 2023-12-17T01:41:54.568691+00:00, run_after=2023-12-17T21:41:54.568691+00:00
2023-12-17 08:42:24,154 INFO - Marking run <DagRun career_link_pipepline_test @ 2023-12-17 01:41:57.842040+00:00: manual__2023-12-17T01:41:57.842040+00:00, state:running, queued_at: 2023-12-17 01:41:57.849522+00:00. externally triggered: True> successful
2023-12-17 08:42:24,154 INFO - DagRun Finished: dag_id=career_link_pipepline_test, execution_date=2023-12-17 01:41:57.842040+00:00, run_id=manual__2023-12-17T01:41:57.842040+00:00, run_start_date=2023-12-17 01:41:58.555199+00:00, run_end_date=2023-12-17 01:42:24.154490+00:00, run_duration=25.599291, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 05:41:57.842040+00:00, data_interval_end=2023-12-17 01:41:57.842040+00:00, dag_hash=597b64aa6373d8a9b1b96bd8c81a1a31
2023-12-17 08:42:25,299 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:42:25,299 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:42:25,299 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:42:25,300 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:42:25,300 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:42:25,302 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:42:26,270 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=2, map_index=-1)
2023-12-17 08:42:26,272 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:42:25.957564+00:00, run_end_date=2023-12-17 01:42:26.073915+00:00, run_duration=0.116351, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:42:25.299963+00:00, queued_by_job_id=2, pid=327523
2023-12-17 08:43:26,807 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:43:26,807 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:43:26,807 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:43:26,808 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:43:26,809 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:43:26,810 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:43:27,704 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=3, map_index=-1)
2023-12-17 08:43:27,707 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:43:27.383736+00:00, run_end_date=2023-12-17 01:43:27.502164+00:00, run_duration=0.118428, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:43:26.807892+00:00, queued_by_job_id=2, pid=327788
2023-12-17 08:44:28,406 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:44:28,406 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:44:28,406 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [scheduled]>
2023-12-17 08:44:28,407 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:44:28,407 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:44:28,409 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']
2023-12-17 08:44:29,165 ERROR - Failed to execute task Command '['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:40:27.983434+00:00', '--local', '--subdir', 'DAGS_FOLDER/create_dag.py']' returned non-zero exit status 1..
2023-12-17 08:44:29,166 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:40:27.983434+00:00', try_number=4, map_index=-1)
2023-12-17 08:44:29,168 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:40:27.983434+00:00, map_index=-1, run_start_date=2023-12-17 01:43:27.383736+00:00, run_end_date=2023-12-17 01:43:27.502164+00:00, run_duration=0.118428, state=queued, executor_state=failed, try_number=4, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:44:28.407107+00:00, queued_by_job_id=2, pid=327788
2023-12-17 08:44:29,168 ERROR - Executor reports task instance <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
2023-12-17 08:44:29,169 ERROR - Executor reports task instance <TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:40:27.983434+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
2023-12-17 08:44:29,173 INFO - Marking task as FAILED. dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, execution_date=20231217T014027, start_date=20231217T014327, end_date=20231217T014429
2023-12-17 08:44:30,195 ERROR - Marking run <DagRun careerlink_pipeline_1 @ 2023-12-17 01:40:27.983434+00:00: manual__2023-12-17T01:40:27.983434+00:00, state:running, queued_at: 2023-12-17 01:40:28.000845+00:00. externally triggered: True> failed
2023-12-17 08:44:30,196 INFO - DagRun Finished: dag_id=careerlink_pipeline_1, execution_date=2023-12-17 01:40:27.983434+00:00, run_id=manual__2023-12-17T01:40:27.983434+00:00, run_start_date=2023-12-17 01:40:28.479712+00:00, run_end_date=2023-12-17 01:44:30.196155+00:00, run_duration=241.716443, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:40:27.983434+00:00, data_interval_end=2023-12-17 01:40:27.983434+00:00, dag_hash=cf06b2cb530061a49cad8a491f61caa5
2023-12-17 08:46:30,517 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:46:31,094 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:46:31,094 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:46:31,094 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.check_daily_file_exists manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:46:31,095 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:46:31,095 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:46:31,095 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:46:31,097 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'check_daily_file_exists', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:46:32,061 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1)
2023-12-17 08:46:32,063 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:46:31.750943+00:00, run_end_date=2023-12-17 01:46:31.825422+00:00, run_duration=0.074479, state=success, executor_state=success, try_number=1, max_tries=3, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:46:31.094705+00:00, queued_by_job_id=2, pid=328775
2023-12-17 08:46:32,101 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:46:32,101 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:46:32,101 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:46:32,102 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:46:32,102 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:46:32,102 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:46:32,104 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:46:33,091 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=1, map_index=-1)
2023-12-17 08:46:33,093 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:46:32.748939+00:00, run_end_date=2023-12-17 01:46:32.883962+00:00, run_duration=0.135023, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:46:32.101991+00:00, queued_by_job_id=2, pid=328777
2023-12-17 08:47:33,683 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:47:33,683 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:47:33,683 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:47:33,684 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:47:33,684 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:47:33,686 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:47:34,630 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=2, map_index=-1)
2023-12-17 08:47:34,632 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:47:34.303214+00:00, run_end_date=2023-12-17 01:47:34.423180+00:00, run_duration=0.119966, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:47:33.684052+00:00, queued_by_job_id=2, pid=329401
2023-12-17 08:48:35,276 INFO - 1 tasks up for execution:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:48:35,276 INFO - DAG careerlink_pipeline_1 has 0/16 running and queued tasks
2023-12-17 08:48:35,276 INFO - Setting the following tasks to queued state:
	<TaskInstance: careerlink_pipeline_1.clean_data_and_ingest manual__2023-12-17T01:46:30.261380+00:00 [scheduled]>
2023-12-17 08:48:35,277 INFO - Sending TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:48:35,277 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:48:35,279 INFO - Executing command: ['airflow', 'tasks', 'run', 'careerlink_pipeline_1', 'clean_data_and_ingest', 'manual__2023-12-17T01:46:30.261380+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 08:48:36,264 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='careerlink_pipeline_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:46:30.261380+00:00', try_number=3, map_index=-1)
2023-12-17 08:48:36,267 INFO - TaskInstance Finished: dag_id=careerlink_pipeline_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:46:30.261380+00:00, map_index=-1, run_start_date=2023-12-17 01:48:35.921740+00:00, run_end_date=2023-12-17 01:48:36.045393+00:00, run_duration=0.123653, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:48:35.276764+00:00, queued_by_job_id=2, pid=329989
2023-12-17 08:49:13,608 ERROR - Marking run <DagRun careerlink_pipeline_1 @ 2023-12-17 01:46:30.261380+00:00: manual__2023-12-17T01:46:30.261380+00:00, state:running, queued_at: 2023-12-17 01:46:30.267393+00:00. externally triggered: True> failed
2023-12-17 08:49:13,608 INFO - DagRun Finished: dag_id=careerlink_pipeline_1, execution_date=2023-12-17 01:46:30.261380+00:00, run_id=manual__2023-12-17T01:46:30.261380+00:00, run_start_date=2023-12-17 01:46:31.075056+00:00, run_end_date=2023-12-17 01:49:13.608489+00:00, run_duration=162.533433, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:46:30.261380+00:00, data_interval_end=2023-12-17 01:46:30.261380+00:00, dag_hash=d2ad1b986cbd90150590f88c9edf1d36
2023-12-17 08:51:30,564 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:52:52,018 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:52:52,019 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:52:52,019 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:52:52,019 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:52:52,020 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:52:52,020 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:52:52,022 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:52:52,978 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1)
2023-12-17 08:52:52,980 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:52:52.662184+00:00, run_end_date=2023-12-17 01:52:52.730208+00:00, run_duration=0.068024, state=success, executor_state=success, try_number=1, max_tries=3, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:52:52.019375+00:00, queued_by_job_id=2, pid=331454
2023-12-17 08:52:53,014 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:52:53,014 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:52:53,014 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:52:53,015 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:52:53,015 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:52:53,015 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:52:53,017 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:52:54,009 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=1, map_index=-1)
2023-12-17 08:52:54,012 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:52:53.657195+00:00, run_end_date=2023-12-17 01:52:53.789899+00:00, run_duration=0.132704, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:52:53.014786+00:00, queued_by_job_id=2, pid=331472
2023-12-17 08:53:54,711 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:53:54,711 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:53:54,711 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:53:54,712 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:53:54,712 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:53:54,714 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:53:55,709 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=2, map_index=-1)
2023-12-17 08:53:55,712 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:53:55.344680+00:00, run_end_date=2023-12-17 01:53:55.482778+00:00, run_duration=0.138098, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:53:54.711730+00:00, queued_by_job_id=2, pid=332087
2023-12-17 08:54:55,526 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:54:55,527 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:54:55,527 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:52:50.516305+00:00 [scheduled]>
2023-12-17 08:54:55,528 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:54:55,528 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:54:55,529 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:52:50.516305+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:54:56,513 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:52:50.516305+00:00', try_number=3, map_index=-1)
2023-12-17 08:54:56,516 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:52:50.516305+00:00, map_index=-1, run_start_date=2023-12-17 01:54:56.192643+00:00, run_end_date=2023-12-17 01:54:56.312515+00:00, run_duration=0.119872, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:54:55.527493+00:00, queued_by_job_id=2, pid=332673
2023-12-17 08:55:13,529 INFO - Setting next_dagrun for sjvlksdnv to 2023-12-17T01:55:12.291602+00:00, run_after=2023-12-17T21:55:12.291602+00:00
2023-12-17 08:55:13,564 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.check_daily_file_exists scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>
2023-12-17 08:55:13,564 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:55:13,565 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.check_daily_file_exists scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>
2023-12-17 08:55:13,565 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:55:13,566 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:55:13,566 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:55:13,567 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:55:14,507 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1)
2023-12-17 08:55:14,510 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=check_daily_file_exists, run_id=scheduled__2023-12-16T05:55:12.291602+00:00, map_index=-1, run_start_date=2023-12-17 01:55:14.233290+00:00, run_end_date=2023-12-17 01:55:14.297791+00:00, run_duration=0.064501, state=success, executor_state=success, try_number=1, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:55:13.565278+00:00, queued_by_job_id=2, pid=332847
2023-12-17 08:55:14,549 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>
2023-12-17 08:55:14,549 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:55:14,549 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest scheduled__2023-12-16T05:55:12.291602+00:00 [scheduled]>
2023-12-17 08:55:14,550 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:55:14,550 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:55:14,550 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:55:14,552 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'scheduled__2023-12-16T05:55:12.291602+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:55:15,478 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T05:55:12.291602+00:00', try_number=1, map_index=-1)
2023-12-17 08:55:15,480 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T05:55:12.291602+00:00, map_index=-1, run_start_date=2023-12-17 01:55:15.171734+00:00, run_end_date=2023-12-17 01:55:15.298811+00:00, run_duration=0.127077, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:55:14.549782+00:00, queued_by_job_id=2, pid=332864
2023-12-17 08:56:06,121 ERROR - Marking run <DagRun sjvlksdnv @ 2023-12-17 01:52:50.516305+00:00: manual__2023-12-17T01:52:50.516305+00:00, state:running, queued_at: 2023-12-17 01:52:50.524300+00:00. externally triggered: True> failed
2023-12-17 08:56:06,122 INFO - DagRun Finished: dag_id=sjvlksdnv, execution_date=2023-12-17 01:52:50.516305+00:00, run_id=manual__2023-12-17T01:52:50.516305+00:00, run_start_date=2023-12-17 01:52:51.999844+00:00, run_end_date=2023-12-17 01:56:06.122202+00:00, run_duration=194.122358, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-17 01:52:50.516305+00:00, data_interval_end=2023-12-17 01:52:50.516305+00:00, dag_hash=0a633e910157b1637b704117f925587c
2023-12-17 08:56:13,089 ERROR - Marking run <DagRun sjvlksdnv @ 2023-12-16 05:55:12.291602+00:00: scheduled__2023-12-16T05:55:12.291602+00:00, state:running, queued_at: 2023-12-17 01:55:13.524145+00:00. externally triggered: False> failed
2023-12-17 08:56:13,089 INFO - DagRun Finished: dag_id=sjvlksdnv, execution_date=2023-12-16 05:55:12.291602+00:00, run_id=scheduled__2023-12-16T05:55:12.291602+00:00, run_start_date=2023-12-17 01:55:13.545479+00:00, run_end_date=2023-12-17 01:56:13.089515+00:00, run_duration=59.544036, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 05:55:12.291602+00:00, data_interval_end=2023-12-17 01:55:12.291602+00:00, dag_hash=0a633e910157b1637b704117f925587c
2023-12-17 08:56:13,091 INFO - Setting next_dagrun for sjvlksdnv to 2023-12-17T01:55:12.291602+00:00, run_after=2023-12-17T21:55:12.291602+00:00
2023-12-17 08:56:30,606 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 08:57:13,636 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:57:13,636 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:57:13,637 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.check_daily_file_exists manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:57:13,637 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 08:57:13,637 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 08:57:13,638 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:57:13,639 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'check_daily_file_exists', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:57:14,589 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='check_daily_file_exists', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1)
2023-12-17 08:57:14,592 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=check_daily_file_exists, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:57:14.282016+00:00, run_end_date=2023-12-17 01:57:14.353320+00:00, run_duration=0.071304, state=success, executor_state=success, try_number=1, max_tries=3, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 01:57:13.637262+00:00, queued_by_job_id=2, pid=333893
2023-12-17 08:57:14,631 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:57:14,631 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:57:14,631 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:57:14,632 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 08:57:14,632 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:57:14,632 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:57:14,634 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:57:15,671 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1)
2023-12-17 08:57:15,674 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:57:15.270450+00:00, run_end_date=2023-12-17 01:57:15.410911+00:00, run_duration=0.140461, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:57:14.631995+00:00, queued_by_job_id=2, pid=333895
2023-12-17 08:59:05,875 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:59:05,875 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:59:05,875 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.clean_data_and_ingest manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:59:05,876 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 08:59:05,876 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:59:05,878 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'clean_data_and_ingest', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:59:19,560 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1)
2023-12-17 08:59:19,562 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:59:06.528542+00:00, run_end_date=2023-12-17 01:59:19.369853+00:00, run_duration=12.841311, state=success, executor_state=success, try_number=2, max_tries=4, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 01:59:05.875860+00:00, queued_by_job_id=2, pid=334976
2023-12-17 08:59:19,610 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:59:19,610 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 08:59:19,610 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 08:59:19,611 WARNING - cannot record scheduled_duration for task upsert_to_warehouse because previous state change time has not been saved
2023-12-17 08:59:19,611 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 08:59:19,611 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:59:19,613 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 08:59:27,202 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=1, map_index=-1)
2023-12-17 08:59:27,205 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 01:59:20.216010+00:00, run_end_date=2023-12-17 01:59:26.992309+00:00, run_duration=6.776299, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 01:59:19.610880+00:00, queued_by_job_id=2, pid=335388
2023-12-17 09:01:30,639 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:02:27,315 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 09:02:27,315 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 09:02:27,315 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 09:02:27,316 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default
2023-12-17 09:02:27,316 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 09:02:27,318 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 09:02:34,845 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=2, map_index=-1)
2023-12-17 09:02:34,847 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 02:02:27.885892+00:00, run_end_date=2023-12-17 02:02:34.644731+00:00, run_duration=6.758839, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 02:02:27.316120+00:00, queued_by_job_id=2, pid=336959
2023-12-17 09:05:34,962 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 09:05:34,962 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 09:05:34,962 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 09:05:34,963 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=3, map_index=-1) to executor with priority 1 and queue default
2023-12-17 09:05:34,963 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 09:05:34,968 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 09:05:43,312 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=3, map_index=-1)
2023-12-17 09:05:43,314 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 02:05:35.604030+00:00, run_end_date=2023-12-17 02:05:43.137001+00:00, run_duration=7.532971, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 02:05:34.963174+00:00, queued_by_job_id=2, pid=338922
2023-12-17 09:06:30,677 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:08:43,427 INFO - 1 tasks up for execution:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 09:08:43,427 INFO - DAG sjvlksdnv has 0/16 running and queued tasks
2023-12-17 09:08:43,427 INFO - Setting the following tasks to queued state:
	<TaskInstance: sjvlksdnv.upsert_to_warehouse manual__2023-12-17T01:57:12.724097+00:00 [scheduled]>
2023-12-17 09:08:43,429 INFO - Sending TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=4, map_index=-1) to executor with priority 1 and queue default
2023-12-17 09:08:43,429 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 09:08:43,431 INFO - Executing command: ['airflow', 'tasks', 'run', 'sjvlksdnv', 'upsert_to_warehouse', 'manual__2023-12-17T01:57:12.724097+00:00', '--local', '--subdir', 'DAGS_FOLDER/sjdkj.py']
2023-12-17 09:08:51,156 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sjvlksdnv', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T01:57:12.724097+00:00', try_number=4, map_index=-1)
2023-12-17 09:08:51,159 INFO - TaskInstance Finished: dag_id=sjvlksdnv, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T01:57:12.724097+00:00, map_index=-1, run_start_date=2023-12-17 02:08:44.011019+00:00, run_end_date=2023-12-17 02:08:50.965787+00:00, run_duration=6.954768, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 02:08:43.428257+00:00, queued_by_job_id=2, pid=340976
2023-12-17 09:08:51,327 ERROR - Marking run <DagRun sjvlksdnv @ 2023-12-17 01:57:12.724097+00:00: manual__2023-12-17T01:57:12.724097+00:00, state:running, queued_at: 2023-12-17 01:57:12.729023+00:00. externally triggered: True> failed
2023-12-17 09:08:51,328 INFO - DagRun Finished: dag_id=sjvlksdnv, execution_date=2023-12-17 01:57:12.724097+00:00, run_id=manual__2023-12-17T01:57:12.724097+00:00, run_start_date=2023-12-17 01:57:13.616197+00:00, run_end_date=2023-12-17 02:08:51.328069+00:00, run_duration=697.711872, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 05:57:12.724097+00:00, data_interval_end=2023-12-17 01:57:12.724097+00:00, dag_hash=0a633e910157b1637b704117f925587c
2023-12-17 09:11:30,706 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:16:30,749 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:21:30,787 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:26:30,831 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:31:30,842 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:33:37,379 INFO - Setting next_dagrun for test_1 to 2023-12-17T02:33:21.397961+00:00, run_after=2023-12-17T22:33:21.397961+00:00
2023-12-17 09:33:37,410 INFO - 2 tasks up for execution:
	<TaskInstance: test_1.crawl_careerlink scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:33:37,410 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:33:37,410 INFO - DAG test_1 has 1/16 running and queued tasks
2023-12-17 09:33:37,410 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.crawl_careerlink scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:33:37,411 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 09:33:37,411 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 09:33:37,411 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 09:33:37,411 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:33:37,411 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 09:33:37,411 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:33:37,413 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:34:20,950 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:08,519 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1)
2023-12-17 09:35:08,519 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1)
2023-12-17 09:35:08,522 INFO - TaskInstance Finished: dag_id=test_1, task_id=crawl_careerlink, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:34:21.581415+00:00, run_end_date=2023-12-17 02:35:08.300836+00:00, run_duration=46.719421, state=success, executor_state=success, try_number=1, max_tries=3, job_id=30, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 02:33:37.410829+00:00, queued_by_job_id=2, pid=348372
2023-12-17 09:35:08,522 INFO - TaskInstance Finished: dag_id=test_1, task_id=crawl_careerlink, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:33:38.071056+00:00, run_end_date=2023-12-17 02:34:20.719136+00:00, run_duration=42.64808, state=success, executor_state=success, try_number=1, max_tries=3, job_id=29, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 02:33:37.410829+00:00, queued_by_job_id=2, pid=347948
2023-12-17 09:35:08,533 ERROR - DagFileProcessorManager (PID=326436) last sent a heartbeat 91.17 seconds ago! Restarting it
2023-12-17 09:35:08,538 INFO - Sending Signals.SIGTERM to group 326436. PIDs of all processes in the group: [326436]
2023-12-17 09:35:08,538 INFO - Sending the signal Signals.SIGTERM to group 326436
2023-12-17 09:35:08,630 INFO - Process psutil.Process(pid=326436, status='terminated', exitcode=0, started='08:41:22') (326436) terminated with exit code 0
2023-12-17 09:35:08,634 INFO - Launched DagFileProcessorManager with pid: 348798
2023-12-17 09:35:08,638 INFO - Configured default timezone Timezone('UTC')
2023-12-17 09:35:08,801 INFO - 2 tasks up for execution:
	<TaskInstance: test_1.check_daily_file_exists scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:35:08,802 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:35:08,802 INFO - DAG test_1 has 1/16 running and queued tasks
2023-12-17 09:35:08,802 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.check_daily_file_exists scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:35:08,803 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 09:35:08,803 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 09:35:08,803 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 09:35:08,803 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:08,804 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 09:35:08,804 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:08,806 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:09,750 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:10,716 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1)
2023-12-17 09:35:10,716 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1)
2023-12-17 09:35:10,719 INFO - TaskInstance Finished: dag_id=test_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:35:10.397455+00:00, run_end_date=2023-12-17 02:35:10.467227+00:00, run_duration=0.069772, state=success, executor_state=success, try_number=1, max_tries=3, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 02:35:08.802673+00:00, queued_by_job_id=2, pid=348849
2023-12-17 09:35:10,719 INFO - TaskInstance Finished: dag_id=test_1, task_id=check_daily_file_exists, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:35:09.440855+00:00, run_end_date=2023-12-17 02:35:09.510975+00:00, run_duration=0.07012, state=success, executor_state=success, try_number=1, max_tries=3, job_id=31, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 02:35:08.802673+00:00, queued_by_job_id=2, pid=348816
2023-12-17 09:35:10,869 INFO - 2 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:35:10,869 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:35:10,869 INFO - DAG test_1 has 1/16 running and queued tasks
2023-12-17 09:35:10,869 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:35:10,870 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 09:35:10,870 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 09:35:10,870 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:35:10,870 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:10,871 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:35:10,871 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:10,872 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:20,576 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:35:32,416 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=1, map_index=-1)
2023-12-17 09:35:32,417 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=1, map_index=-1)
2023-12-17 09:35:32,419 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:35:21.220964+00:00, run_end_date=2023-12-17 02:35:32.150118+00:00, run_duration=10.929154, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:35:10.870086+00:00, queued_by_job_id=2, pid=349152
2023-12-17 09:35:32,420 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:35:11.506750+00:00, run_end_date=2023-12-17 02:35:20.325226+00:00, run_duration=8.818476, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=33, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:35:10.870086+00:00, queued_by_job_id=2, pid=348852
2023-12-17 09:36:30,890 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:38:20,554 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
2023-12-17 09:38:20,554 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:38:20,554 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
2023-12-17 09:38:20,555 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:38:20,555 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:38:20,557 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:38:30,018 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=2, map_index=-1)
2023-12-17 09:38:30,021 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:38:21.159421+00:00, run_end_date=2023-12-17 02:38:29.746575+00:00, run_duration=8.587154, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=35, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:38:20.555192+00:00, queued_by_job_id=2, pid=351096
2023-12-17 09:38:32,969 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:38:32,969 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:38:32,970 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:38:32,970 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:38:32,971 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:38:32,972 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:38:42,419 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=2, map_index=-1)
2023-12-17 09:38:42,422 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:38:33.719466+00:00, run_end_date=2023-12-17 02:38:42.193953+00:00, run_duration=8.474487, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=36, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:38:32.970298+00:00, queued_by_job_id=2, pid=351410
2023-12-17 09:41:29,947 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
2023-12-17 09:41:29,947 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:41:29,947 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
2023-12-17 09:41:29,948 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:41:29,948 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:41:29,950 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:41:34,669 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=3, map_index=-1)
2023-12-17 09:41:34,671 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:41:30.547356+00:00, run_end_date=2023-12-17 02:41:34.492000+00:00, run_duration=3.944644, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=37, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:41:29.947895+00:00, queued_by_job_id=2, pid=353584
2023-12-17 09:41:34,691 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:41:42,868 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:41:42,868 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:41:42,868 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:41:42,869 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:41:42,869 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:41:42,871 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:41:47,594 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=3, map_index=-1)
2023-12-17 09:41:47,597 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:41:43.465366+00:00, run_end_date=2023-12-17 02:41:47.394243+00:00, run_duration=3.928877, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=38, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:41:42.868830+00:00, queued_by_job_id=2, pid=353938
2023-12-17 09:44:35,009 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
2023-12-17 09:44:35,010 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:44:35,010 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest scheduled__2023-12-16T06:33:21.397961+00:00 [scheduled]>
2023-12-17 09:44:35,010 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:44:35,011 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:44:35,012 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'scheduled__2023-12-16T06:33:21.397961+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:44:39,963 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='scheduled__2023-12-16T06:33:21.397961+00:00', try_number=4, map_index=-1)
2023-12-17 09:44:39,966 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, map_index=-1, run_start_date=2023-12-17 02:44:35.594061+00:00, run_end_date=2023-12-17 02:44:39.751597+00:00, run_duration=4.157536, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=39, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:44:35.010445+00:00, queued_by_job_id=2, pid=355485
2023-12-17 09:44:39,999 ERROR - Marking run <DagRun test_1 @ 2023-12-16 06:33:21.397961+00:00: scheduled__2023-12-16T06:33:21.397961+00:00, state:running, queued_at: 2023-12-17 02:33:37.374817+00:00. externally triggered: False> failed
2023-12-17 09:44:40,000 INFO - DagRun Finished: dag_id=test_1, execution_date=2023-12-16 06:33:21.397961+00:00, run_id=scheduled__2023-12-16T06:33:21.397961+00:00, run_start_date=2023-12-17 02:33:37.392843+00:00, run_end_date=2023-12-17 02:44:40.000050+00:00, run_duration=662.607207, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-12-16 06:33:21.397961+00:00, data_interval_end=2023-12-17 02:33:21.397961+00:00, dag_hash=6cbc0fa2fc22135dbb147534e184480a
2023-12-17 09:44:40,002 INFO - Setting next_dagrun for test_1 to 2023-12-17T02:33:21.397961+00:00, run_after=2023-12-17T22:33:21.397961+00:00
2023-12-17 09:44:48,141 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:44:48,141 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 09:44:48,141 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T02:33:36.360825+00:00 [scheduled]>
2023-12-17 09:44:48,142 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default
2023-12-17 09:44:48,142 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:44:48,145 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T02:33:36.360825+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 09:44:52,758 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T02:33:36.360825+00:00', try_number=4, map_index=-1)
2023-12-17 09:44:52,761 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T02:33:36.360825+00:00, map_index=-1, run_start_date=2023-12-17 02:44:48.734557+00:00, run_end_date=2023-12-17 02:44:52.539894+00:00, run_duration=3.805337, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=40, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 02:44:48.142040+00:00, queued_by_job_id=2, pid=355702
2023-12-17 09:44:52,901 ERROR - Marking run <DagRun test_1 @ 2023-12-17 02:33:36.360825+00:00: manual__2023-12-17T02:33:36.360825+00:00, state:running, queued_at: 2023-12-17 02:33:36.367976+00:00. externally triggered: True> failed
2023-12-17 09:44:52,901 INFO - DagRun Finished: dag_id=test_1, execution_date=2023-12-17 02:33:36.360825+00:00, run_id=manual__2023-12-17T02:33:36.360825+00:00, run_start_date=2023-12-17 02:33:37.392986+00:00, run_end_date=2023-12-17 02:44:52.901467+00:00, run_duration=675.508481, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 06:33:36.360825+00:00, data_interval_end=2023-12-17 02:33:36.360825+00:00, dag_hash=6cbc0fa2fc22135dbb147534e184480a
2023-12-17 09:46:34,718 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:51:34,744 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 09:56:34,785 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 10:01:14,077 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:01:14,077 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 10:01:14,077 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.crawl_careerlink manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:01:14,078 WARNING - cannot record scheduled_duration for task crawl_careerlink because previous state change time has not been saved
2023-12-17 10:01:14,078 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
2023-12-17 10:01:14,078 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:01:14,080 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'crawl_careerlink', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:01:49,677 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='crawl_careerlink', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)
2023-12-17 10:01:49,680 INFO - TaskInstance Finished: dag_id=test_1, task_id=crawl_careerlink, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:01:14.759131+00:00, run_end_date=2023-12-17 03:01:49.430547+00:00, run_duration=34.671416, state=success, executor_state=success, try_number=1, max_tries=3, job_id=41, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2023-12-17 03:01:14.077834+00:00, queued_by_job_id=2, pid=361088
2023-12-17 10:01:49,707 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 10:01:49,710 INFO - Marked 1 SchedulerJob instances as failed
2023-12-17 10:01:49,839 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:01:49,839 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 10:01:49,840 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.check_daily_file_exists manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:01:49,840 WARNING - cannot record scheduled_duration for task check_daily_file_exists because previous state change time has not been saved
2023-12-17 10:01:49,841 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2023-12-17 10:01:49,841 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:01:49,843 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'check_daily_file_exists', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:01:50,795 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='check_daily_file_exists', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)
2023-12-17 10:01:50,798 INFO - TaskInstance Finished: dag_id=test_1, task_id=check_daily_file_exists, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:01:50.478232+00:00, run_end_date=2023-12-17 03:01:50.554505+00:00, run_duration=0.076273, state=success, executor_state=success, try_number=1, max_tries=3, job_id=42, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2023-12-17 03:01:49.840348+00:00, queued_by_job_id=2, pid=361445
2023-12-17 10:01:50,945 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:01:50,945 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 10:01:50,945 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.clean_data_and_ingest manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:01:50,946 WARNING - cannot record scheduled_duration for task clean_data_and_ingest because previous state change time has not been saved
2023-12-17 10:01:50,946 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2023-12-17 10:01:50,946 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:01:50,948 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'clean_data_and_ingest', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:02:02,399 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='clean_data_and_ingest', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)
2023-12-17 10:02:02,402 INFO - TaskInstance Finished: dag_id=test_1, task_id=clean_data_and_ingest, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:01:51.586343+00:00, run_end_date=2023-12-17 03:02:02.185495+00:00, run_duration=10.599152, state=success, executor_state=success, try_number=1, max_tries=3, job_id=43, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2023-12-17 03:01:50.945823+00:00, queued_by_job_id=2, pid=361463
2023-12-17 10:02:02,549 INFO - 1 tasks up for execution:
	<TaskInstance: test_1.upsert_to_warehouse manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:02:02,549 INFO - DAG test_1 has 0/16 running and queued tasks
2023-12-17 10:02:02,549 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_1.upsert_to_warehouse manual__2023-12-17T03:01:13.587963+00:00 [scheduled]>
2023-12-17 10:02:02,550 WARNING - cannot record scheduled_duration for task upsert_to_warehouse because previous state change time has not been saved
2023-12-17 10:02:02,550 INFO - Sending TaskInstanceKey(dag_id='test_1', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2023-12-17 10:02:02,550 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_1', 'upsert_to_warehouse', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:02:02,552 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_1', 'upsert_to_warehouse', 'manual__2023-12-17T03:01:13.587963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_dags.py']
2023-12-17 10:02:15,037 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_1', task_id='upsert_to_warehouse', run_id='manual__2023-12-17T03:01:13.587963+00:00', try_number=1, map_index=-1)
2023-12-17 10:02:15,039 INFO - TaskInstance Finished: dag_id=test_1, task_id=upsert_to_warehouse, run_id=manual__2023-12-17T03:01:13.587963+00:00, map_index=-1, run_start_date=2023-12-17 03:02:03.201423+00:00, run_end_date=2023-12-17 03:02:14.812392+00:00, run_duration=11.610969, state=success, executor_state=success, try_number=1, max_tries=3, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-12-17 03:02:02.549828+00:00, queued_by_job_id=2, pid=361820
2023-12-17 10:02:15,071 INFO - Marking run <DagRun test_1 @ 2023-12-17 03:01:13.587963+00:00: manual__2023-12-17T03:01:13.587963+00:00, state:running, queued_at: 2023-12-17 03:01:13.594697+00:00. externally triggered: True> successful
2023-12-17 10:02:15,071 INFO - DagRun Finished: dag_id=test_1, execution_date=2023-12-17 03:01:13.587963+00:00, run_id=manual__2023-12-17T03:01:13.587963+00:00, run_start_date=2023-12-17 03:01:14.060180+00:00, run_end_date=2023-12-17 03:02:15.071894+00:00, run_duration=61.011714, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-12-16 07:01:13.587963+00:00, data_interval_end=2023-12-17 03:01:13.587963+00:00, dag_hash=6cbc0fa2fc22135dbb147534e184480a
2023-12-17 10:06:49,733 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 10:11:49,757 INFO - Adopting or resetting orphaned tasks for active dag runs
2023-12-17 10:16:49,788 INFO - Adopting or resetting orphaned tasks for active dag runs
